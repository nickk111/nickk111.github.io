[{"content":"","date":"2024-08-25T11:05:09+08:00","permalink":"https://nickk111.github.io/p/second/","title":"Second"},{"content":"程序人生 \u0026ndash;以下是《深入浅出MFC》作者侯捷先生于2001年来华中科技大学做人文讲座时的演讲内容。\n如果你不曾听过侯捷的名字，不曾知道侯捷做的事情，你不可能有兴趣走入会场。因此，各位远道而来，我窃以为，无非想看看侯捷本人，听听他说话。如果你期盼在这种场合听到某某技术的剖析，某某趋势的发展，肯定你会失望。我不是趋势专家，对此也毫无兴趣。台上说话和台下聊天不同，我不能也不敢讲我没有心得没有研究的话题。“ 程序人生” 这个话题旨在让大家对一个你感兴趣的人（侯捷我）的学习历程有些了解，或许从中给你一些灵感或激励。 我在一个被昵称为“ 少林寺”（台湾工研院）的地方，磨练三年。后半期因为发现了自己浓烈的兴趣与不错的天赋，决定转向技术写作与教育这条路。3 0岁之后的我， 行事常思“ 贡献度”，我知道自己在技术写作与教育这条路上能够走得比程序开发更好，所以决定把自己摆在最适当的位置。一口食物，放在嘴里是佳肴，吐出来就成了秽物。天生我材必有用，每个人都应该仔细思考，自己真正的兴趣和才能在哪里。很多人都问，3 0 岁之后做不动程序员了怎么办。3 0 年正是英年，体力和智力和成熟度都正达到巅峰，怎么会做不动程序？想往管理阶层走当然很好，那就努力充实自己，并且扪心自问，你做管理快乐吗？要知道，人事绝对比机器让你更焦头烂额。如果你决定争取一个粥少僧多的职位，就不要再问“ 怎么办”。还能怎么办？努力以赴呀。比赛还没开始就问输了怎么办，这不像话，你注定要输。 技术养成阶段，对我影响最大的一件事是，我自动请缨做一套公用程序库，目标给全部门乃至全所使用。这使我学习到技术的整理、文件（d o c u m e n t s ）的撰写、人际的沟通。重要的不在具体实作，而在多方培养了正确观念。如果你问我，对于程序，我最重视什么？我最重视可读性（含说明文件）、维护性、复用性，完整性。这些其实是一体多面。 转向技术写作后，我的生活和待在业界没有什么改变，只不过业界的产出是软体，我的产出是书籍和文章。写一本书和规划一个专案（p r o j e c t ）没什么两样。但是，专心于技术写作之后，从此我有绝对的自由钻研我最感兴趣的“ 技术本质” 与“ 技术核心”。我周遭的朋友，但凡表现不凡者，都有非凡的资料整理功夫。如今网络发达，资讯爆炸，硬盘又便宜，资料整理功夫更显重要。没有经过自己整理的资料，形同垃圾。许多人喜欢上网“ 收集” 一大堆电子书、电子文档。你得想个办法把这些庞大的资料化为你的图书馆，而不是搁在硬盘角落里做为安慰或炫耀。书籍也一样，买来要看，安慰自己或炫耀他人都没有任何意义。当然，一旦你到达某种层次，以及某种经济能力，你买书不见得马上看，不见得整本看。我有个私人小图书馆，其中的书有许多还没看，当初购买是准备随时参考用的，也有些是当做学习的目标，摆着准备有空时看。今年是我写作的第1 0 个年头。我认为自己确实走上了一条最适合我的路，尤其今天这么热烈的场面，实在令我情绪激昂。我不会忸怩作态地不愿承认我的作品给别人带来帮助，然而我要说，作者和读者是相互激励相互影响的，我们彼此进入了一个良性循环。没有优秀的读者，就没有优秀的作者。艺术家可能不是这样，但电脑技术写作，或更缩小范围地说，我，是这样。因此，我要衷心感谢那些给我鼓舞、给我勘误、给我赞美、给我批评的热情读者。下面回答几个常被提出来的问题。 1 . 如何学习 大哉问。学习需要明师。但是明师可遇不可求，所以退而求其次你需要好书，并尽早建立自修的基础。迷时师渡，悟了自渡，寻好书看好书，就是你的自渡法门。切记，徒学不足以自行，计算机是实作性很强的一门科技，你一定要动手做，最忌讳眼高手低。学而不思则罔，思而不学则殆，一定要思考、沉淀、整理。整理的功夫我要特别强调。许多人一味勇往直前，追求最新技术发展，却忽略了整理沉淀的功夫。如果知识不能深刻内化为你的思想，那么这份知识很快会离你而去。 2 . 科班与非科班, 名校与非名校 各位身为名校学生，身为科班生，从来不必在乎这个问题，那是饱人不知饿人饥。这个题目上我是5 0 比5 0 ，我出身名校，但非科班。虽然我从来没有被这个问题所惑，但的确有许多年轻学子为此辗转反侧，苦恼不已。学历和背景只是一个证明，证明你曾经经历过某种考验，证明你曾经经历过某种训练。但并不保证考验后或训练后的质量。你所处的环境如果极重视出身，这是你无能为力的— — 毛主席要废除封建，千百年来的人心却难以废除。但是不要气馁，你总有机会证明你的能力。上天不会不给任何人至少一个机会，关键在于机会来时你准备好了没有。 3 . 升学（ 考研）与就业 先升学好还是先就业好？未曾深刻对发问者的个人背景做一番了解与分析，就遽然给答案，是不负责任的骗子。我只能说，以我的经验和我的观察，如果你能够先就业再继续深造，就业所得的各种经验会对你的治学方式带来很大的帮助。就连你的人生历练，都会对你和你的指导教授的相处带来帮助─ 这可是件大事， 影响你3 ~ 6 年的生活。（ 注：台湾硕士生两年，博士生四年，大陆硕士生三年，博士生三年）。 4 . 培养自信心 嘴巴无法培养自信心，手才能够。只要切切实实地动手做点东西，你的自信心就会逐渐建立起来。随着自信心的建立，你就再也不会问“C + + 还有前途吗”、 “J a v a 还有前途吗”、“V B 还有前途吗”这种问题。下面是我给同学的七个勉励 1 . 乐趣 L i n u x 操作系统的创造者 L i n u s 最近出了一本自传：《J u s t f o r F u n 》，简体版译名为《乐者为王》。如果我来译，我就译为《一切只为乐趣》。是的，兴趣才能使你乐在其中，乐在其中你才会产生热情，热情才能使你卓越。要忠于自己的兴趣。有人问，怎样才能找到自己的兴趣，如果我有答案，我就可以开一个“ 卡内基兴趣开发中心”，成为全球首富。这种问题不会有明确答案的，你的兴趣要别人来帮你开发，咄咄怪事。你可以多方尝试，但是首先要有起码的坚持。练琴很辛苦，音阶训练枯燥无比，但如果稍加坚持，也许你得到了赞美，也就发掘了兴趣。很多人说兴趣不能当饭吃，错，兴趣可以当饭吃。出问题的不在“ 兴趣何方”， 而在 “ 能否坚持”。 2 . 坚持 我在今年四月份给新竹交通大学资讯系一个演讲，题目是：唯坚持得成功。我自己才能平庸，但我很能坚持。我的这种个性在朋友之间是被称道的。坚持并不代表一定成功，不过坚持本身就是一种美好的情操。所谓谋事在人，成事在天，只要坚持，我们总可以心安理得地说：那美好的战我打过了。人生最后要的不就是心安理得吗？ 3 . 格调 做事不但要坚持，而且要坚持高格调。格调使人高贵。俗世成功不保证格调，格调也不保证俗世成功，但是格调使人拥有尊严，使人获得尊敬。我在台湾，观察计算机书籍的写作与出版，对于格调特别有所感触。有些作者与出版社，并不在乎格调，也不在乎贡献，只在乎生意，只在乎利润。生意要做，利润要赚，传道还需道粮嘛，但是金钱绝不能摆在第一位，否则生意和利润都不会长远。因金钱而结合的，终将因金钱而分手而结束。关于这个，台湾有许多活生生的例子，可为大陆出版社借鉴。 4 . 谦虚与教养 再怎么开明的师长前辈，也许可以接纳年轻人的飞扬跋扈，也许可以接受年轻人的无理取闹，但当他真正需要帮手或真正要培养人才时，他一定特别考虑谦虚有教养的年轻人。没有什么是不能挑战的，但是做为挑战者，你要言之有物、言之有理。毛主席说， “ 没有调查就没有发言权”，这话说的真好。毛主席又说“造反有理”，言下之意是所有的造反都有理，这话就很没有道理。 5 . 气势 气势和先前说到的谦虚，两间之间不好拿捏，拿捏尺寸属于艺术范畴。圆熟的人生历练，才能把两者调理得恰到好处。我的想法是：做人要谦虚，做事要有气势。这次来内地演讲，接触读者，网上很多的评语是：他很谦虚。为什么这么说？难道侯捷曾经给人不谦虚的印象吗？是因为我文章中的气势吗？谦虚和气势，并不是两条平行线。 6 . 勤奋 爱迪生说，“ 成功是百分之一的天才加上百分之九十九的努力”。道理非常清楚，我没有什么引申。你问任何一位你认为成功的人他是否勤奋，看看他怎么说。我有一位大学同学，跳舞打牌爱吃爱玩，但是每次微积分考试都比我好。我比他勤奋，他比我聪明。天赋使然，别在上面钻牛角尖（我曾经钻得很痛苦）。要知道，人生的成绩单和学校的成绩单没有必然关联。人生很长，要看长远，要计久长。 7 . 超越自己的“ 局限” 清华一位同学问我，最佩服哪些程序员，我一时答不上来。经过同学的引导，我说了几个名字。同学又问我，我佩服的都是些外国人吗？我略略想了一下说是。同学（似乎）失望地坐了下来。 事实上，在那个突然的问题中，我的思考迷了路。我的回答并不真正代表我的心意。我从来没有想过谁是我最佩服的程序员。在我的生活中那是一个不存在的话题。技术不是真理，我没有崇拜过哪一位程序员或技术大师。我知道大陆有着地位极为崇高（近乎民族英雄）的程序员，他们的事迹对来自台湾地区的我而言，总是有着一层陌生。当然，传奇令人神往，我也爱听他们的事迹。至于台湾，从来没有知名的程序员，台湾不曾走过这样一个个人英雄时代。现在，我要修正我在清华的回答。我真正佩服的，是那些超越自己局限的人- - 任何人，不只是程序员。“ 局限” 是你的家庭、你的环境加在你身上的先天桎梏，谁能摆脱先天桎梏，谁便是人生勇者，值得最大的尊敬与佩服。如果我的读者之中有人佩服我，我希望那是因为我对技术写作的执着以及对年轻学子的关怀，不是因为我的技术。再且，我的技术也只普通而已。 任重而道远 我为什么有机会在华中科技大学和同学们有这么热烈的一次接触？原因是我的书在华中科技大学出版社出版，而他们追求质量的态度，对作者的尊重，令我感动。当我拿到《Essential C++》简体版，我大吃一惊，制作质量完全不逊于繁体版。我告诉我的编辑，侯捷所有后续书籍秉此办理。这几天，仔细了解《深入浅出MFC》一波三折的出版过程后，真正体会到，没有优秀的后援，好书终究到不了读者手上，那么，作者再多的质量、坚持、格调，终是一场空。 身为一个自由作家，没有任何理由我需要在乎计算器技术书籍的整体发展。我把自己的书写好，已经很对得起我的社会责任。然而我诚恳告诉各位，计算器技术书籍的整体发展和侯捷个人的发展，两者在我心中有相同的比重。前者说小了，影响大家的求知，说大了，影响国家的IT产业。读者对于这方面的殷切期待，在侯捷网站上的读者来函中一再出现。昨天我从周老师手上又获得几封读者来信，其中一封言词诚恳，不卑不亢，特别令我感动，我把它念出来与大家分享。信中对我个人的谬赞，不敢当。\n","date":"2024-08-24T16:04:14+08:00","permalink":"https://nickk111.github.io/p/myfirstblog/","title":"MyFirstBlog"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://nickk111.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu4699868770670889127.jpg","permalink":"https://nickk111.github.io/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://nickk111.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://nickk111.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://nickk111.github.io/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"https://nickk111.github.io/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2019-03-08T00:00:00Z","permalink":"https://nickk111.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://nickk111.github.io/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://nickk111.github.io/p/emoji-support/","title":"Emoji Support"},{"content":"YOLOv11改进 | Conv/卷积篇 | 利用2024最新YOLOv9的GELAN模块替换C3k2结构（附轻量化版本 + 高效涨点版本 + 结构图 一、本文介绍 本文给大家带来的改进机制是利用2024/02/21号发布的 YOLOv9其中提出的GELAN模块来改进YOLOv11中的C3k2 ，GELAN融合了CSPNet和ELAN机制同时其中利用到了RepConv在获取更多有效特征的同时在推理时专用单分支结构从而不影响推理速度，同时本文的内容提供了两种版本一种是参数量更低涨点效果略微弱一些的版本，另一种是参数量稍多一些但是效果要不参数量低的效果要好一些（均为我个人整理），提供两种版本是为了适配不同需求的读者，具体选择那种大家可以根据自身的需求来选择即可，文章中我都均已提供， 同时本文的结构存在大量的二次创新机会，后面我也会提供。\n** 专栏回顾： ** ** YOLOv11改进系列专栏——本专栏持续复习各种顶会内容——科研必备******\n目录\n一、本文介绍\n二、GELAN的原理\n2.1 Generalized ELAN\n2.2 Generalized ELAN结构图\n三、核心代码\n四、手把手教你添加GELAN机制\n4.1 修改一\n4.2 修改二\n4.3 修改三\n4.4 修改四\n五、GELAN的yaml文件和运行记录\n5.1 GELAN低参数量版本的yaml文件\n5.2 GELAN高参数量版本的yaml文件\n5.3 训练代码\n5.3 GELAN的训练过程截图\n5.3.1 低参数量版本\n5.3.2 高参数量版本\n五、本文总结\n二、GELAN的原理 2.1 Generalized ELAN 在本节中，我们描述了提出的新网络架构 - GELAN。通过结合两种神经网络架构CSPNet和ELAN，这两种架构都是以梯度路径规划设计的，我们设计了考虑了轻量级、推理速度和准确性的广义高效层聚合网络（GELAN）。其整体架构如图4所示。我们推广了ELAN的能力，ELAN原本只使用卷积层的堆叠，到一个新的架构，可以使用任何计算块。\n这张图（图4）展示了广义高效层聚合网络（GELAN）的架构，以及它是如何从CSPNet和ELAN这两种神经网络架构演变而来的。这两种架构都设计有梯度路径规划。\na) CSPNet： 在CSPNet的架构中，输入通过一个转换层被分割为两部分，然后分别通过任意的计算块。之后，这些分支被重新合并（通过concatenation），并再次通过转换层。\nb) ELAN： 与CSPNet相比，ELAN采用了堆叠的卷积层，其中每一层的输出都会与下一层的输入相结合，再经过卷积处理。\nc) GELAN： 结合了CSPNet和ELAN的设计，提出了GELAN。它采用了CSPNet的分割和重组的概念，并在每一部分引入了ELAN的层级卷积处理方式。不同之处在于GELAN不仅使用卷积层，还可以使用任何计算块，使得网络更加灵活，能够根据不同的应用需求定制。\nGELAN的设计考虑到了轻量化、推理速度和精确度，以此来提高模型的整体性能。图中显示的模块和分区的可选性进一步增加了网络的适应性和可定制性。GELAN的这种结构允许它支持多种类型的计算块，这使得它可以更好地适应各种不同的计算需求和硬件约束。\n总的来说，GELAN的架构是为了提供一个更加通用和高效的网络，可以适应从轻量级到复杂的深度学习任务，同时保持或增强计算效率和性能。通过这种方式，GELAN旨在解决现有架构的限制，提供一个可扩展的解决方案，以适应未来深度学习的发展。\n大家看图片一眼就能看出来它融合了什么，就是将CSPHet的anyBlock模块堆叠的方式和ELAN融合到了一起。\n2.2 Generalized ELAN结构图 YOLOv9最主要的创新目前能够得到的就是其中的 GELAN结构 ，我也是分析其代码根据论文将其结构图绘画出来。\n下面的文件为YOLOv9的yaml文件。可以看到的是其提出了一种结构名字RepNCSPELAN4，其中的结构图concat后的通道数我没有画是因为它有计算中间的参数的变量是根据个人设置来的。\n其代码和结构图如下所示！\nclass RepNCSPELAN4(nn.Module): # csp-elan def __init__(self, c1, c2, c5=1): # c5 = repeat super().__init__() c3 = int(c2 / 2) c4 = int(c3 / 2) self.c = c3 // 2 self.cv1 = Conv(c1, c3, 1, 1) self.cv2 = nn.Sequential(RepNCSP(c3 // 2, c4, c5), Conv(c4, c4, 3, 1)) self.cv3 = nn.Sequential(RepNCSP(c4, c4, c5), Conv(c4, c4, 3, 1)) self.cv4 = Conv(c3 + (2 * c4), c2, 1, 1) def forward(self, x): y = list(self.cv1(x).chunk(2, 1)) y.extend((m(y[-1])) for m in [self.cv2, self.cv3]) return self.cv4(torch.cat(y, 1)) def forward_split(self, x): y = list(self.cv1(x).split((self.c, self.c), 1)) y.extend(m(y[-1]) for m in [self.cv2, self.cv3]) return self.cv4(torch.cat(y, 1))\r三、核心代码 核心代码的使用方式看章节四！\nimport torchimport torch.nn as nnimport numpy as np __all__ = ['RepNCSPELAN4_low', 'RepNCSPELAN4_high'] class RepConvN(nn.Module): \u0026quot;\u0026quot;\u0026quot;RepConv is a basic rep-style block, including training and deploy status This code is based on https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py \u0026quot;\u0026quot;\u0026quot; default_act = nn.SiLU() # default activation def __init__(self, c1, c2, k=3, s=1, p=1, g=1, d=1, act=True, bn=False, deploy=False): super().__init__() assert k == 3 and p == 1 self.g = g self.c1 = c1 self.c2 = c2 self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity() self.bn = None self.conv1 = Conv(c1, c2, k, s, p=p, g=g, act=False) self.conv2 = Conv(c1, c2, 1, s, p=(p - k // 2), g=g, act=False) def forward_fuse(self, x): \u0026quot;\u0026quot;\u0026quot;Forward process\u0026quot;\u0026quot;\u0026quot; return self.act(self.conv(x)) def forward(self, x): \u0026quot;\u0026quot;\u0026quot;Forward process\u0026quot;\u0026quot;\u0026quot; id_out = 0 if self.bn is None else self.bn(x) return self.act(self.conv1(x) + self.conv2(x) + id_out) def get_equivalent_kernel_bias(self): kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1) kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2) kernelid, biasid = self._fuse_bn_tensor(self.bn) return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid def _avg_to_3x3_tensor(self, avgp): channels = self.c1 groups = self.g kernel_size = avgp.kernel_size input_dim = channels // groups k = torch.zeros((channels, input_dim, kernel_size, kernel_size)) k[np.arange(channels), np.tile(np.arange(input_dim), groups), :, :] = 1.0 / kernel_size ** 2 return k def _pad_1x1_to_3x3_tensor(self, kernel1x1): if kernel1x1 is None: return 0 else: return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1]) def _fuse_bn_tensor(self, branch): if branch is None: return 0, 0 if isinstance(branch, Conv): kernel = branch.conv.weight running_mean = branch.bn.running_mean running_var = branch.bn.running_var gamma = branch.bn.weight beta = branch.bn.bias eps = branch.bn.eps elif isinstance(branch, nn.BatchNorm2d): if not hasattr(self, 'id_tensor'): input_dim = self.c1 // self.g kernel_value = np.zeros((self.c1, input_dim, 3, 3), dtype=np.float32) for i in range(self.c1): kernel_value[i, i % input_dim, 1, 1] = 1 self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device) kernel = self.id_tensor running_mean = branch.running_mean running_var = branch.running_var gamma = branch.weight beta = branch.bias eps = branch.eps std = (running_var + eps).sqrt() t = (gamma / std).reshape(-1, 1, 1, 1) return kernel * t, beta - running_mean * gamma / std def fuse_convs(self): if hasattr(self, 'conv'): return kernel, bias = self.get_equivalent_kernel_bias() self.conv = nn.Conv2d(in_channels=self.conv1.conv.in_channels, out_channels=self.conv1.conv.out_channels, kernel_size=self.conv1.conv.kernel_size, stride=self.conv1.conv.stride, padding=self.conv1.conv.padding, dilation=self.conv1.conv.dilation, groups=self.conv1.conv.groups, bias=True).requires_grad_(False) self.conv.weight.data = kernel self.conv.bias.data = bias for para in self.parameters(): para.detach_() self.__delattr__('conv1') self.__delattr__('conv2') if hasattr(self, 'nm'): self.__delattr__('nm') if hasattr(self, 'bn'): self.__delattr__('bn') if hasattr(self, 'id_tensor'): self.__delattr__('id_tensor') class RepNBottleneck(nn.Module): # Standard bottleneck def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5): # ch_in, ch_out, shortcut, kernels, groups, expand super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = RepConvN(c1, c_, k[0], 1) self.cv2 = Conv(c_, c2, k[1], 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) class RepNCSP(nn.Module): # CSP Bottleneck with 3 convolutions def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(RepNBottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) def autopad(k, p=None, d=1): # kernel, padding, dilation # Pad to 'same' shape outputs if d \u0026gt; 1: k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k] # actual kernel-size if p is None: p = k // 2 if isinstance(k, int) else [x // 2 for x in k] # auto-pad return p class Conv(nn.Module): # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation) default_act = nn.SiLU() # default activation def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True): super().__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity() def forward(self, x): return self.act(self.bn(self.conv(x))) def forward_fuse(self, x): return self.act(self.conv(x)) class RepNCSPELAN4_low(nn.Module): # csp-elan def __init__(self, c1, c2, c5=1): # c5 = repeat super().__init__() c3 = int(c2 / 2) c4 = int(c3 / 2) self.c = c3 // 2 self.cv1 = Conv(c1, c3, 1, 1) self.cv3 = nn.Sequential(RepNCSP(c3, c3, c5)) self.cv4 = Conv(c3 + (2 * c4), c2, 1, 1) def forward(self, x): temp = self.cv1(x) temp3 = self.cv3(temp) y = list(temp.chunk(2, 1)) y.append(temp3) temp2 = torch.cat(y, 1) return self.cv4(temp2) def forward_split(self, x): y = list(self.cv1(x).split((self.c, self.c), 1)) y.extend(m(y[-1]) for m in [self.cv2, self.cv3]) return self.cv4(torch.cat(y, 1)) class RepNCSPELAN4_high(nn.Module): # csp-elan def __init__(self, c1, c2, c5=1): # c5 = repeat super().__init__() c3 = c2 c4 = int(c3 / 2) self.c = c3 // 2 self.cv1 = Conv(c1, c3, 1, 1) self.cv2 = nn.Sequential(RepNCSP(c3 // 2, c4, c5), Conv(c4, c4, 3, 1)) self.cv3 = nn.Sequential(RepNCSP(c4, c4, c5), Conv(c4, c4, 3, 1)) self.cv4 = Conv(c3 + (2 * c4), c2, 1, 1) def forward(self, x): y = list(self.cv1(x).chunk(2, 1)) y.extend((m(y[-1])) for m in [self.cv2, self.cv3]) return self.cv4(torch.cat(y, 1)) def forward_split(self, x): y = list(self.cv1(x).split((self.c, self.c), 1)) y.extend(m(y[-1]) for m in [self.cv2, self.cv3]) return self.cv4(torch.cat(y, 1)) if __name__ == \u0026quot;__main__\u0026quot;: # Generating Sample image image_size = (1, 24, 224, 224) image = torch.rand(*image_size) # Model mobilenet_v1 = RepNCSPELAN4_low(24, 24) out = mobilenet_v1(image) print(out.size())\r四、手把手教你添加GELAN机制 4.1 修改一 第一还是建立文件，我们找到如下ultralytics/nn/modules文件夹下建立一个目录名字呢就是\u0026rsquo;Addmodules\u0026rsquo;文件夹( 用群内的文件的话已经有了无需新建) ！然后在其内部建立一个新的py文件将核心代码复制粘贴进去即可。\n4.2 修改二 第二步我们在该目录下创建一个新的py文件名字为\u0026rsquo;init.py\u0026rsquo;( 用群内的文件的话已经有了无需新建) ，然后在其内部导入我们的检测头如下图所示。\n4.3 修改三 第三步我门中到如下文件\u0026rsquo;ultralytics/nn/tasks.py\u0026rsquo;进行导入和注册我们的模块( 用群内的文件的话已经有了无需重新导入直接开始第四步即可) ！\n从今天开始以后的教程就都统一成这个样子了，因为我默认大家用了我群内的文件来进行修改！！\n4.4 修改四 按照我的添加在parse_model里添加即可。\n到此就修改完成了，大家可以复制下面的yaml文件运行。\n五、GELAN的yaml文件和运行记录 5.1 GELAN低参数量版本的yaml文件 此版本训练信息：YOLO11-RepGELAN-low summary: 403 layers, 2,218,027 parameters, 2,218,011 gradients, 6.3 GFLOPs\n# Ultralytics YOLO 🚀, AGPL-3.0 license# YOLO11 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect # Parametersnc: 80 # number of classesscales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n' # [depth, width, max_channels] n: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs s: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs m: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs l: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs x: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs # YOLO11n backbonebackbone: # [from, repeats, module, args] - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2 - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4 - [-1, 2, RepNCSPELAN4_low, [256]] - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8 - [-1, 2, RepNCSPELAN4_low, [512]] - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16 - [-1, 2, RepNCSPELAN4_low, [512]] - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32 - [-1, 2, RepNCSPELAN4_low, [1024]] - [-1, 1, SPPF, [1024, 5]] # 9 - [-1, 2, C2PSA, [1024]] # 10 # YOLO11n headhead: - [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]] - [[-1, 6], 1, Concat, [1]] # cat backbone P4 - [-1, 2, RepNCSPELAN4_low, [512]] # 13 - [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]] - [[-1, 4], 1, Concat, [1]] # cat backbone P3 - [-1, 2, RepNCSPELAN4_low, [256]] # 16 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 13], 1, Concat, [1]] # cat head P4 - [-1, 2, RepNCSPELAN4_low, [512]] # 19 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 10], 1, Concat, [1]] # cat head P5 - [-1, 2, RepNCSPELAN4_low, [1024]] # 22 (P5/32-large) - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\r5.2 GELAN高参数量版本的yaml文件 此版本训练信息：YOLO11-RepGELAN-high summary: 651 layers, 3,837,803 parameters, 3,837,787 gradients, 12.1 GFLOPs\n# Ultralytics YOLO 🚀, AGPL-3.0 license# YOLO11 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect # Parametersnc: 80 # number of classesscales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n' # [depth, width, max_channels] n: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs s: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs m: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs l: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs x: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs # YOLO11n backbonebackbone: # [from, repeats, module, args] - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2 - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4 - [-1, 2, RepNCSPELAN4_high, [256]] - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8 - [-1, 2, RepNCSPELAN4_high, [512]] - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16 - [-1, 2, RepNCSPELAN4_high, [512]] - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32 - [-1, 2, RepNCSPELAN4_high, [1024]] - [-1, 1, SPPF, [1024, 5]] # 9 - [-1, 2, C2PSA, [1024]] # 10 # YOLO11n headhead: - [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]] - [[-1, 6], 1, Concat, [1]] # cat backbone P4 - [-1, 2, RepNCSPELAN4_high, [512]] # 13 - [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]] - [[-1, 4], 1, Concat, [1]] # cat backbone P3 - [-1, 2, RepNCSPELAN4_high, [256]] # 16 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 13], 1, Concat, [1]] # cat head P4 - [-1, 2, RepNCSPELAN4_high, [512]] # 19 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 10], 1, Concat, [1]] # cat head P5 - [-1, 2, RepNCSPELAN4_high, [1024]] # 22 (P5/32-large) - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\r5.3 训练代码 大家可以创建一个py文件将我给的代码复制粘贴进去，配置好自己的文件路径即可运行。\nimport warningswarnings.filterwarnings('ignore')from ultralytics import YOLO if __name__ == '__main__': model = YOLO('ultralytics/cfg/models/v8/yolov8-C2f-FasterBlock.yaml') # model.load('yolov8n.pt') # loading pretrain weights model.train(data=r'替换数据集yaml文件地址', # 如果大家任务是其它的'ultralytics/cfg/default.yaml'找到这里修改task可以改成detect, segment, classify, pose cache=False, imgsz=640, epochs=150, single_cls=False, # 是否是单类别检测 batch=4, close_mosaic=10, workers=0, device='0', optimizer='SGD', # using SGD # resume='', # 如过想续训就设置last.pt的地址 amp=False, # 如果出现训练损失为Nan可以关闭amp project='runs/train', name='exp', )\r5.3 GELAN的训练过程截图 5.3.1 低参数量版本 5.3.2 高参数量版本 五、本文总结 到此本文的正式分享内容就结束了，在这里给大家推荐我的YOLOv11改进有效涨点专栏，本专栏目前为新开的平均质量分98分，后期我会根据各种最新的前沿顶会进行论文复现，也会对一些老的改进机制进行补充，如果大家觉得本文帮助到你了，订阅本专栏，关注后续更多的更新~\n","date":"0001-01-01T00:00:00Z","permalink":"https://nickk111.github.io/p/","title":""},{"content":"YOLOv11 | 一文带你深入理解ultralytics最新作品yolov11的创新 | 训练、推理、验证、导出 （附网络结构图） 目录\n一、本文介绍\n二、YOLOv11和YOLOv8对比\n三、YOLOv11的网络结构解析\n四、YOLOv11下载、环境安装、数据集获取\n五、模型训练\n5.1 训练的三种方式\n5.1.1 方式一\n5.1.2 方式二\n5.1.3 方式三 （推荐，避免keyError错误.）\n六、模型验证/测试\n七、模型推理\n八、模型输出\n一、本文介绍 ultralytics发布了最新的作品YOLOv11，这一次YOLOv11的变化相对于ultralytics公司的上一代作品YOLOv8变化不是很大的（YOLOv9、YOLOv10均不是ultralytics公司作品），其中改变的位置涉及到C2f变为C3K2，在SPPF后面加了一层类似于注意力机制的C2PSA，还有一个变化大家从yaml文件是看不出来的就是它的检测头内部替换了两个DWConv，以及模型的深度和宽度参数进行了大幅度调整，但是在损失函数方面就没有变化还是采用的CIoU作为边界框回归损失，下面带大家深入理解一下ultralytics最新作品YOLOv11的创新点。\n下图为最近的YOLO系列发布时间线！\n二、YOLOv11和YOLOv8对比 在YOLOYOLOv5，YOLOv8，和YOLOv11是ultralytics公司作品（ultralytics出品必属精品），下面用一张图片从yaml文件来带大家对比一下YOLOv8和YOLOv11的区别，配置文件变得内容比较少大家可以看一卡，左侧为YOLOv8右侧为YOLOv11，不同的点我用黑线标注了出来。\n三、YOLOv11的网络结构解析 下面的图片为YOLOv11的网络结构图。\n**其中主要创新点可以总结如下- \u0026gt; **\n1. 提出C3k2机制，其中C3k2有参数为c3k，其中在网络的浅层c3k设置为False（下图中可以看到c3k2第二个参数被设置为False，就是对应的c3k参数）。\n此时所谓的C3k2就相当于YOLOv8中的C2f，其网络结构为一致的，其中的C3k机制的网络结构图如下图所示 （为什么叫C3k2，我个人理解是因为C3k的调用时C3k其中的参数N固定设置为2的原因，个人理解不一定对 ）。\n2. 第二个创新点是提出C2PSA机制，这是一个C2（C2f的前身）机制内部嵌入了一个多头注意力机制，在这个过程中我还发现作者尝试了C2fPSA机制但是估计效果不如C2PSA，有的时候机制有没有效果理论上真的很难解释通，下图为C2PSA机制的原理图，仔细观察把Attention哪里去掉则C2PSA机制就变为了C2所以我上面说C2PSA就是C2里面嵌入了一个PSA机制。\n3. 第三个创新点可以说是原先的解耦头中的分类检测头增加了两个DWConv，具体的对比大家可以看下面两个图下面的是YOLOv11的解耦头，上面的是YOLOv8的解耦头.\n我们上面看到了在分类检测头中YOLOv11插入了两个DWConv这样的做法可以大幅度减少参数量和计算量（原先两个普通的Conv大家要注意到卷积和是由3变为了1的，这是形成了两个深度可分离Conv），大家可能不太理解为什么加入了两个DWConv还能够减少计算量，以及什么是深度可分离Conv，下面我来解释一下。\nDWConv 代表 Depthwise Convolution（深度卷积），是一种在卷积神经网络中常用的高效卷积操作。它主要用于减少计算复杂度和参数量，尤其在移动端或轻量化网络（如 MobileNet）中十分常见。\n1. 标准卷积的计算过程\n在标准卷积操作中，对于一个输入张量（通常是一个多通道的特征图），卷积核的尺寸是 (h, w, C_in)，其中 h 和 w 是卷积核的空间尺寸，C_in 是输入通道的数量。而卷积核与输入张量做的是完整的卷积运算，每个输出通道都与所有输入通道相连并参与卷积操作，导致计算量比较大。\n标准卷积的计算过程是这样的：\n每个输出通道是所有输入通道的组合（加权求和），卷积核在每个位置都会计算与所有输入通道的点积。 假设有 C_in 个输入通道和 C_out 个输出通道，那么卷积核的总参数量是 C_in * C_out * h * w。 2. Depthwise Convolution（DWConv）\n与标准卷积不同， 深度卷积 将输入的每个通道单独处理，即 每个通道都有自己的卷积核进行卷积 ，不与其他通道进行交互。它可以被看作是标准卷积的一部分，专注于空间维度上的卷积运算。\n深度卷积的计算过程：\n假设输入张量有 C_in 个通道，每个通道会使用一个 h × w 的卷积核进行卷积操作。这个过程称为“深度卷积”，因为每个通道独立进行卷积运算。 输出的通道数与输入通道数一致，每个输出通道只和对应的输入通道进行卷积，没有跨通道的组合。 参数量和计算量相比标准卷积大大减少，卷积核的参数量是 C_in * h * w。 深度卷积的优点：\n计算效率高 ：相对于标准卷积，深度卷积显著减少了计算量。它只处理空间维度上的卷积，不再处理通道间的卷积。 参数量减少 ：由于每个卷积核只对单个通道进行卷积，参数量大幅减少。例如，标准卷积的参数量为 C_in * C_out * h * w，而深度卷积的参数量为 C_in * h * w。 结合点卷积可提升效果 ：为了弥补深度卷积缺乏跨通道信息整合的问题，通常深度卷积后会配合 1x1 的点卷积（Pointwise Convolution）使用，通过 1x1 的卷积核整合跨通道的信息。这种组合被称为 深度可分离卷积 （Depthwise Separable Convolution） | 这也是我们本文YOLOv11中的做法 。 3. 深度卷积与标准卷积的区别\n操作类型 卷积核大小 输入通道数 输出通道数 参数量 标准卷积 h × w C_in C_out C_in * C_out * h * w 深度卷积（DWConv） h × w C_in C_in C_in * h * w 可以看出，深度卷积在相同的卷积核大小下，参数量减少了约 C_out 倍 （细心的人可以发现用最新版本的ultralytics仓库运行YOLOv8参数量相比于之前的YOLOv8以及大幅度减少了这就是因为检测头改了的原因但是名字还是Detect，所以如果你想继续用YOLOv8发表论文做实验那么不要更新最近的ultralytics仓库）。\n4. 深度可分离卷积 (Depthwise Separable Convolution)\n深度卷积常与 1x1 的点卷积配合使用，这称为深度可分离卷积。其过程如下：\n先对输入张量进行深度卷积，对每个通道独立进行空间卷积。 然后通过 1x1 点卷积，对通道维度进行混合，整合不同通道的信息。 这样既可以保证计算量的减少，又可以保持跨通道的信息流动。\n5. 总结\nDWConv 是一种高效的卷积方式，通过单独处理每个通道来减少计算量，结合 1x1 的点卷积，形成深度可分离卷积，可以在保持网络性能的同时极大地减少模型的计算复杂度和参数量。\n看到这里大家应该明白了为什么加入了两个DWConv还能减少参数量以及YOLOv11的检测头创新点在哪里。\n4.YOLOv11和YOLOv8还有一个不同的点就是其各个版本的模型（N - S - M- L - X）网络深度和宽度变了 可以看到在深度（depth）和宽度 （width）两个地方YOLOv8和YOLOv11是基本上完全不同了，这里我理解这么做的含义就是模型网络变小了，所以需要加深一些模型的放缩倍数来弥补模型之前丧失的能力从而来达到一个平衡。\n本章总结： YOLOv11的改进点其实并不多更多的都是一些小的结构上的创新，相对于之前的YOLOv5到YOLOv8的创新，其实YOLOv11的创新点不算多，但是其是ultralytics公司的出品，同时ultralytics仓库的使用量是非常多的（不像YOLOv9和YOLOv10）所以在未来的很长一段时间内其实YOLO系列估计不会再更新了，YOLOv11作为最新的SOTA肯定是十分适合大家来发表论文和创新的。\n最后强调： 本文只是对YOLOv11的创新部分进行了部分解析，其余部分其实和YOLOv8保持一致大家有需要的可以自行查阅其它资料，同时有解析不对的地方欢迎大家评论区指出和讨论。\n四、YOLOv11下载、环境安装、数据集获取 YOLOv11的下载大家可以通过点击下面的链接进行下载-\u0026gt;\n官方下载地址：YOLOv11官方Github下载地址点击此处即可跳转.\n点进去之后按照如下图所示的操作即可下载ultralytics仓库到本地.\n下载到本地之后大家解压缩利用自己的IDEA打开即可了，环境搭建大家可以看我另一篇文章，这里由于篇幅原因就不多介绍了，如果你自己有环境了跳过此步即可.\n环境安装下载 ：环境安装教程点击此处即可跳转.\n数据集获取方法可以看以下文章内容，利用roboflow获取大量数据集（1000w+数据集任你挑选）\n数据集下载教程：roboflow一键导出Voc、COCO、Yolo、Csv、yolo等格式数据集教程\n五、模型训练 上面给大家讲完了网络的创新下面给大家讲一下YOLOv11如何进行训练预测验证等操作。\n我门打开ultralytics/cfg/default.yaml文件可以配置模型的参数， 在其中和模型训练有关的参数及其解释如下:\n参数名 输入类型 参数解释 0 task str YOLO模型的任务选择，选择你是要进行检测、分类等操作 1 mode str YOLO模式的选择，选择要进行训练、推理、输出、验证等操作 2 model str/optional 模型的文件，可以是官方的预训练模型，也可以是训练自己模型的yaml文件 3 data str/optional 模型的地址，可以是文件的地址，也可以是配置好地址的yaml文件 4 epochs int 训练的轮次，将你的数据输入到模型里进行训练的次数 5 patience int 早停机制，当你的模型精度没有改进了就提前停止训练 6 batch int 我们输入的数据集会分解为多个子集，一次向模型里输入多少个子集 7 imgsz int/list 输入的图片的大小，可以是整数就代表图片尺寸为int*int，或者list分别代表宽和高[w，h] 8 save bool 是否保存模型以及预测结果 9 save_period int 在训练过程中多少次保存一次模型文件,就是生成的pt文件 10 cache bool 参数cache用于控制是否启用缓存机制。 11 device int/str/list/optional GPU设备的选择：cuda device=0 or device=0,1,2,3 or device=cpu 12 workers int 工作的线程，Windows系统一定要设置为0否则很可能会引起线程报错 13 name str/optional 模型保存的名字，结果会保存到\u0026rsquo;project/name\u0026rsquo; 目录下 14 exist_ok bool 如果模型存在的时候是否进行覆盖操作 15 prepetrained bool\n| 参数pretrained用于控制是否使用预训练模型。\n16| optimizer| str| 优化器的选择choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n17| verbose| bool| 用于控制在执行过程中是否输出详细的信息和日志。\n18| seed| int| 随机数种子，模型中涉及到随机的时候，根据随机数种子进行生成\n19| deterministic| bool| 用于控制是否启用确定性模式，在确定性模式下，算法的执行将变得可重复，即相同的输入将产生相同的输出\n20| single_cls| bool| 是否是单标签训练\n21| rect| bool| 当 rect 设置为 True 时，表示启用矩形训练或验证。矩形训练或验证是一种数据处理技术，其中在训练或验证过程中，输入数据会被调整为具有相同宽高比的矩形形状。\n22|\ncos_lr\n| bool| 控制是否使用余弦学习率调度器\n23| close_mosaic| int| 控制在最后几个 epochs 中是否禁用马赛克数据增强\n24| resume| bool| 用于从先前的训练检查点（checkpoint）中恢复模型的训练。\n25| amp| bool| 用于控制是否进行自动混合精度\n26| fraction| float| 用于指定训练数据集的一部分进行训练的比例。默认值为 1.0\n27| profile| bool| 用于控制是否在训练过程中启用 ONNX 和 TensorRT 的性能分析\n28| freeze| int/list/optinal| 用于指定在训练过程中冻结前 n 层或指定层索引的列表，以防止它们的权重更新。这对于迁移学习或特定层的微调很有用。\n5.1 训练的三种方式 5.1.1 方式一 我们可以通过命令直接进行训练在其中指定参数，但是这样的方式，我们每个参数都要在其中打出来。命令如下:\nyolo task=detect mode=train model=yolov11n.pt data=data.yaml batch=16 epochs=100 imgsz=640 workers=0 device=0\r需要注意的是如果你是Windows系统的电脑其中的Workers最好设置成0否则容易报线程的错误。\n5.1.2 方式二 通过指定cfg直接进行训练，我们配置好ultralytics/cfg/default.yaml这个文件之后，可以直接执行这个文件进行训练，这样就不用在命令行输入其它的参数了。\nyolo cfg=ultralytics/cfg/default.yaml\r5.1.3 方式三 （推荐，避免keyError错误.） 我们可以通过创建py文件来进行训练，这样的好处就是不用在终端上打命令，这也能省去一些工作量，我们在根目录下创建一个名字为train.py的文件，在其中输入代码\nimport warnings\rwarnings.filterwarnings('ignore')\rfrom ultralytics import YOLO\rif __name__ == '__main__':\rmodel = YOLO('yolo11.yaml')\r# 如何切换模型版本, 上面的ymal文件可以改为 yolov11s.yaml就是使用的v11s,\r# 类似某个改进的yaml文件名称为yolov11-XXX.yaml那么如果想使用其它版本就把上面的名称改为yolov11l-XXX.yaml即可（改的是上面YOLO中间的名字不是配置文件的）！\r# model.load('yolov11n.pt') # 是否加载预训练权重,科研不建议大家加载否则很难提升精度\rmodel.train(data=r\u0026quot;填写你数据集data.yaml文件的地址\u0026quot;,\r# 如果大家任务是其它的'ultralytics/cfg/default.yaml'找到这里修改task可以改成detect, segment, classify, pose\rcache=False,\rimgsz=640,\repochs=100,\rsingle_cls=False, # 是否是单类别检测\rbatch=4,\rclose_mosaic=0,\rworkers=0,\rdevice='0',\roptimizer='SGD', # using SGD 优化器 默认为auto建议大家使用固定的.\r# resume=, # 续训的话这里填写True, yaml文件的地方改为lats.pt的地址,需要注意的是如果你设置训练200轮次模型训练了200轮次是没有办法进行续训的.\ramp=True, # 如果出现训练损失为Nan可以关闭amp\rproject='runs/train',\rname='exp',\r)\r无论通过上述的哪一种方式在控制台输出如下图片的内容就代表着开始训练成功了！\n六、模型验证/测试 参数名 类型 参数讲解 1 val bool 用于控制是否在训练过程中进行验证/测试。 2 split str 用于指定用于验证/测试的数据集划分。可以选择 \u0026lsquo;val\u0026rsquo;、\u0026rsquo;test\u0026rsquo; 或 \u0026rsquo;train\u0026rsquo; 中的一个作为验证/测试数据集 3 save_json bool 用于控制是否将结果保存为 JSON 文件 4 save_hybird bool 用于控制是否保存标签和附加预测结果的混合版本 5 conf float/optional 用于设置检测时的目标置信度阈值 6 iou float 用于设置非极大值抑制（NMS）的交并比（IoU）阈值。 7 max_det int 用于设置每张图像的最大检测数。 8 half bool 用于控制是否使用半精度（FP16）进行推断。 9 dnn bool ，用于控制是否使用 OpenCV DNN 进行 ONNX 推断。 10 plots bool 用于控制在训练/验证过程中是否保存绘图结果。 验证我们划分的验证集/测试集的情况，也就是评估我们训练出来的best.pt模型好与坏\n命令行命令如下:\nyolo task=detect mode=val model=best.pt data=data.yaml device=0\r七、模型推理 我们训练好自己的模型之后，都会生成一个模型文件,保存在你设置的目录下,当我们再次想要实验该模型的效果之后就可以调用该模型进行推理了，我们也可以用官方的预训练权重来进行推理。\n推理的方式和训练一样我们这里就选一种来进行举例其它的两种方式都是一样的操作只是需要改一下其中的一些参数即可:\n参数讲解\n参数名 类型 参数讲解 0 source str/optinal 用于指定图像或视频的目录 1 show bool 用于控制是否在可能的情况下显示结果 2 save_txt bool 用于控制是否将结果保存为 .txt 文件 3 save_conf bool 用于控制是否在保存结果时包含置信度分数 4 save_crop bool 用于控制是否将带有结果的裁剪图像保存下来 5 show_labels bool 用于控制在绘图结果中是否显示目标标签 6 show_conf bool 用于控制在绘图结果中是否显示目标置信度分数 7 vid_stride int/optional 用于设置视频的帧率步长 8 stream_buffer bool 用于控制是否缓冲所有流式帧（True）或返回最新的帧（False） 9 line_width int/list[int]/optional 用于设置边界框的线宽度，如果缺失则自动设置 10 visualize bool 用于控制是否可视化模型的特征 11 augment bool 用于控制是否对预测源应用图像增强 12 agnostic_nms bool 用于控制是否使用无关类别的非极大值抑制（NMS） 13 classes int/list[int]/optional 用于按类别筛选结果 14 retina_masks bool 用于控制是否使用高分辨率分割掩码 15 boxes bool 用于控制是否在分割预测中显示边界框。 命令行命令如下:\nyolo task=detect mode=predict model=best.pt source=images device=0\r这里需要需要注意的是我们用模型进行推理的时候可以选择照片也可以选择一个视频的格式都可以。支持的视频格式有\nMP4（.mp4）：这是一种常见的视频文件格式，通常具有较高的压缩率和良好的视频质量\nAVI（.avi）：这是一种较旧但仍广泛使用的视频文件格式。它通常具有较大的文件大小\nMOV（.mov）：这是一种常见的视频文件格式，通常与苹果设备和QuickTime播放器相关\nMKV（.mkv）：这是一种开放的多媒体容器格式，可以容纳多个视频、音频和字幕轨道\nFLV（.flv）：这是一种用于在线视频传输的流式视频文件格式\n八、模型输出 当我们进行部署的时候可以进行文件导出，然后在进行部署。\nYOLOv8支持的输出格式有如下\n1. ONNX（Open Neural Network Exchange）：ONNX 是一个开放的深度学习模型表示和转换的标准。它允许在不同的深度学习框架之间共享模型，并支持跨平台部署。导出为 ONNX 格式的模型可以在支持 ONNX 的推理引擎中进行部署和推理。\n2. TensorFlow SavedModel：TensorFlow SavedModel 是 TensorFlow 框架的标准模型保存格式。它包含了模型的网络结构和参数，可以方便地在 TensorFlow 的推理环境中加载和使用。\n3. PyTorch JIT（Just-In-Time）：PyTorch JIT 是 PyTorch 的即时编译器，可以将 PyTorch 模型导出为优化的 Torch 脚本或 Torch 脚本模型。这种格式可以在没有 PyTorch 环境的情况下进行推理，并且具有更高的性能。\n4. Caffe Model：Caffe 是一个流行的深度学习框架，它使用自己的模型表示格式。导出为 Caffe 模型的文件可以在 Caffe 框架中进行部署和推理。\n5. TFLite（TensorFlow Lite）：TFLite 是 TensorFlow 的移动和嵌入式设备推理框架，支持在资源受限的设备上进行高效推理。模型可以导出为 TFLite 格式，以便在移动设备或嵌入式系统中进行部署。\n6. Core ML（Core Machine Learning）：Core ML 是苹果的机器学习框架，用于在 iOS 和 macOS 上进行推理。模型可以导出为 Core ML 格式，以便在苹果设备上进行部署。\n这些格式都提供了不同的优势和适用场景。选择合适的导出格式应该考虑到目标平台和部署环境的要求，以及所使用的深度学习框架的支持情况。\n模型输出的参数有如下\n参数名 类型 参数解释 0 format str 导出模型的格式 1 keras bool 表示是否使用Keras 2 optimize bool 用于在导出TorchScript模型时进行优化，以便在移动设备上获得更好的性能 3 int8 bool 用于在导出CoreML或TensorFlow模型时进行INT8量化 4 dynamic bool 用于在导出CoreML或TensorFlow模型时进行INT8量化 5 simplify bool 用于在导出ONNX模型时进行模型简化 6 opset int/optional 用于指定导出ONNX模型时的opset版本 7 workspace int 用于指定TensorRT模型的工作空间大小，以GB为单位 8 nms bool 用于在导出CoreML模型时添加非极大值抑制（NMS） 命令行命令如下:\nyolo task=detect mode=export model=best.pt format=onnx 到此为止本文的讲解就结束了,希望对大家对于YOLOv11模型理解有帮助，希望本文能够帮助到大家。\n","date":"0001-01-01T00:00:00Z","permalink":"https://nickk111.github.io/p/","title":""},{"content":"\rYOLOv11改进 | 主干/Backbone篇 | 目标检测网络FasterNeT轻量化网络助力yolov11改进（提高FPS和检测效率）-CSDN博客\rYOLOv11改进 | 主干/Backbone篇 | 目标检测网络FasterNeT轻量化网络助力yolov11改进（提高FPS和检测效率）\rSnu77\r已于 2024-11-01 11:02:19 修改\r阅读量1k\r收藏\r19\r点赞数\r13\r分类专栏：\rYOLOv11有效涨点专栏\r文章标签：\rYOLO\r目标检测\r深度学习\r计算机视觉\r人工智能\rpython\rYOLOv11\r于 2024-11-01 11:02:09 首次发布\r版权声明：本文为博主原创文章，遵循\rCC 4.0 BY-SA\r版权协议，转载请附上原文出处链接和本声明。\r本文链接：\rhttps://blog.csdn.net/java1314777/article/details/143419794\r版权\rYOLOv11有效涨点专栏\r专栏收录该内容\r100 篇文章\r0 订阅\r¥159.90\r¥299.90\r已订阅\r8折续费\r您已是超级会员，正在免费阅读会员专享内容\r查看更多超级会员权益\r一、本文介绍\r本文给大家带来的改进机制是FasterNet网络，将其用来替换我们的\r特征提取网络\r，其旨在\r提高计算速度而不牺牲准确性\r，特别是在视觉任务中。它通过一种称为\r部分卷积（PConv）\r的新技术来减少冗余计算和内存访问。这种方法使得FasterNet在多种设备上运行速度比其他网络快得多，同时在各种视觉任务中保持高准确率。经过我的实验该主干网络确实能够涨点在大中小三种物体检测上，大家可以在源代码中进行修改版本的使用。\r本文通过介绍其主要框架原理，然后教大家如何添加该网络结构到网络模型中。\r（本文内容可根据yolov11的N、S、M、L、X进行二次缩放，轻量化更上一层）。\r专栏回顾：\rYOLOv11改进系列专栏——本专栏持续复习各种顶会内容——科研必备\r二、FasterNet原理\r​\r论文地址：\r官方论文地址\r代码地址：\r官方代码地址\r​\r2.1 FasterNet的基本原理\rFasterNet\r是一种高效的神经网络架构，旨在\r提高计算速度而不牺牲准确性\r，特别是在视觉任务中。它通过一种称为\r部分卷积（PConv）\r的新技术来减少冗余计算和内存访问。这种方法使得FasterNet在多种设备上运行速度比其他网络快得多，同时在各种视觉任务中保持高准确率。例如，FasterNet在ImageNet-1k数据集上的表现超过了其他模型，如\rMobileViT-XXS\r，展现了其在速度和准确度方面的优势。\rFasterNet的基本原理可以总结为以下几点：\r1. 部分卷积（PConv）:\rFasterNet引入了部分卷积（PConv），这是一种新型的卷积方法，它通过只处理输入通道的一部分来减少计算量和内存访问。\r2. 加速神经网络\r: FasterNet利用PConv的优势，实现了在多种设备上比其他现有神经网络更快的运行速度，同时保持了较高的准确度。\r下面为大家展示的是\rFasterNet的整体架构\r。\r​\r它包括四个层次化的阶段，每个阶段由一系列FasterNet块组成，并由嵌入或合并层开头。最后三层用于特征分类。在每个FasterNet块中，PConv层之后是两个点状卷积（PWConv）层。为了保持特征多样性并实现更低的延迟，仅在中间层之后放置了\r归一化和激活层\r。\r2.2\r部分卷积\r部分卷积（PConv）\r是一种\r卷积神经网络中的操作\r，旨在提高计算效率。它通过\r只在输入特征图的一部分上执行卷积操作\r，而非传统卷积操作中的全面应用。这样，PConv可以减少不必要的计算和内存访问，因为它忽略了输入中认为是冗余的部分。这种方法特别适合在资源有限的设备上运行深度学习模型，因为它可以在不牺牲太多性能的情况下，显著降低计算需求。\r下面我为大家展示了FasterNet中的\r部分卷积（PConv）与传统卷积和深度卷积/分组卷积的比较\r：\r​\rPConv通过仅对输入通道的一小部分应用滤波器，同时保持其余通道不变，实现了快速和高效的特性提取。PConv的计算复杂度\r（FLOPs）\r低于常规卷积，但高于深度卷积/分组卷积，这样在减少计算资源的同时提高了运算性能。\r2.3\r加速神经网络\r加速神经网络\r主要通过优化计算路径、减少模型大小和复杂性、提高操作效率，以及使用高效的硬件实现等方式来降低模型的推理时间。这些方法包括\r简化网络层\r、\r使用更快的激活函数\r、\r采用量化技术\r将\r浮点运算转换为整数运算\r，以及使用特殊的算法来减少内存访问次数等。通过这些策略，可以在不损害模型准确性的前提下，使神经网络能够更快地处理数据和做出预测。\r三、FasterNet的核心代码\r# Copyright (c) Microsoft Corporation.# Licensed under the MIT License.import torchimport torch.nn as nnfrom timm.models.layers import DropPath, trunc_normal_from functools import partialfrom typing import Listfrom torch import Tensorimport copyimport os class Partial_conv3(nn.Module): def __init__(self, dim, n_div, forward): super().__init__() self.dim_conv3 = dim // n_div self.dim_untouched = dim - self.dim_conv3 self.partial_conv3 = nn.Conv2d(self.dim_conv3, self.dim_conv3, 3, 1, 1, bias=False) if forward == 'slicing': self.forward = self.forward_slicing elif forward == 'split_cat': self.forward = self.forward_split_cat else: raise NotImplementedError def forward_slicing(self, x: Tensor) -\u0026gt; Tensor: # only for inference x = x.clone() # !!! Keep the original input intact for the residual connection later x[:, :self.dim_conv3, :, :] = self.partial_conv3(x[:, :self.dim_conv3, :, :]) return x def forward_split_cat(self, x: Tensor) -\u0026gt; Tensor: # for training/inference x1, x2 = torch.split(x, [self.dim_conv3, self.dim_untouched], dim=1) x1 = self.partial_conv3(x1) x = torch.cat((x1, x2), 1) return x class MLPBlock(nn.Module): def __init__(self, dim, n_div, mlp_ratio, drop_path, layer_scale_init_value, act_layer, norm_layer, pconv_fw_type ): super().__init__() self.dim = dim self.mlp_ratio = mlp_ratio self.drop_path = DropPath(drop_path) if drop_path \u0026gt; 0. else nn.Identity() self.n_div = n_div mlp_hidden_dim = int(dim * mlp_ratio) mlp_layer: List[nn.Module] = [ nn.Conv2d(dim, mlp_hidden_dim, 1, bias=False), norm_layer(mlp_hidden_dim), act_layer(), nn.Conv2d(mlp_hidden_dim, dim, 1, bias=False) ] self.mlp = nn.Sequential(*mlp_layer) self.spatial_mixing = Partial_conv3( dim, n_div, pconv_fw_type ) if layer_scale_init_value \u0026gt; 0: self.layer_scale = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) self.forward = self.forward_layer_scale else: self.forward = self.forward def forward(self, x: Tensor) -\u0026gt; Tensor: shortcut = x x = self.spatial_mixing(x) x = shortcut + self.drop_path(self.mlp(x)) return x def forward_layer_scale(self, x: Tensor) -\u0026gt; Tensor: shortcut = x x = self.spatial_mixing(x) x = shortcut + self.drop_path( self.layer_scale.unsqueeze(-1).unsqueeze(-1) * self.mlp(x)) return x class BasicStage(nn.Module): def __init__(self, dim, depth, n_div, mlp_ratio, drop_path, layer_scale_init_value, norm_layer, act_layer, pconv_fw_type ): super().__init__() blocks_list = [ MLPBlock( dim=dim, n_div=n_div, mlp_ratio=mlp_ratio, drop_path=drop_path[i], layer_scale_init_value=layer_scale_init_value, norm_layer=norm_layer, act_layer=act_layer, pconv_fw_type=pconv_fw_type ) for i in range(depth) ] self.blocks = nn.Sequential(*blocks_list) def forward(self, x: Tensor) -\u0026gt; Tensor: x = self.blocks(x) return x class PatchEmbed(nn.Module): def __init__(self, patch_size, patch_stride, in_chans, embed_dim, norm_layer): super().__init__() self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_stride, bias=False) if norm_layer is not None: self.norm = norm_layer(embed_dim) else: self.norm = nn.Identity() def forward(self, x: Tensor) -\u0026gt; Tensor: x = self.norm(self.proj(x)) return x class PatchMerging(nn.Module): def __init__(self, patch_size2, patch_stride2, dim, norm_layer): super().__init__() self.reduction = nn.Conv2d(dim, 2 * dim, kernel_size=patch_size2, stride=patch_stride2, bias=False) if norm_layer is not None: self.norm = norm_layer(2 * dim) else: self.norm = nn.Identity() def forward(self, x: Tensor) -\u0026gt; Tensor: x = self.norm(self.reduction(x)) return x class FasterNet(nn.Module): def __init__(self, in_chans=3, num_classes=1000, embed_dim=96, depths=(1, 2, 8, 2), mlp_ratio=2., n_div=4, patch_size=4, patch_stride=4, patch_size2=2, # for subsequent layers patch_stride2=2, patch_norm=True, feature_dim=1280, drop_path_rate=0.1, layer_scale_init_value=0, norm_layer='BN', act_layer='RELU', fork_feat=True, init_cfg=None, pretrained=None, pconv_fw_type='split_cat', **kwargs): super().__init__() if norm_layer == 'BN': norm_layer = nn.BatchNorm2d else: raise NotImplementedError if act_layer == 'GELU': act_layer = nn.GELU elif act_layer == 'RELU': act_layer = partial(nn.ReLU, inplace=True) else: raise NotImplementedError if not fork_feat: self.num_classes = num_classes self.num_stages = len(depths) self.embed_dim = embed_dim self.patch_norm = patch_norm self.num_features = int(embed_dim * 2 ** (self.num_stages - 1)) self.mlp_ratio = mlp_ratio self.depths = depths # split image into non-overlapping patches self.patch_embed = PatchEmbed( patch_size=patch_size, patch_stride=patch_stride, in_chans=in_chans, embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None ) # stochastic depth decay rule dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] # build layers stages_list = [] for i_stage in range(self.num_stages): stage = BasicStage(dim=int(embed_dim * 2 ** i_stage), n_div=n_div, depth=depths[i_stage], mlp_ratio=self.mlp_ratio, drop_path=dpr[sum(depths[:i_stage]):sum(depths[:i_stage + 1])], layer_scale_init_value=layer_scale_init_value, norm_layer=norm_layer, act_layer=act_layer, pconv_fw_type=pconv_fw_type ) stages_list.append(stage) # patch merging layer if i_stage \u0026lt; self.num_stages - 1: stages_list.append( PatchMerging(patch_size2=patch_size2, patch_stride2=patch_stride2, dim=int(embed_dim * 2 ** i_stage), norm_layer=norm_layer) ) self.stages = nn.Sequential(*stages_list) self.fork_feat = fork_feat self.forward = self.forward_det # add a norm layer for each output self.out_indices = [0, 2, 4, 6] for i_emb, i_layer in enumerate(self.out_indices): if i_emb == 0 and os.environ.get('FORK_LAST3', None): raise NotImplementedError else: layer = norm_layer(int(embed_dim * 2 ** i_emb)) layer_name = f'norm{i_layer}' self.add_module(layer_name, layer) self.apply(self.cls_init_weights) self.init_cfg = copy.deepcopy(init_cfg) if self.fork_feat and (self.init_cfg is not None or pretrained is not None): self.init_weights() self.width_list = [i.size(1) for i in self.forward(torch.randn(1, 3, 640, 640))] def cls_init_weights(self, m): if isinstance(m, nn.Linear): trunc_normal_(m.weight, std=.02) if isinstance(m, nn.Linear) and m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, (nn.Conv1d, nn.Conv2d)): trunc_normal_(m.weight, std=.02) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, (nn.LayerNorm, nn.GroupNorm)): nn.init.constant_(m.bias, 0) nn.init.constant_(m.weight, 1.0) def forward_det(self, x: Tensor) -\u0026gt; Tensor: # output the features of four stages for dense prediction x = self.patch_embed(x) outs = [] for idx, stage in enumerate(self.stages): x = stage(x) if self.fork_feat and idx in self.out_indices: norm_layer = getattr(self, f'norm{idx}') x_out = norm_layer(x) outs.append(x_out) return outs if __name__ == \"__main__\": # Generating Sample image image_size = (1, 3, 640, 640) image = torch.rand(*image_size) # Model model = FasterNet() out = model(image) print(len(out))\r四、手把手教你添加FasterNet机制\r4.1 修改一\r第一步还是建立文件，我们找到如下ultralytics/nn文件夹下建立一个目录名字呢就是'Addmodules'文件夹(\r用群内的文件的话已经有了无需新建)\r！然后在其内部建立一个新的py文件将核心代码复制粘贴进去即可\r4.2 修改二\r第二步我们在该目录下创建一个新的py文件名字为'__init__.py'(\r用群内的文件的话已经有了无需新建)\r，然后在其内部导入我们的检测头如下图所示。\r4.3 修改三\r第三步我门中到如下文件'ultralytics/nn/tasks.py'进行导入和注册我们的模块(\r用群内的文件的话已经有了无需重新导入直接开始第四步即可)\r！\r从今天开始以后的教程就都统一成这个样子了，因为我默认大家用了我群内的文件来进行修改！！\r4.4 修改四\r添加如下两行代码！！！\r​\r4.5 修改五\r找到七百多行大概把具体看图片，按照图片来修改就行，添加红框内的部分，注意没有()只是函数名。\relif m in {自行添加对应的模型即可，下面都是一样的}: m = m(*args) c2 = m.width_list # 返回通道列表 backbone = True\r4.6 修改六\r下面的两个红框内都是需要改动的。\r​\rif isinstance(c2, list): m_ = m m_.backbone = True else: m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n \u0026gt; 1 else m(*args) # module t = str(m)[8:-2].replace('__main__.', '') # module type m.np = sum(x.numel() for x in m_.parameters()) # number params m_.i, m_.f, m_.type = i + 4 if backbone else i, f, t # attach index, 'from' index, type\r4.7 修改七\r如下的也需要修改，全部按照我的来。\r​\r代码如下把原先的代码替换了即可。\rif verbose: LOGGER.info(f'{i:\u0026gt;3}{str(f):\u0026gt;20}{n_:\u0026gt;3}{m.np:10.0f} {t:\u0026lt;45}{str(args):\u0026lt;30}') # print save.extend(x % (i + 4 if backbone else i) for x in ([f] if isinstance(f, int) else f) if x != -1) # append to savelist layers.append(m_) if i == 0: ch = [] if isinstance(c2, list): ch.extend(c2) if len(c2) != 5: ch.insert(0, 0) else: ch.append(c2)\r4.8 修改八\r修改八和前面的都不太一样，需要修改前向传播中的一个部分， 已经离开了parse_model方法了。\r可以在图片中开代码行数，没有离开task.py文件都是同一个文件。 同时这个部分有好几个前向传播都很相似，大家不要看错了，\r是70多行左右的！！！，同时我后面提供了代码，大家直接复制粘贴即可，有时间我针对这里会出一个视频。\r​​\r代码如下-\u0026gt;\rdef _predict_once(self, x, profile=False, visualize=False, embed=None): \"\"\" Perform a forward pass through the network. Args: x (torch.Tensor): The input tensor to the model. profile (bool): Print the computation time of each layer if True, defaults to False. visualize (bool): Save the feature maps of the model if True, defaults to False. embed (list, optional): A list of feature vectors/embeddings to return. Returns: (torch.Tensor): The last output of the model. \"\"\" y, dt, embeddings = [], [], [] # outputs for m in self.model: if m.f != -1: # if not from previous layer x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f] # from earlier layers if profile: self._profile_one_layer(m, x, dt) if hasattr(m, 'backbone'): x = m(x) if len(x) != 5: # 0 - 5 x.insert(0, None) for index, i in enumerate(x): if index in self.save: y.append(i) else: y.append(None) x = x[-1] # 最后一个输出传给下一层 else: x = m(x) # run y.append(x if m.i in self.save else None) # save output if visualize: feature_visualization(x, m.type, m.i, save_dir=visualize) if embed and m.i in embed: embeddings.append(nn.functional.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1)) # flatten if m.i == max(embed): return torch.unbind(torch.cat(embeddings, 1), dim=0) return x\r到这里就完成了修改部分，但是这里面细节很多，大家千万要注意不要替换多余的代码，导致报错，也不要拉下任何一部，都会导致运行失败，而且报错很难排查！！！很难排查！！！\r注意！！！ 额外的修改！\r关注我的其实都知道，我大部分的修改都是一样的，这个网络需要额外的修改一步，就是s一个参数，将下面的s改为640！！！即可完美运行！！\r打印计算量问题解决方案\r我们找到如下文件'ultralytics/utils/torch_utils.py'按照如下的图片进行修改，否则容易打印不出来计算量。\r​\r注意事项！！！\r如果大家在验证的时候报错形状不匹配的错误可以固定验证集的图片尺寸，方法如下 -\u0026gt;\r找到下面这个文件ultralytics/models/yolo/detect/train.py然后其中有一个类是DetectionTrainer class中的build_dataset函数中的一个参数rect=mode == 'val'改为rect=False\r五、FasterNet的yaml文件\r5.1 FasterNet的yaml文件\r训练信息：YOLO11-FasterNet summary: 351 layers, 2,384,163 parameters, 2,384,147 gradients, 5.6 GFLOPs\r使用说明：\r# 下面 [-1, 1, FasterNet, [0.25]] 参数位置的0.25是通道放缩的系数, YOLOv11N是0.25 YOLOv11S是0.5 YOLOv11M是1. YOLOv11l是1 YOLOv11是1.5大家根据自己训练的YOLO版本设定即可.\r# Ultralytics YOLO 🚀, AGPL-3.0 license# YOLO11 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect # Parametersnc: 80 # number of classesscales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n' # [depth, width, max_channels] n: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs s: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs m: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs l: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs x: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs # 下面 [-1, 1, FasterNet, [0.25]] 参数位置的0.25是通道放缩的系数, YOLOv11N是0.25 YOLOv11S是0.5 YOLOv11M是1. YOLOv11l是1 YOLOv11是1.5大家根据自己训练的YOLO版本设定即可.# YOLO11n backbonebackbone: # [from, repeats, module, args] - [-1, 1, FasterNet, [0.25]] # 0-4 P1/2 这里是四层大家不要被yaml文件限制住了思维，不会画图进群看视频. - [-1, 1, SPPF, [1024, 5]] # 5 - [-1, 2, C2PSA, [1024]] # 6 # YOLO11n headhead: - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]] - [[-1, 3], 1, Concat, [1]] # cat backbone P4 - [-1, 2, C3k2, [512, False]] # 9 - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]] - [[-1, 2], 1, Concat, [1]] # cat backbone P3 - [-1, 2, C3k2, [256, False]] # 12 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 9], 1, Concat, [1]] # cat head P4 - [-1, 2, C3k2, [512, False]] # 15 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 6], 1, Concat, [1]] # cat head P5 - [-1, 2, C3k2, [1024, True]] # 18 (P5/32-large) - [[12, 15, 18], 1, Detect, [nc]] # Detect(P3, P4, P5)\r5.2 训练文件的代码\r可以复制我的运行文件进行运行。\rimport warningswarnings.filterwarnings('ignore')from ultralytics import YOLO if __name__ == '__main__': model = YOLO('yolov8-MLLA.yaml') # 如何切换模型版本, 上面的ymal文件可以改为 yolov8s.yaml就是使用的v8s, # 类似某个改进的yaml文件名称为yolov8-XXX.yaml那么如果想使用其它版本就把上面的名称改为yolov8l-XXX.yaml即可（改的是上面YOLO中间的名字不是配置文件的）！ # model.load('yolov8n.pt') # 是否加载预训练权重,科研不建议大家加载否则很难提升精度 model.train(data=r\"C:\\Users\\Administrator\\PycharmProjects\\yolov5-master\\yolov5-master\\Construction Site Safety.v30-raw-images_latestversion.yolov8\\data.yaml\", # 如果大家任务是其它的'ultralytics/cfg/default.yaml'找到这里修改task可以改成detect, segment, classify, pose cache=False, imgsz=640, epochs=150, single_cls=False, # 是否是单类别检测 batch=16, close_mosaic=0, workers=0, device='0', optimizer='SGD', # using SGD # resume='runs/train/exp21/weights/last.pt', # 如过想续训就设置last.pt的地址 amp=True, # 如果出现训练损失为Nan可以关闭amp project='runs/train', name='exp', )\r六、成功运行记录\r下面是成功运行的截图，已经完成了有1个epochs的训练，图片太大截不全第2个epochs了。\r​\r七、本文总结\r到此本文的正式分享内容就结束了，在这里给大家推荐我的YOLOv11改进有效涨点专栏，本专栏目前为新开的平均质量分98分，后期我会根据各种最新的前沿顶会进行论文复现，也会对一些老的改进机制进行补充\r，\r如果大家觉得本文帮助到你了，订阅本专栏，关注后续更多的更新~\r专栏回顾：\rYOLOv11改进系列专栏——本专栏持续复习各种顶会内容——科研必备\r​​\r文章知识点与官方知识档案匹配，可进一步学习相关知识\rOpenCV技能树\rOpenCV中的深度学习\r图像分类\r29518\r人正在系统学习中\r确定要放弃本次机会？\r福利倒计时\r:\r:\r立减 ¥\r普通VIP年卡可用\r立即使用\r目录\r一、本文介绍\r二、FasterNet原理\r2.1 FasterNet的基本原理\r2.2 部分卷积\r2.3 加速神经网络\r三、FasterNet的核心代码\r四、手把手教你添加FasterNet机制\r4.1 修改一\r4.2 修改二\r4.3 修改三\r4.4 修改四\r4.5 修改五\r4.6 修改六\r4.7 修改七\r4.8 修改八\r注意！！！ 额外的修改！\r打印计算量问题解决方案\r注意事项！！！\r五、FasterNet的yaml文件\r5.1 FasterNet的yaml文件\r5.2 训练文件的代码\r六、成功运行记录\r七、本文总结\r确定\r取消\r举报\r选择你想要举报的内容（必选）\r内容涉黄\r政治相关\r内容抄袭\r涉嫌广告\r内容侵权\r侮辱谩骂\r样式问题\r其他\r原文链接（必填）\r请选择具体原因（必选）\r包含不实信息\r涉及个人隐私\r请选择具体原因（必选）\r侮辱谩骂\r诽谤\r请选择具体原因（必选）\r搬家样式\r博文样式\r补充说明（选填）\r取消\r确定\r","date":"0001-01-01T00:00:00Z","permalink":"https://nickk111.github.io/p/","title":""},{"content":"\rYOLOv11改进 | 主干/Backbone篇 | 视觉变换器SwinTransformer目标检测网络（ 适配yolov11全系列模型）_yolo11代码中transformer在框架中的哪个部分-CSDN博客\rYOLOv11改进 | 主干/Backbone篇 | 视觉变换器SwinTransformer目标检测网络（ 适配yolov11全系列模型）\rSnu77\r已于 2024-11-02 19:55:43 修改\r阅读量1.2k\r收藏\r32\r点赞数\r24\r分类专栏：\rYOLOv11有效涨点专栏\r文章标签：\rYOLO\r目标检测\r深度学习\r计算机视觉\r人工智能\rpython\rYOLOv11\r1024程序员节\r于 2024-10-24 22:45:32 首次发布\r版权声明：本文为博主原创文章，遵循\rCC 4.0 BY-SA\r版权协议，转载请附上原文出处链接和本声明。\r本文链接：\rhttps://blog.csdn.net/java1314777/article/details/143100866\r版权\rYOLOv11有效涨点专栏\r专栏收录该内容\r100 篇文章\r7 订阅\r¥159.90\r¥299.90\r已订阅\r8折续费\r您已是超级会员，正在免费阅读会员专享内容\r查看更多超级会员权益\r一、本文介绍\r本文给大家带来的改进机制是利用\rSwin Transformer\r替换\rYOLOv11中的骨干网络\r其是一个开创性的视觉\r变换器\r模型，它通过使用位移窗口来构建分层的特征图，有效地适应了计算机视觉任务。与传统的变换器模型不同，Swin Transformer的自注意力计算仅限于局部窗口内，使得\r计算复杂度与图像大小成线性关系，而非二次方\r。这种设计不仅提高了模型的效率，还保持了强大的特征提取能力。Swin Transformer的创新在于其能够在不同层次上捕捉图像的细节和全局信息，使其成为各种视觉任务的强大通用骨干网络。\r亲测在小目标检测和大尺度目标检测的数据集上都有涨点效果。\r（本文内容可根据yolov11的N、S、M、L、X进行二次缩放，轻量化更上一层）\r专栏回顾：\rYOLOv11改进系列专栏——本专栏持续复习各种顶会内容——科研必备\r目录\r一、本文介绍\r二、Swin Transformer原理\r2.1 Swin Transformer的基本原理\r2.2 层次化特征映射\r2.3 局部自注意力计算\r2.4 移动窗口自注意力\r2.5 移动窗口分区\r三、 Swin Transformer的完整代码\r四、手把手教你添加Swin Transformer网络结构\r修改一\r修改二\r修改三\r修改四\r修改五\r修改六\r修改七\r修改八\r五、Swin Transformer的yaml文件\r六、成功运行记录\r七、本文总结\r二、Swin Transformer原理\r论文地址：\r官方论文地址\r代码地址：\r官方代码、地址\r2.1\rSwin Transformer的基本原理\rSwin Transformer\r是一个新的视觉变换器，能够作为通用的计算机视觉骨干网络。这个模型解决了将Transformer从语言处理领域适应到视觉任务中的挑战，主要是因为这两个领域之间存在差异，例如视觉实体的尺度变化大，以及图像中像素的高分辨率与文本中的单词相比。下图对比展示了Swin Transformer与Vision Transformer (ViT)的不同之处，清楚地展示了Swin Transformer在\r构建特征映射和处理计算复杂度方面\r的创新优势。\r(a) Swin Transformer：\r提出的Swin Transformer通过在更深层次合并图像小块（灰色部分所示）来构建层次化的特征映射。在每个局部窗口（红色部分所示）内只计算自注意力，因此它对输入图像大小有线性的计算复杂度。它可以作为通用的骨干网络，用于图像分类和密集识别任务，如分割和检测。\r(b) Vision Transformer (ViT)：\r以前的视觉Transformer模型（如ViT）产生单一低分辨率的特征映射，并且由于全局自注意力的计算，其计算复杂度与输入图像大小呈二次方关系。\r我们可以将Swin Transformer的基本原理分为以下几点：\r1. 层次化特征映射：\rSwin Transformer通过合并图像的相邻小块（patches），在更深的Transformer层次中逐步构建层次化的特征映射。这样的层次化特征映射可以方便地利用密集预测的高级技术，如特征金字塔网络（Feature Pyramid Networks, FPN）或U-Net。\r2. 局部自注意力计算：\r为了实现线性计算复杂性，Swin Transformer在非重叠的局部窗口内计算自注意力，这些窗口是通过划分图像来创建的。每个窗口内的小块数量是固定的，因此计算复杂性与图像大小成线性关系。\r3. 移动窗口自注意力（Shifted Window based Self-Attention）：\r标准的Transformer架构在全局范围内计算自注意力，即计算一个标记与所有其他标记之间的关系。这种全局计算导致与标记数量成二次方的计算复杂性，不适用于许多需要处理大规模高维数据的视觉问题。Swin Transformer通过一个基于移动窗口的多头自注意力（MSA）模块取代了传统的MSA模块。每个Swin Transformer块由一个基于移动窗口的MSA模块组成，然后是两层带有GELU非线性的MLP，之前是LayerNorm（LN）层，之后是残差连接。\r4. 移动窗口分区：\r为了在连续的Swin Transformer块中引入跨窗口连接的同时保持非重叠窗口的有效计算，提出了一种移动窗口分区方法。这种方法在连续的块之间交替使用两种分区配置。第一个模块使用常规的窗口分区策略，然后下一个模块采用的窗口配置与前一层相比，通过移动窗口偏移了一定距离，从而实现窗口的交替。\r下图详细展示了\rSwin Transformer的架构和两个连续Swin Transformer块的设计\r。图中的W-MSA和SW-MSA分别代表带有常规和移动窗口配置的多头自注意力模块。这两种类型的注意力模块交替使用，允许模型在保持局部计算的同时，也能够捕捉更广泛的上下文信息。\r(a) 架构（Architecture）：\r图展示了Swin Transformer的四个阶段。每个阶段都包含若干Swin Transformer块。输入图像首先通过“Patch Partition”被划分成小块，并通过“Linear Embedding”转换成向量序列。各个阶段通过“Patch Merging”操作降低\r特征图\r的分辨率，同时增加特征维数（例如，第一阶段输出的特征维数为C，第二阶段为2C，依此类推）。\r(b) 两个连续的Swin Transformer块（Two Successive Swin Transformer Blocks）：\r每个Swin Transformer块由多头自注意力模块（W-MSA和SW-MSA）和多层感知机（MLP）组成，其中W-MSA使用常规窗口配置，而SW-MSA使用移动窗口配置。每个块内部，先是LayerNorm（LN）层，然后是自注意力模块，再是另一个LayerNorm层，最后是MLP。块之间通过残差连接进行连接，这样的设计可以避免深层网络中的梯度消失问题，并允许信息在网络中更有效地流动。\r2.2\r层次化特征映射\r层次化特征映射可以使\rSwin Transformer有效地处理不同分辨率的特征\r，并适用于各种视觉任务，如图像分类、对象检测和语义分割。这种层次化设计使Swin Transformer与以往基于Transformer的架构（这些架构产生单一分辨率的特征图并具有二次方复杂度）形成对比，后者不适合需要在像素级进行密集预测的视觉任务。Swin Transformer的\r层次化特征映射\r主要通过以下步骤实现：\r1. 分块和线性嵌入：\r首先，输入图像被分割成小块（通常是4x4像素大小），每个小块被视为一个“标记”，其特征是原始像素RGB值的串联。然后，一个线性嵌入层被应用于这些原始值特征，将其投影到任意维度（表示为C）。这些步骤构成了所谓的“第1阶段”。\r2. 分块合并：\r随着网络深入，通过合并层减少标记的数量，从而降低特征图的分辨率。例如，第一个合并层将每组2x2相邻小块的特征合并，并应用一个线性层到这些4C维度的串联特征上，这样做将标记的数量减少了4倍（分辨率降低了2倍），并将输出维度设为2C。这个过程在后续的“第2阶段”、“第3阶段”和“第4阶段”中重复，分别产生更低分辨率的输出。\r3. 层次化特征图：\r通过在更深的Transformer层合并相邻小块，Swin Transformer构建了层次化的特征映射。这些层次化特征映射允许模型方便地使用密集预测的高级技术，例如特征金字塔网络（FPN）或U-Net。\r4. 计算效率：\rSwin Transformer在非重叠的局部窗口内局部计算自注意力，从而实现了线性的计算复杂度。每个窗口中的小块数量是固定的，因此复杂度与图像大小成线性关系。\r2.3\r局部自注意力计算\rSwin Transformer的局部自注意力计算通过\r在小窗口内计算自注意力以及通过移动窗口在连续层之间引入跨窗口的信息流通，使得计算更加高效，同时保留了模型捕捉长距离依赖的能力。Swin Transformer中的局部自注意力计算我们可以通过以下方式实现：\r1. 替代标准多头自注意力模块：\rSwin Transformer使用基于移动窗口的多头自注意力（MSA）模块替代了传统Transformer块中的标准多头自注意力模块，其他层保持不变。每个Swin Transformer块由一个基于移动窗口的MSA模块组成，后跟一个两层的MLP，中间包含GELU非线性激活函数。在每个MSA模块和MLP之前都会应用一个LayerNorm（LN）层，每个模块之后都会应用残差连接。\r2. 在各个窗口内计算自注意力：\r在每一层中，采用常规的窗口分区方案，每个窗口内部独立计算自注意力。在下一层中，窗口分区会发生移动，形成新的窗口。新窗口中的自注意力计算会跨越之前层中窗口的边界，建立它们之间的连接。\r3. 非重叠窗口中的自注意力：\r为了有效的建模，Swin Transformer在非重叠的局部窗口内计算自注意力。这些窗口被安排以均匀非重叠的方式分割图像。假设每个窗口包含M×M个小块，全局MSA模块和基于窗口的MSA模块的计算复杂度分别为二次方和线性，当M固定时（默认设为7）。\r4. 循环位移和掩码机制：\r提出了一种通过循环位移来提高批量计算的效率的方法。通过这种位移，一个批次的窗口可能由几个在特征图中不相邻的子窗口组成，因此采用掩码机制限制在每个子窗口内计算自注意力。这种循环位移保持了批次窗口的数量与常规窗口分区相同。\r5. 窗口间的位移：\r为了在连续层之间实现更高效的硬件实现，Swin Transformer提出在连续层之间位移窗口，这样的位移允许跨窗口的连接，同时维持计算的高效性。\r6. 相对位置偏置：\r在计算自注意力时，Swin Transformer包括了相对位置偏置B，以增强模型对不同位置之间关系的学习能力。\r2.4\r移动窗口自注意力\r移动窗口自注意力是Swin Transformer设计的核心元素，它\r通过在局部窗口内计算自注意力\r并在连续层之间引入窗口位移，以实现高效的计算和强大的建模能力。在Swin Transformer论文中，移动窗口自注意力（shifted window self-attention）的\r主要特点\r包括：\r1. 替代多头自注意力模块：\r在Swin Transformer块中，标准的\r多头自注意力（MSA）模块\r被基于移动窗口的MSA模块替换。这种基于移动窗口的MSA模块后跟一个两层的MLP，中间有GELU非线性激活函数。每个MSA模块和MLP之前都会应用一个LayerNorm（LN）层，每个模块之后都会应用残差连接。\r2. 移动窗口分区：\r在连续的Swin Transformer块中，窗口分区策略在每一层之间交替。在某一层中，采用常规窗口分区，而在下一层中，窗口分区会发生移动，从而形成新的窗口。这种移动窗口分区方法能够跨越前一层中窗口的边界，提供窗口间的连接。\r3. 交替分区配置：\r移动窗口分区方法在连续的Swin Transformer块中交替使用两种分区配置。例如，第一个模块从左上角像素开始使用常规窗口分区策略，接着下一个模块采用的窗口配置将与前一层相比移动一定距离。\r4. 移动窗口自注意力的计算：\r移动窗口自注意力计算的有效性不仅在图像分类、目标检测和语义分割任务中得到了验证，而且它的实现也被证明在所有MLP架构中有益。\r5. 效率：\r相比于滑动窗口方法，移动窗口方法具有更低的延迟，但在建模能力上却相似。此外，移动窗口方法也有助于提高批量计算的效率。\r6. 连续块的计算：\r在移动窗口分区方法中，连续的Swin Transformer块的\r计算方式\r如下：\r，然后是MLP层，之后是\r。这里，\r和\r分别代表块l的(S)W-MSA模块和MLP模块的输出特征。\r下面我给大家展示了所提出的Swin Transformer架构中用于\r计算自注意力的移动窗口方法\r。\r在第l层（左侧），采用了常规窗口划分方案，并且在每个窗口内计算自注意力。在接下来的第l+1层（右侧），窗口划分被移动，结果在新的窗口中进行了自注意力计算。这些新窗口中的自注意力计算跨越了l层中之前窗口的边界，提供了它们之间的连接。这种移动窗口方法提高了效率，\r因为它限制了自注意力计算在非重叠的局部窗口内，同时允许窗口间的交叉连接。\r2.5\r移动窗口分区\r移动窗口分区是Swin Transformer中一项关键的创新，它\r通过在连续层之间交替窗口的分区方式\r，\r有效地促进了信息在窗口之间的流动，同时保持了处理高分辨率图像时的计算效率。下面我将通过图片\r解释如何使用循环位移来计算在移动窗口中的自注意力，以及如何高效地实施这一计算\r。\r（1）窗口分区（Window partition）：\r首先，图像被分成多个窗口。\r（2）循环位移（Cyclic shift）：\r接着，为了计算自注意力，窗口内的像素或特征会进行循环位移。这样可以将本来不相邻的像素或特征暂时性地排列到同一个窗口内，使得可以在局部窗口中计算原本跨窗口的自注意力。\r（3）掩码多头自注意力（Masked MSA）：\r在经过循环位移后，可以在这些临时形成的窗口上执行掩码多头自注意力操作，以此计算注意力得分和更新特征。\r（4）逆循环位移（Reverse cyclic shift）：\r完成自注意力计算后，特征会进行逆循环位移，恢复到它们原来在图像中的位置。\r三、 Swin Transformer的完整代码\rimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.utils.checkpoint as checkpointimport numpy as npfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_ __all__ = ['SwinTransformer'] class Mlp(nn.Module): \"\"\" Multilayer perceptron.\"\"\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() out_features = out_features or in_features hidden_features = hidden_features or in_features self.fc1 = nn.Linear(in_features, hidden_features) self.act = act_layer() self.fc2 = nn.Linear(hidden_features, out_features) self.drop = nn.Dropout(drop) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.drop(x) x = self.fc2(x) x = self.drop(x) return x def window_partition(x, window_size): \"\"\" Args: x: (B, H, W, C) window_size (int): window size Returns: windows: (num_windows*B, window_size, window_size, C) \"\"\" B, H, W, C = x.shape x = x.view(B, H // window_size, window_size, W // window_size, window_size, C) windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C) return windows def window_reverse(windows, window_size, H, W): \"\"\" Args: windows: (num_windows*B, window_size, window_size, C) window_size (int): Window size H (int): Height of image W (int): Width of image Returns: x: (B, H, W, C) \"\"\" B = int(windows.shape[0] / (H * W / window_size / window_size)) x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1) x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1) return x class WindowAttention(nn.Module): \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Args: dim (int): Number of input channels. window_size (tuple[int]): The height and width of the window. num_heads (int): Number of attention heads. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 \"\"\" def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.): super().__init__() self.dim = dim self.window_size = window_size # Wh, Ww self.num_heads = num_heads head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 # define a parameter table of relative position bias self.relative_position_bias_table = nn.Parameter( torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)) # 2*Wh-1 * 2*Ww-1, nH # get pair-wise relative position index for each token inside the window coords_h = torch.arange(self.window_size[0]) coords_w = torch.arange(self.window_size[1]) coords = torch.stack(torch.meshgrid([coords_h, coords_w])) # 2, Wh, Ww coords_flatten = torch.flatten(coords, 1) # 2, Wh*Ww relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :] # 2, Wh*Ww, Wh*Ww relative_coords = relative_coords.permute(1, 2, 0).contiguous() # Wh*Ww, Wh*Ww, 2 relative_coords[:, :, 0] += self.window_size[0] - 1 # shift to start from 0 relative_coords[:, :, 1] += self.window_size[1] - 1 relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1 relative_position_index = relative_coords.sum(-1) # Wh*Ww, Wh*Ww self.register_buffer(\"relative_position_index\", relative_position_index) self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) trunc_normal_(self.relative_position_bias_table, std=.02) self.softmax = nn.Softmax(dim=-1) def forward(self, x, mask=None): \"\"\" Forward function. Args: x: input features with shape of (num_windows*B, N, C) mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None \"\"\" B_, N, C = x.shape qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) q = q * self.scale attn = (q @ k.transpose(-2, -1)) relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view( self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous() # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias.unsqueeze(0) if mask is not None: nW = mask.shape[0] attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0) attn = attn.view(-1, self.num_heads, N, N) attn = self.softmax(attn) else: attn = self.softmax(attn) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B_, N, C) x = self.proj(x) x = self.proj_drop(x) return x class SwinTransformerBlock(nn.Module): \"\"\" Swin Transformer Block. Args: dim (int): Number of input channels. num_heads (int): Number of attention heads. window_size (int): Window size. shift_size (int): Shift size for SW-MSA. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set. drop (float, optional): Dropout rate. Default: 0.0 attn_drop (float, optional): Attention dropout rate. Default: 0.0 drop_path (float, optional): Stochastic depth rate. Default: 0.0 act_layer (nn.Module, optional): Activation layer. Default: nn.GELU norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm \"\"\" def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super().__init__() self.dim = dim self.num_heads = num_heads self.window_size = window_size self.shift_size = shift_size self.mlp_ratio = mlp_ratio assert 0 \u0026lt;= self.shift_size \u0026lt; self.window_size, \"shift_size must in 0-window_size\" self.norm1 = norm_layer(dim) self.attn = WindowAttention( dim, window_size=to_2tuple(self.window_size), num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop) self.drop_path = DropPath(drop_path) if drop_path \u0026gt; 0. else nn.Identity() self.norm2 = norm_layer(dim) mlp_hidden_dim = int(dim * mlp_ratio) self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop) self.H = None self.W = None def forward(self, x, mask_matrix): \"\"\" Forward function. Args: x: Input feature, tensor size (B, H*W, C). H, W: Spatial resolution of the input feature. mask_matrix: Attention mask for cyclic shift. \"\"\" B, L, C = x.shape H, W = self.H, self.W assert L == H * W, \"input feature has wrong size\" shortcut = x x = self.norm1(x) x = x.view(B, H, W, C) # pad feature maps to multiples of window size pad_l = pad_t = 0 pad_r = (self.window_size - W % self.window_size) % self.window_size pad_b = (self.window_size - H % self.window_size) % self.window_size x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b)) _, Hp, Wp, _ = x.shape # cyclic shift if self.shift_size \u0026gt; 0: shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)) attn_mask = mask_matrix.type(x.dtype) else: shifted_x = x attn_mask = None # partition windows x_windows = window_partition(shifted_x, self.window_size) # nW*B, window_size, window_size, C x_windows = x_windows.view(-1, self.window_size * self.window_size, C) # nW*B, window_size*window_size, C # W-MSA/SW-MSA attn_windows = self.attn(x_windows, mask=attn_mask) # nW*B, window_size*window_size, C # merge windows attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C) shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp) # B H' W' C # reverse cyclic shift if self.shift_size \u0026gt; 0: x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)) else: x = shifted_x if pad_r \u0026gt; 0 or pad_b \u0026gt; 0: x = x[:, :H, :W, :].contiguous() x = x.view(B, H * W, C) # FFN x = shortcut + self.drop_path(x) x = x + self.drop_path(self.mlp(self.norm2(x))) return x class PatchMerging(nn.Module): \"\"\" Patch Merging Layer Args: dim (int): Number of input channels. norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm \"\"\" def __init__(self, dim, norm_layer=nn.LayerNorm): super().__init__() self.dim = dim self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False) self.norm = norm_layer(4 * dim) def forward(self, x, H, W): \"\"\" Forward function. Args: x: Input feature, tensor size (B, H*W, C). H, W: Spatial resolution of the input feature. \"\"\" B, L, C = x.shape assert L == H * W, \"input feature has wrong size\" x = x.view(B, H, W, C) # padding pad_input = (H % 2 == 1) or (W % 2 == 1) if pad_input: x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2)) x0 = x[:, 0::2, 0::2, :] # B H/2 W/2 C x1 = x[:, 1::2, 0::2, :] # B H/2 W/2 C x2 = x[:, 0::2, 1::2, :] # B H/2 W/2 C x3 = x[:, 1::2, 1::2, :] # B H/2 W/2 C x = torch.cat([x0, x1, x2, x3], -1) # B H/2 W/2 4*C x = x.view(B, -1, 4 * C) # B H/2*W/2 4*C x = self.norm(x) x = self.reduction(x) return x class BasicLayer(nn.Module): \"\"\" A basic Swin Transformer layer for one stage. Args: dim (int): Number of feature channels depth (int): Depths of this stage. num_heads (int): Number of attention head. window_size (int): Local window size. Default: 7. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set. drop (float, optional): Dropout rate. Default: 0.0 attn_drop (float, optional): Attention dropout rate. Default: 0.0 drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0 norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False. \"\"\" def __init__(self, dim, depth, num_heads, window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False): super().__init__() self.window_size = window_size self.shift_size = window_size // 2 self.depth = depth self.use_checkpoint = use_checkpoint # build blocks self.blocks = nn.ModuleList([ SwinTransformerBlock( dim=dim, num_heads=num_heads, window_size=window_size, shift_size=0 if (i % 2 == 0) else window_size // 2, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop, drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer) for i in range(depth)]) # patch merging layer if downsample is not None: self.downsample = downsample(dim=dim, norm_layer=norm_layer) else: self.downsample = None def forward(self, x, H, W): \"\"\" Forward function. Args: x: Input feature, tensor size (B, H*W, C). H, W: Spatial resolution of the input feature. \"\"\" # calculate attention mask for SW-MSA Hp = int(np.ceil(H / self.window_size)) * self.window_size Wp = int(np.ceil(W / self.window_size)) * self.window_size img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device) # 1 Hp Wp 1 h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) cnt = 0 for h in h_slices: for w in w_slices: img_mask[:, h, w, :] = cnt cnt += 1 mask_windows = window_partition(img_mask, self.window_size) # nW, window_size, window_size, 1 mask_windows = mask_windows.view(-1, self.window_size * self.window_size) attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2) attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0)) for blk in self.blocks: blk.H, blk.W = H, W if self.use_checkpoint: x = checkpoint.checkpoint(blk, x, attn_mask) else: x = blk(x, attn_mask) if self.downsample is not None: x_down = self.downsample(x, H, W) Wh, Ww = (H + 1) // 2, (W + 1) // 2 return x, H, W, x_down, Wh, Ww else: return x, H, W, x, H, W class PatchEmbed(nn.Module): \"\"\" Image to Patch Embedding Args: patch_size (int): Patch token size. Default: 4. in_chans (int): Number of input image channels. Default: 3. embed_dim (int): Number of linear projection output channels. Default: 96. norm_layer (nn.Module, optional): Normalization layer. Default: None \"\"\" def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None): super().__init__() patch_size = to_2tuple(patch_size) self.patch_size = patch_size self.in_chans = in_chans self.embed_dim = embed_dim self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) if norm_layer is not None: self.norm = norm_layer(embed_dim) else: self.norm = None def forward(self, x): \"\"\"Forward function.\"\"\" # padding _, _, H, W = x.size() if W % self.patch_size[1] != 0: x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1])) if H % self.patch_size[0] != 0: x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0])) x = self.proj(x) # B C Wh Ww if self.norm is not None: Wh, Ww = x.size(2), x.size(3) x = x.flatten(2).transpose(1, 2) x = self.norm(x) x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww) return x class SwinTransformer(nn.Module): \"\"\" Swin Transformer backbone. A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows` - https://arxiv.org/pdf/2103.14030 Args: pretrain_img_size (int): Input image size for training the pretrained model, used in absolute postion embedding. Default 224. patch_size (int | tuple(int)): Patch size. Default: 4. in_chans (int): Number of input image channels. Default: 3. embed_dim (int): Number of linear projection output channels. Default: 96. depths (tuple[int]): Depths of each Swin Transformer stage. num_heads (tuple[int]): Number of attention head of each stage. window_size (int): Window size. Default: 7. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4. qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. drop_rate (float): Dropout rate. attn_drop_rate (float): Attention dropout rate. Default: 0. drop_path_rate (float): Stochastic depth rate. Default: 0.2. norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm. ape (bool): If True, add absolute position embedding to the patch embedding. Default: False. patch_norm (bool): If True, add normalization after patch embedding. Default: True. out_indices (Sequence[int]): Output from which stages. frozen_stages (int): Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False. \"\"\" def __init__(self, factor=0.5, depth_factor=0.5, pretrain_img_size=224, patch_size=4, in_chans=3, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2, norm_layer=nn.LayerNorm, ape=False, patch_norm=True, out_indices=(0, 1, 2, 3), frozen_stages=-1, use_checkpoint=False): super().__init__() embed_dim = int(embed_dim * factor) depths = [max(1, int(dim * depth_factor)) for dim in depths] self.pretrain_img_size = pretrain_img_size self.num_layers = len(depths) self.embed_dim = embed_dim self.ape = ape self.patch_norm = patch_norm self.out_indices = out_indices self.frozen_stages = frozen_stages # split image into non-overlapping patches self.patch_embed = PatchEmbed( patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None) # absolute position embedding if self.ape: pretrain_img_size = to_2tuple(pretrain_img_size) patch_size = to_2tuple(patch_size) patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1]] self.absolute_pos_embed = nn.Parameter(torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1])) trunc_normal_(self.absolute_pos_embed, std=.02) self.pos_drop = nn.Dropout(p=drop_rate) # stochastic depth dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] # stochastic depth decay rule # build layers self.layers = nn.ModuleList() for i_layer in range(self.num_layers): layer = BasicLayer( dim=int(embed_dim * 2 ** i_layer), depth=depths[i_layer], num_heads=num_heads[i_layer], window_size=window_size, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])], norm_layer=norm_layer, downsample=PatchMerging if (i_layer \u0026lt; self.num_layers - 1) else None, use_checkpoint=use_checkpoint) self.layers.append(layer) num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)] self.num_features = num_features # add a norm layer for each output for i_layer in out_indices: layer = norm_layer(num_features[i_layer]) layer_name = f'norm{i_layer}' self.add_module(layer_name, layer) self.width_list = [i.size(1) for i in self.forward(torch.randn(1, 3, 640, 640))] def forward(self, x): \"\"\"Forward function.\"\"\" x = self.patch_embed(x) Wh, Ww = x.size(2), x.size(3) if self.ape: # interpolate the position embedding to the corresponding size absolute_pos_embed = F.interpolate(self.absolute_pos_embed, size=(Wh, Ww), mode='bicubic') x = (x + absolute_pos_embed).flatten(2).transpose(1, 2) # B Wh*Ww C else: x = x.flatten(2).transpose(1, 2) x = self.pos_drop(x) outs = [] for i in range(self.num_layers): layer = self.layers[i] x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww) if i in self.out_indices: norm_layer = getattr(self, f'norm{i}') x_out = norm_layer(x_out) out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous() outs.append(out) return outs\r四、手把手教你添加Swin Transformer网络结构\r这个主干的网络结构添加起来算是所有的改进机制里最麻烦的了，因为有一些网略结构可以用yaml文件搭建出来，有一些网络结构其中的一些细节根本没有办法用yaml文件去搭建，用yaml文件去搭建会损失一些细节部分(而且一个网络结构设计很多细节的结构修改方式都不一样，一个一个去修改大家难免会出错)，所以这里让网络直接返回整个网络，然后修改部分 yolo代码以后就都以这种形式添加了，以后我提出的网络模型基本上都会通过这种方式修改，我也会进行一些模型细节改进。创新出新的网络结构大家直接拿来用就可以的。\r下面开始添加教程-\u0026gt;\r(同时每一个后面都有代码，大家拿来复制粘贴替换即可，但是要看好了不要复制粘贴替换多了)\r4.1 修改一\r第一还是建立文件，我们找到如下ultralytics/nn文件夹下建立一个目录名字呢就是'Addmodules'文件夹\r(用群内的文件的话已经有了无需新建)\r！\r然后在其内部建立一个新的py文件将核心代码复制粘贴进去即可。\r​\r4.2 修改二\r第二步我们在该目录下创建一个新的py文件名字为'__init__.py'(\r用群内的文件的话已经有了无需新建)\r，然后在其内部导入我们的检测头如下图所示。\r4.3 修改三\r第三步我门中到如下文件'ultralytics/nn/tasks.py'进行导入和注册我们的模块(\r用群内的文件的话已经有了无需重新导入直接开始第四步即可)\r！\r从今天开始以后的教程就都统一成这个样子了，因为我默认大家用了我群内的文件来进行修改！！\r4.4 修改四\r添加如下两行代码！！！\r​\r4.5 修改五\r找到七百多行大概把具体看图片，按照图片来修改就行，添加红框内的部分，注意没有()只是函数名。\r​\relif m in {自行添加对应的模型即可，下面都是一样的}: # 这段代码是自己添加的原代码中没有 m = m(*args) c2 = m.width_list # 返回通道列表 backbone = True\r4.6 修改六\r下面的两个红框内都是需要改动的。\r​\rif isinstance(c2, list): m_ = m m_.backbone = True else: m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n \u0026gt; 1 else m(*args) # module t = str(m)[8:-2].replace('__main__.', '') # module type m.np = sum(x.numel() for x in m_.parameters()) # number params m_.i, m_.f, m_.type = i + 4 if backbone else i, f, t # attach index, 'from' index, type\r4.7 修改七\r如下的也需要修改，全部按照我的来。\r​\r代码如下把原先的代码替换了即可。\rif verbose: LOGGER.info(f'{i:\u0026gt;3}{str(f):\u0026gt;20}{n_:\u0026gt;3}{m.np:10.0f} {t:\u0026lt;45}{str(args):\u0026lt;30}') # print save.extend(x % (i + 4 if backbone else i) for x in ([f] if isinstance(f, int) else f) if x != -1) # append to savelist layers.append(m_) if i == 0: ch = [] if isinstance(c2, list): ch.extend(c2) if len(c2) != 5: ch.insert(0, 0) else: ch.append(c2)\r4.8 修改八\r修改七和前面的都不太一样，需要修改前向传播中的一个部分， 已经离开了parse_model方法了。\r可以在图片中开代码行数，没有离开task.py文件都是同一个文件。 同时这个部分有好几个前向传播都很相似，大家不要看错了，\r是70多行左右的！！！，同时我后面提供了代码，大家直接复制粘贴即可，有时间我针对这里会出一个视频。\r​​\r代码如下-\u0026gt;\rdef _predict_once(self, x, profile=False, visualize=False, embed=None): \"\"\" Perform a forward pass through the network. Args: x (torch.Tensor): The input tensor to the model. profile (bool): Print the computation time of each layer if True, defaults to False. visualize (bool): Save the feature maps of the model if True, defaults to False. embed (list, optional): A list of feature vectors/embeddings to return. Returns: (torch.Tensor): The last output of the model. \"\"\" y, dt, embeddings = [], [], [] # outputs for m in self.model: if m.f != -1: # if not from previous layer x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f] # from earlier layers if profile: self._profile_one_layer(m, x, dt) if hasattr(m, 'backbone'): x = m(x) if len(x) != 5: # 0 - 5 x.insert(0, None) for index, i in enumerate(x): if index in self.save: y.append(i) else: y.append(None) x = x[-1] # 最后一个输出传给下一层 else: x = m(x) # run y.append(x if m.i in self.save else None) # save output if visualize: feature_visualization(x, m.type, m.i, save_dir=visualize) if embed and m.i in embed: embeddings.append(nn.functional.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1)) # flatten if m.i == max(embed): return torch.unbind(torch.cat(embeddings, 1), dim=0) return x\r到这里就完成了修改部分，但是这里面细节很多，大家千万要注意不要替换多余的代码，导致报错，也不要拉下任何一部，都会导致运行失败，而且报错很难排查！！！很难排查！！！\r4.9 修改九\r我们找到如下文件'ultralytics/utils/torch_utils.py'按照如下的图片进行修改，否则容易打印不出来计算量。\r​\r五、Swintransformer的yaml文件\r复制如下yaml文件进行运行！！！\r此版本训练信息：YOLO11-SwinTransformer summary: 325 layers, 2,514,792 parameters, 2,514,776 gradients, 6.1 GFLOPs\r使用说明：# 下面 [-1, 1, LSKNet, [0.25，0.5]] 参数位置的0.25是通道放缩的系数, YOLOv11N是0.25 YOLOv11S是0.5 YOLOv11M是1. YOLOv11l是1 YOLOv11是1.5大家根据自己训练的YOLO版本设定即可.\r# 0.5对应的是模型的深度系数\r# Ultralytics YOLO 🚀, AGPL-3.0 license# YOLO11 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect # Parametersnc: 80 # number of classesscales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n' # [depth, width, max_channels] n: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs s: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs m: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs l: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs x: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs # 下面 [-1, 1, SwinTransformer, [0.25，0.5]] 参数位置的0.25是通道放缩的系数, YOLOv11N是0.25 YOLOv11S是0.5 YOLOv11M是1. YOLOv11l是1 YOLOv11是1.5大家根据自己训练的YOLO版本设定即可. # 0.5对应的是模型的深度系数 # YOLO11n backbonebackbone: # [from, repeats, module, args] - [-1, 1, SwinTransformer, [0.25,0.5]] # 0-4 P1/2 - [-1, 1, SPPF, [1024, 5]] # 5 - [-1, 2, C2PSA, [1024]] # 6 # YOLO11n headhead: - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]] - [[-1, 3], 1, Concat, [1]] # cat backbone P4 - [-1, 2, C3k2, [512, False]] # 9 - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]] - [[-1, 2], 1, Concat, [1]] # cat backbone P3 - [-1, 2, C3k2, [256, False]] # 12 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 9], 1, Concat, [1]] # cat head P4 - [-1, 2, C3k2, [512, False]] # 15 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 6], 1, Concat, [1]] # cat head P5 - [-1, 2, C3k2, [1024, True]] # 18 (P5/32-large) - [[12, 15, 18], 1, Detect, [nc]] # Detect(P3, P4, P5)\r六、成功运行记录\r下面是成功运行的截图，已经完成了有1个epochs的训练，图片太大截不全第2个epochs了。\r​\r七、本文总结\r到此本文的正式分享内容就结束了，在这里给大家推荐我的YOLOv11改进有效涨点专栏，本专栏目前为新开的平均质量分98分，后期我会根据各种最新的前沿顶会进行论文复现，也会对一些老的改进机制进行补充，\r目前本专栏免费阅读(暂时，大家尽早关注不迷路~)，\r如果大家觉得本文帮助到你了，订阅本专栏，关注后续更多的更新~\r专栏回顾：\rYOLOv11改进系列专栏——本专栏持续复习各种顶会内容——科研必备\r​​\r文章知识点与官方知识档案匹配，可进一步学习相关知识\rOpenCV技能树\rOpenCV中的深度学习\r图像分类\r29518\r人正在系统学习中\r确定要放弃本次机会？\r福利倒计时\r:\r:\r立减 ¥\r普通VIP年卡可用\r立即使用\r目录\r一、本文介绍\r二、Swin Transformer原理\r2.1 Swin Transformer的基本原理\r2.2 层次化特征映射\r2.3 局部自注意力计算\r2.4 移动窗口自注意力\r2.5 移动窗口分区\r三、 Swin Transformer的完整代码\r四、手把手教你添加Swin Transformer网络结构\r4.1 修改一\r4.2 修改二\r4.3 修改三\r4.4 修改四\r4.5 修改五\r4.6 修改六\r4.7 修改七\r4.8 修改八\r4.9 修改九\r五、Swintransformer的yaml文件\r六、成功运行记录\r七、本文总结\r确定\r取消\r举报\r选择你想要举报的内容（必选）\r内容涉黄\r政治相关\r内容抄袭\r涉嫌广告\r内容侵权\r侮辱谩骂\r样式问题\r其他\r原文链接（必填）\r请选择具体原因（必选）\r包含不实信息\r涉及个人隐私\r请选择具体原因（必选）\r侮辱谩骂\r诽谤\r请选择具体原因（必选）\r搬家样式\r博文样式\r补充说明（选填）\r取消\r确定\r","date":"0001-01-01T00:00:00Z","permalink":"https://nickk111.github.io/p/","title":""},{"content":"YOLOv11改进 | Conv篇 | AKConv轻量级架构下的高效检测（附代码 + 修改方法 + 二次创新） 一、本文介绍 本文给大家带来的改进内容是 AKConv 是一种创新的 变核卷积 ，它旨在解决标准卷积操作中的固有缺陷（采样形状是固定的），AKConv的 核心思想 在于它为卷积核提供了任意数量的参数和任意采样形状，能够使用任意数量的参数（如1, 2, 3, 4, 5, 6, 7等）来提取特征，这在标准卷积和可变形卷积中并未实现​​。AKConv能够根据硬件环境，使卷积参数的数量呈线性增减 （ 非常适用于轻量化模型的读者）。 本文通过先介绍AKConv的基本网络结构和原理让大家对该卷积有一个大概的了解，然后教大家如何将该卷积添加到自己的网络结构中， 二次创新C3k2 。\n专栏回顾： YOLOv11改进系列专栏——本专栏持续复习各种顶会内容——科研必备\n目录\n一、本文介绍\n二、AKConv网络结构讲解\n2.1 AKConv的主要思想和改进\n2.1.1 灵活的卷积核设计\n2.1.2 初始采样坐标算法\n2.1.3 适应性采样位置调整\n2.1.4 线性增减卷积参数的数量\n三、AKConv的代码\n四、手把手教你添加AKConv\n4.1 修改一\n4.2 修改二\n4.3 修改三\n4.4 修改四\n4.5 AKConv的yaml文件和训练截图\n4.5.1 AKConv的yaml文件1\n4.5.2 AKConv的yaml文件2\n4.5.3 AKConv的训练过程截图\n五、AKConv可添加的位置\n5.1 推荐AKConv可添加的位置\n5.2 图示AKConv可添加的位置\n六、本文总结\n二、AKConv网络结构讲解 论文地址： ** ** 官方论文地址****\n代码地址： ** ** 官方代码地址****\n2.1 AKConv的主要思想和改进 AKConv的主要思想： AKConv（可变核卷积）主要提供一种灵活的卷积机制， 允许卷积核具有任意数量的参数和采样形状 。这种方法突破了传统卷积局限于固定局部窗口和固定采样形状的限制，从而使得卷积操作能够更加精准地适应不同数据集和不同位置的目标。\nAKConv的改进点：\n灵活的卷积核设计 ：AKConv允许卷积核具有任意数量的参数，这使得其可以根据实际需求调整大小和形状，从而更有效地适应目标的变化。\n初始采样坐标算法 ：针对不同大小的卷积核，AKConv提出了一种新的算法来生成初始采样坐标，这进一步增强了其在处理各种尺寸目标时的灵活性。\n适应性采样位置调整 ：为适应目标的不同变化，AKConv通过获得的偏移量调整不规则卷积核的采样位置，从而提高了特征提取的准确性。\n减少模型参数和计算开销 ：AKConv支持线性增减卷积参数的数量，有助于在硬件环境中优化性能，尤其适合于轻量级模型的应用。\n个人总结： 总的来说，AKConv通过其创新的可变核卷积设计，为卷积神经网络带来了显著的性能提升。其能够根据不同的数据集和目标灵活调整卷积核的大小和形状，从而实现更高效的特征提取。\n图片展示了AKConv结构的详细示意图， 并附上我个人的过程理解：\n1. 输入： 输入图像具有维度(C, H, W)，其中C是通道数，H和W分别是图像的高度和宽度。\n2. 初始采样形状： 这一步是AKConv特有的，它给出了卷积核的初始采样形状。\n3. 卷积操作： 使用Conv2d对输入图像执行卷积操作。\n4. 偏移： 通过学习得到的偏移量来调整初始采样形状。这一步是AKConv的关键，允许卷积核形状动态调整以适应图像的特征。\n5. 重采样： 根据调整后的采样形状对特征图进行重采样。\n6. 输出管道： 重采样后的特征图经过重塑、再次卷积、标准化，最后通过激活函数SiLU输出最终结果。\n底部的三行展示了采样坐标的变化：\n原始坐标：显示了卷积核在没有任何偏移的情况下的初始采样位置。 偏移：展示了学习到的偏移量，这些偏移量将应用于原始坐标。 修改后的坐标：应用偏移后的采样坐标。 总结： 官方这个图说明了AKConv如何为任意大小的卷积分配初始采样坐标，并通过可学习的偏移调整采样形状。与原始采样形状相比，每个位置的采样形状都通过重采样进行了改变，这使得AKConv可以根据图像内容动态调整其操作，为卷积网络提供了前所未有的灵活性和适应性。\n2.1.1 灵活的卷积核设计 AKConv中的灵活卷积核设计是一种创新的机制，旨在使卷积网络更加适应性和有效率。以下是其主要原理和机制的总结：\n主要原理\n任意参数数量 ：传统的卷积核通常具有固定的尺寸和形状，例如3x3或5x5的方形网格。而AKConv的核心原理是允许卷积核具有任意数量的参数。这意味着卷积核不再局限于标准的方形网格，而是可以根据图像特征和任务需求，采用更多样化和灵活的形状 (如下图所示，任意参数数量) 。\n自适应采样形状 ：在处理不同的图像和目标时，AKConv的卷积核能够自动调整其采样形状。这是通过引入一种新的坐标生成算法实现的，该算法能够为不同大小和形状的卷积核生成初始采样坐标 (如下图所示，自适应采样形状) 。\n工作机制\n初始坐标生成 ：AKConv首先通过其坐标生成算法确定卷积核的初始采样位置。这些位置不再是固定不变的，而是可以根据图像中的特征和目标动态变化。\n采样位置调整 ：为了更好地适应图像中目标的大小和形状变化，AKConv会根据目标的特点调整卷积核的采样位置。这种调整是通过添加偏移量来实现的，使得卷积操作更加灵活和适应性强。\n个人总结： 通过这种灵活的设计，AKConv能够有效地适应各种大小和形状的目标，提高了特征提取的准确性和效率。它在标准卷积核基础上引入了更多的灵活性和自适应性，从而使得卷积神经网络在处理复杂和多样化的图像数据时更为高效。这种灵活的卷积核设计不仅提升了模型的性能，还为减少模型参数和计算开销提供了可能， 特别是在轻量级模型的应用中显示出其优势。\n2.1.2 初始采样坐标算法 AKConv中的 初始采样坐标算法 是其核心特征之一，这个算法为AKConv的灵活性和适应性提供了基础。以下是该算法的主要原理和机制的概述：\n主要原理\n针对多样化尺寸的适应性 ：传统卷积操作通常使用固定尺寸的卷积核，这限制了其在处理不同尺寸和形状目标时的效果。AKConv的初始采样坐标算法旨在解决这一问题，通过允许卷积核适应不同大小的目标，增强其灵活性和有效性。\n动态采样坐标生成 ：该算法能够根据目标的尺寸和形状动态生成卷积核的初始采样坐标。这种动态生成方式使卷积核能够更精确地覆盖和处理图像中的不同区域，从而提高特征提取的精度。\n工作机制\n适应不同目标尺寸 ：对于每一个卷积操作，算法首先考虑目标的尺寸。基于这一信息，它生成一组初始坐标，这些坐标定义了卷积核将要采样的位置。\n灵活的坐标调整 ：生成的初始坐标不是固定不变的，而是可以根据图像中的特征动态调整。这意味着卷积核可以根据图像内容的不同而改变其采样策略，从而更有效地提取特征。\n个人总结： 通过引入这种初始采样坐标算法，AKConv能够更灵活地处理各种尺寸的目标，无论是大尺寸还是小尺寸的目标，都能得到更准确的特征提取。\n2.1.3 适应性采样位置调整 AKConv的适应性采样位置调整机制是其核心之一，该机制允许卷积核基于图像内容进行动态调整。这里是对这一机制的概述：\n动态采样调整 ：传统的卷积网络使用固定形状的卷积核在图像上滑动来提取特征，这种方法忽略了图像中对象形状和尺寸的多样性。AKConv采用一种新颖的方法，它允许卷积核的形状和位置根据图像内容动态调整，更好地匹配和覆盖目标区域。\n偏移量学习 ：在AKConv中，卷积核的位置可以通过学习到的偏移量来调整。在训练过程中，网络学习到对于特定图像和目标最有效的偏移量，以便在采样过程中自动调整卷积核的位置。\n提高特征提取准确性 ：通过这种自适应调整，AKConv能够更准确地对齐并提取图像中的关键特征，特别是当目标的形状和大小在不同图像中有所变化时。\n个人总结： AKConv的适应性采样位置调整为卷积网络提供了前所未有的灵活性和适应性，使其能够对各种不同形状和尺寸的目标实现更精确的特征提取。\n2.1.4 线性增减卷积参数的数量 AKConv通过其独特的设计减少了模型参数和计算开销实现方式如下：\n1. 线性参数调整： AKConv允许卷积核的参数数量根据需要进行线性调整。这与传统卷积网络中参数数量随着卷积核尺寸平方级增长的情况形成对比。通过支持参数数量的线性调整，AKConv能够根据任务需求和硬件能力灵活地增减模型的复杂度。\n2. 优化性能： 在硬件资源有限的环境中，AKConv能够通过减少不必要的参数来优化性能。这样不仅减轻了对存储和计算资源的需求，还有助于加快模型的训练和推理速度，同时降低能耗。\n3. 轻量级模型设计： AKConv特别适合于轻量级模型的设计，这类模型需要在保持高性能的同时，尽可能地减少参数数量。AKConv的这一特性使其成为设计紧凑而高效模型的理想选择，特别是在移动设备、嵌入式系统和物联网设备等资源受限的平台上。\n总结： AKConv通过支持卷积参数的线性增减，提供了一种在不牺牲性能的前提下，降低模型参数和计算开销的有效方法。这使得AKConv不仅在实现高精度的特征提取方面表现出色，而且在实际应用中具有显著的资源效率优势。\n三、AKConv的代码 在AKConv的官方代码中有一个版本的警告我给进行了一定的处理解决了，该代码的使用方式我们看章节四进行使用。\nimport math\rimport torch\rimport torch.nn as nn\rfrom einops import rearrange\r__all__ = ['AKConv', 'C3k2_AKConv']\rclass AKConv(nn.Module):\rdef __init__(self, inc, outc, num_param=2, stride=1, bias=None):\rsuper(AKConv, self).__init__()\rself.num_param = num_param\rself.stride = stride\rself.conv = nn.Sequential(nn.Conv2d(inc, outc, kernel_size=(num_param, 1), stride=(num_param, 1), bias=bias),\rnn.BatchNorm2d(outc),\rnn.SiLU()) # the conv adds the BN and SiLU to compare original Conv in YOLOv5.\rself.p_conv = nn.Conv2d(inc, 2 * num_param, kernel_size=3, padding=1, stride=stride)\rnn.init.constant_(self.p_conv.weight, 0)\rself.p_conv.register_full_backward_hook(self._set_lr)\r@staticmethod\rdef _set_lr(module, grad_input, grad_output):\rgrad_input = (grad_input[i] * 0.1 for i in range(len(grad_input)))\rgrad_output = (grad_output[i] * 0.1 for i in range(len(grad_output)))\rdef forward(self, x):\r# N is num_param.\roffset = self.p_conv(x)\rdtype = offset.data.type()\rN = offset.size(1) // 2\r# (b, 2N, h, w)\rp = self._get_p(offset, dtype)\r# (b, h, w, 2N)\rp = p.contiguous().permute(0, 2, 3, 1)\rq_lt = p.detach().floor()\rq_rb = q_lt + 1\rq_lt = torch.cat([torch.clamp(q_lt[..., :N], 0, x.size(2) - 1), torch.clamp(q_lt[..., N:], 0, x.size(3) - 1)],\rdim=-1).long()\rq_rb = torch.cat([torch.clamp(q_rb[..., :N], 0, x.size(2) - 1), torch.clamp(q_rb[..., N:], 0, x.size(3) - 1)],\rdim=-1).long()\rq_lb = torch.cat([q_lt[..., :N], q_rb[..., N:]], dim=-1)\rq_rt = torch.cat([q_rb[..., :N], q_lt[..., N:]], dim=-1)\r# clip p\rp = torch.cat([torch.clamp(p[..., :N], 0, x.size(2) - 1), torch.clamp(p[..., N:], 0, x.size(3) - 1)], dim=-1)\r# bilinear kernel (b, h, w, N)\rg_lt = (1 + (q_lt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_lt[..., N:].type_as(p) - p[..., N:]))\rg_rb = (1 - (q_rb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_rb[..., N:].type_as(p) - p[..., N:]))\rg_lb = (1 + (q_lb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_lb[..., N:].type_as(p) - p[..., N:]))\rg_rt = (1 - (q_rt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_rt[..., N:].type_as(p) - p[..., N:]))\r# resampling the features based on the modified coordinates.\rx_q_lt = self._get_x_q(x, q_lt, N)\rx_q_rb = self._get_x_q(x, q_rb, N)\rx_q_lb = self._get_x_q(x, q_lb, N)\rx_q_rt = self._get_x_q(x, q_rt, N)\r# bilinear\rx_offset = g_lt.unsqueeze(dim=1) * x_q_lt + \\\rg_rb.unsqueeze(dim=1) * x_q_rb + \\\rg_lb.unsqueeze(dim=1) * x_q_lb + \\\rg_rt.unsqueeze(dim=1) * x_q_rt\rx_offset = self._reshape_x_offset(x_offset, self.num_param)\rout = self.conv(x_offset)\rreturn out\r# generating the inital sampled shapes for the AKConv with different sizes.\rdef _get_p_n(self, N, dtype):\rbase_int = round(math.sqrt(self.num_param))\rrow_number = self.num_param // base_int\rmod_number = self.num_param % base_int\rp_n_x, p_n_y = torch.meshgrid(\rtorch.arange(0, row_number),\rtorch.arange(0, base_int))\rp_n_x = torch.flatten(p_n_x)\rp_n_y = torch.flatten(p_n_y)\rif mod_number \u0026gt; 0:\rmod_p_n_x, mod_p_n_y = torch.meshgrid(\rtorch.arange(row_number, row_number + 1),\rtorch.arange(0, mod_number))\rmod_p_n_x = torch.flatten(mod_p_n_x)\rmod_p_n_y = torch.flatten(mod_p_n_y)\rp_n_x, p_n_y = torch.cat((p_n_x, mod_p_n_x)), torch.cat((p_n_y, mod_p_n_y))\rp_n = torch.cat([p_n_x, p_n_y], 0)\rp_n = p_n.view(1, 2 * N, 1, 1).type(dtype)\rreturn p_n\r# no zero-padding\rdef _get_p_0(self, h, w, N, dtype):\rp_0_x, p_0_y = torch.meshgrid(\rtorch.arange(0, h * self.stride, self.stride),\rtorch.arange(0, w * self.stride, self.stride))\rp_0_x = torch.flatten(p_0_x).view(1, 1, h, w).repeat(1, N, 1, 1)\rp_0_y = torch.flatten(p_0_y).view(1, 1, h, w).repeat(1, N, 1, 1)\rp_0 = torch.cat([p_0_x, p_0_y], 1).type(dtype)\rreturn p_0\rdef _get_p(self, offset, dtype):\rN, h, w = offset.size(1) // 2, offset.size(2), offset.size(3)\r# (1, 2N, 1, 1)\rp_n = self._get_p_n(N, dtype)\r# (1, 2N, h, w)\rp_0 = self._get_p_0(h, w, N, dtype)\rp = p_0 + p_n + offset\rreturn p\rdef _get_x_q(self, x, q, N):\rb, h, w, _ = q.size()\rpadded_w = x.size(3)\rc = x.size(1)\r# (b, c, h*w)\rx = x.contiguous().view(b, c, -1)\r# (b, h, w, N)\rindex = q[..., :N] * padded_w + q[..., N:] # offset_x*w + offset_y\r# (b, c, h*w*N)\rindex = index.contiguous().unsqueeze(dim=1).expand(-1, c, -1, -1, -1).contiguous().view(b, c, -1)\rx_offset = x.gather(dim=-1, index=index).contiguous().view(b, c, h, w, N)\rreturn x_offset\r# Stacking resampled features in the row direction.\r@staticmethod\rdef _reshape_x_offset(x_offset, num_param):\rb, c, h, w, n = x_offset.size()\r# using Conv3d\r# x_offset = x_offset.permute(0,1,4,2,3), then Conv3d(c,c_out, kernel_size =(num_param,1,1),stride=(num_param,1,1),bias= False)\r# using 1 × 1 Conv\r# x_offset = x_offset.permute(0,1,4,2,3), then, x_offset.view(b,c×num_param,h,w) finally, Conv2d(c×num_param,c_out, kernel_size =1,stride=1,bias= False)\r# using the column conv as follow， then, Conv2d(inc, outc, kernel_size=(num_param, 1), stride=(num_param, 1), bias=bias)\rx_offset = rearrange(x_offset, 'b c h w n -\u0026gt; b c (h n) w')\rreturn x_offset\r​ ​ class Bottleneck(nn.Module): # Standard bottleneck with DCN def init(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5): # ch_in, ch_out, shortcut, groups, kernels, expand super().init() c_ = int(c2 * e) # hidden channels\nself.cv1 = Conv(c1, c_, k[0], 1)\rself.cv2 = AKConv(c_, c2, 3)\rself.add = shortcut and c1 == c2\rdef forward(self, x):\rreturn x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\rdef autopad(k, p=None, d=1): # kernel, padding, dilation\r\u0026quot;\u0026quot;\u0026quot;Pad to 'same' shape outputs.\u0026quot;\u0026quot;\u0026quot;\rif d \u0026gt; 1:\rk = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k] # actual kernel-size\rif p is None:\rp = k // 2 if isinstance(k, int) else [x // 2 for x in k] # auto-pad\rreturn p\r​ ​ class Conv(nn.Module): \u0026ldquo;\u0026ldquo;\u0026ldquo;Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\u0026rdquo;\u0026rdquo;\u0026rdquo; default_act = nn.SiLU() # default activation\ndef __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\r\u0026quot;\u0026quot;\u0026quot;Initialize Conv layer with given arguments including activation.\u0026quot;\u0026quot;\u0026quot;\rsuper().__init__()\rself.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\rself.bn = nn.BatchNorm2d(c2)\rself.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\rdef forward(self, x):\r\u0026quot;\u0026quot;\u0026quot;Apply convolution, batch normalization and activation to input tensor.\u0026quot;\u0026quot;\u0026quot;\rreturn self.act(self.bn(self.conv(x)))\rdef forward_fuse(self, x):\r\u0026quot;\u0026quot;\u0026quot;Perform transposed convolution of 2D data.\u0026quot;\u0026quot;\u0026quot;\rreturn self.act(self.conv(x))\r​ class C2f(nn.Module): \u0026ldquo;\u0026ldquo;\u0026ldquo;Faster Implementation of CSP Bottleneck with 2 convolutions.\u0026rdquo;\u0026rdquo;\u0026rdquo;\ndef __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\r\u0026quot;\u0026quot;\u0026quot;Initializes a CSP bottleneck with 2 convolutions and n Bottleneck blocks for faster processing.\u0026quot;\u0026quot;\u0026quot;\rsuper().__init__()\rself.c = int(c2 * e) # hidden channels\rself.cv1 = Conv(c1, 2 * self.c, 1, 1)\rself.cv2 = Conv((2 + n) * self.c, c2, 1) # optional act=FReLU(c2)\rself.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\rdef forward(self, x):\r\u0026quot;\u0026quot;\u0026quot;Forward pass through C2f layer.\u0026quot;\u0026quot;\u0026quot;\ry = list(self.cv1(x).chunk(2, 1))\ry.extend(m(y[-1]) for m in self.m)\rreturn self.cv2(torch.cat(y, 1))\rdef forward_split(self, x):\r\u0026quot;\u0026quot;\u0026quot;Forward pass using split() instead of chunk().\u0026quot;\u0026quot;\u0026quot;\ry = list(self.cv1(x).split((self.c, self.c), 1))\ry.extend(m(y[-1]) for m in self.m)\rreturn self.cv2(torch.cat(y, 1))\rclass C3(nn.Module):\r\u0026quot;\u0026quot;\u0026quot;CSP Bottleneck with 3 convolutions.\u0026quot;\u0026quot;\u0026quot;\rdef __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\r\u0026quot;\u0026quot;\u0026quot;Initialize the CSP Bottleneck with given channels, number, shortcut, groups, and expansion values.\u0026quot;\u0026quot;\u0026quot;\rsuper().__init__()\rc_ = int(c2 * e) # hidden channels\rself.cv1 = Conv(c1, c_, 1, 1)\rself.cv2 = Conv(c1, c_, 1, 1)\rself.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2)\rself.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))\rdef forward(self, x):\r\u0026quot;\u0026quot;\u0026quot;Forward pass through the CSP bottleneck with 2 convolutions.\u0026quot;\u0026quot;\u0026quot;\rreturn self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\rclass C3k(C3):\r\u0026quot;\u0026quot;\u0026quot;C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks.\u0026quot;\u0026quot;\u0026quot;\rdef __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):\r\u0026quot;\u0026quot;\u0026quot;Initializes the C3k module with specified channels, number of layers, and configurations.\u0026quot;\u0026quot;\u0026quot;\rsuper().__init__(c1, c2, n, shortcut, g, e)\rc_ = int(c2 * e) # hidden channels\r# self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\rself.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\rclass C3k2_AKConv(C2f):\r\u0026quot;\u0026quot;\u0026quot;Faster Implementation of CSP Bottleneck with 2 convolutions.\u0026quot;\u0026quot;\u0026quot;\rdef __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):\r\u0026quot;\u0026quot;\u0026quot;Initializes the C3k2 module, a faster CSP Bottleneck with 2 convolutions and optional C3k blocks.\u0026quot;\u0026quot;\u0026quot;\rsuper().__init__(c1, c2, n, shortcut, g, e)\rself.m = nn.ModuleList(\rC3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)\r)\r​ ​ if name == \u0026ldquo;main\u0026rdquo;: # Generating Sample image image_size = (1, 64, 224, 224) image = torch.rand(*image_size)\n# Model\rmodel = C3k2_AKConv(64, 64)\rout = model(image)\rprint(out.size())\r四、手把手教你添加AKConv 4.1 修改一 第一还是建立文件，我们找到如下ultralytics/nn文件夹下建立一个目录名字呢就是\u0026rsquo;Addmodules\u0026rsquo;文件夹( 用群内的文件的话已经有了无需新建) ！然后在其内部建立一个新的py文件将核心代码复制粘贴进去即可。\n4.2 修改二 第二步我们在该目录下创建一个新的py文件名字为\u0026rsquo;init.py\u0026rsquo;( 用群内的文件的话已经有了无需新建) ，然后在其内部导入我们的检测头如下图所示。\n4.3 修改三 第三步我门中到如下文件\u0026rsquo;ultralytics/nn/tasks.py\u0026rsquo;进行导入和注册我们的模块( 用群内的文件的话已经有了无需重新导入直接开始第四步即可) ！\n从今天开始以后的教程就都统一成这个样子了，因为我默认大家用了我群内的文件来进行修改！！\n4.4 修改四 按照我的添加在parse_model里添加即可。\n到此就修改完成了，大家可以复制下面的yaml文件运行。\n4.5 AKConv的yaml文件和训练截图 4.5.1 AKConv的yaml文件1 此版本训练信息：YOLO11-C3k2-AKConv summary: 353 layers, 2,471,697 parameters, 2,471,681 gradients, 6.2 GFLOPs\n# Ultralytics YOLO 🚀, AGPL-3.0 license\r# YOLO11 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect\r# Parameters\rnc: 80 # number of classes\rscales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\r# [depth, width, max_channels]\rn: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs\rs: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\rm: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs\rl: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs\rx: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs\r# YOLO11n backbone\rbackbone:\r# [from, repeats, module, args]\r- [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\r- [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\r- [-1, 2, C3k2_AKConv, [256, False, 0.25]]\r- [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\r- [-1, 2, C3k2_AKConv, [512, False, 0.25]]\r- [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\r- [-1, 2, C3k2_AKConv, [512, True]]\r- [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\r- [-1, 2, C3k2_AKConv, [1024, True]]\r- [-1, 1, SPPF, [1024, 5]] # 9\r- [-1, 2, C2PSA, [1024]] # 10\r# YOLO11n head\rhead:\r- [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]]\r- [[-1, 6], 1, Concat, [1]] # cat backbone P4\r- [-1, 2, C3k2_AKConv, [512, False]] # 13\r- [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]]\r- [[-1, 4], 1, Concat, [1]] # cat backbone P3\r- [-1, 2, C3k2_AKConv, [256, False]] # 16 (P3/8-small)\r- [-1, 1, Conv, [256, 3, 2]]\r- [[-1, 13], 1, Concat, [1]] # cat head P4\r- [-1, 2, C3k2_AKConv, [512, False]] # 19 (P4/16-medium)\r- [-1, 1, Conv, [512, 3, 2]]\r- [[-1, 10], 1, Concat, [1]] # cat head P5\r- [-1, 2, C3k2_AKConv, [1024, True]] # 22 (P5/32-large)\r- [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\r4.5.2 AKConv的yaml文件2 此版本训练信息：YOLO11-AKConv summary: 337 layers, 2,173,923 parameters, 2,173,907 gradients, 5.5 GFLOPs\n# Ultralytics YOLO 🚀, AGPL-3.0 license\r# YOLO11 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect\r# Parameters\rnc: 80 # number of classes\rscales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\r# [depth, width, max_channels]\rn: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs\rs: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\rm: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs\rl: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs\rx: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs\r# YOLO11n backbone\rbackbone:\r# [from, repeats, module, args]\r- [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\r- [-1, 1, AKConv, [128, 3, 2]] # 1-P2/4\r- [-1, 2, C3k2, [256, False, 0.25]]\r- [-1, 1, AKConv, [256, 3, 2]] # 3-P3/8\r- [-1, 2, C3k2, [512, False, 0.25]]\r- [-1, 1, AKConv, [512, 3, 2]] # 5-P4/16\r- [-1, 2, C3k2, [512, True]]\r- [-1, 1, AKConv, [1024, 3, 2]] # 7-P5/32\r- [-1, 2, C3k2, [1024, True]]\r- [-1, 1, SPPF, [1024, 5]] # 9\r- [-1, 2, C2PSA, [1024]] # 10\r# YOLO11n head\rhead:\r- [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]]\r- [[-1, 6], 1, Concat, [1]] # cat backbone P4\r- [-1, 2, C3k2, [512, False]] # 13\r- [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]]\r- [[-1, 4], 1, Concat, [1]] # cat backbone P3\r- [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\r- [-1, 1, AKConv, [256, 3, 2]]\r- [[-1, 13], 1, Concat, [1]] # cat head P4\r- [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\r- [-1, 1, AKConv, [512, 3, 2]]\r- [[-1, 10], 1, Concat, [1]] # cat head P5\r- [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\r- [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\r4.5.3 AKConv的训练过程截图 下面是添加了 AKConv 的训练截图。\n​​​​\n五、AKConv可添加的位置 5.1 推荐AKConv可添加的位置 AKConv是一种即插即用的模块\n文字大家可能看我描述不太懂，大家可以看下面的网络结构图中我进行了标注。\n5.2 图示 AKConv 可添加的位置 ​​​​\n","date":"0001-01-01T00:00:00Z","permalink":"https://nickk111.github.io/p/","title":""},{"content":"YOLOv11改进 | 主干/Backbone篇 | 视觉变换器SwinTransformer目标检测网络（ 适配yolov11全系列模型） 一、本文介绍 本文给大家带来的改进机制是利用 Swin Transformer 替换 YOLOv11中的骨干网络 其是一个开创性的视觉变换器模型，它通过使用位移窗口来构建分层的特征图，有效地适应了计算机视觉任务。与传统的变换器模型不同，Swin Transformer的自注意力计算仅限于局部窗口内，使得 计算复杂度与图像大小成线性关系，而非二次方 。这种设计不仅提高了模型的效率，还保持了强大的特征提取能力。Swin Transformer的创新在于其能够在不同层次上捕捉图像的细节和全局信息，使其成为各种视觉任务的强大通用骨干网络。 亲测在小目标检测和大尺度目标检测的数据集上都有涨点效果。\n（本文内容可根据yolov11的N、S、M、L、X进行二次缩放，轻量化更上一层）\n专栏回顾： ** ** ** ** ** ** YOLOv11改进系列专栏——本专栏持续复习各种顶会内容——科研必备************\n目录\n一、本文介绍\n二、Swin Transformer原理\n2.1 Swin Transformer的基本原理\n2.2 层次化特征映射\n2.3 局部自注意力计算\n2.4 移动窗口自注意力\n2.5 移动窗口分区\n三、 Swin Transformer的完整代码\n四、手把手教你添加Swin Transformer网络结构\n修改一\n修改二\n修改三\n修改四\n修改五\n修改六\n修改七\n修改八\n","date":"0001-01-01T00:00:00Z","permalink":"https://nickk111.github.io/p/","title":""},{"content":"使用Labelimg制作自己的YOLO数据集 目标检测常用标注工具-LabelImg的使用 目前我们的很多教程都是围绕目标检测展开的，一般情况下我都提供了我标注好的数据集，但是有的小伙伴需要根据自己的数据集来进行标注，这个时候掌握其中一种标注工具的使用显得至关重要。其中在计算机视觉的任务中，常用的标注工具有两类，一类是labelimg，主要是用于目标检测类数据的标注，一类是labelme，常用于分割任务的标注。考虑到平时大家的任务以检测为主，所以本期我们主要讲的是标注软件labelimg的使用。\n安装labelimg labelimg的安装非常简单，只需要执行对应的pip指令即可。\n请保证你在安装之前已经安装好了miniconda和pycharm，如果这里还没有完成，请看这期视频：【2024毕设系列】Anaconda和Pycharm如何使用_哔哩哔哩_bilibili\n首先我们需要创建一个对应的用于这个标注功能的虚拟环境，请执行下面两个指令完成虚拟环境的创建和激活。\nconda create -n labelimg_env python==3.8.5\rconda activate labelimg_env\r激活环境之后请在命令行中安装对应的labelimg软件。\npip install labelimg\r之后再命令行中输入labelimg即可启动标注软件，如下图所示。\n准备你的图像数据集 一个完整的目标检测的数据集由图像和标签构成，所以在开始之前。你需要先准备好你的图像数据集，这里的图像数据集可以是你从视频中进行截取的，或者是你从你的课题组或者是你从你的项目中获取的。总之，你会有一堆的图像放在一个文件夹下。\n注意，你存放图像的路径最好只包含英文，中文可能导致后续你读取图像的时候出现乱码，为了能够让你的图像不出现乱码的问题，我这里写了一段脚本，你可以将你的图像进行事先的重命名，保证你的图像路径不会出现错误。\n# -*-coding:utf-8 -*-\r\u0026quot;\u0026quot;\u0026quot;\r#-------------------------------\r# @Author : 肆十二\r# @QQ : 3045834499 可定制毕设\r#-------------------------------\r# @File : step2_get_names.py\r# @Description: 文件描述\r# @Software : PyCharm\r# @Time : 2024/2/14 13:20\r#-------------------------------\r\u0026quot;\u0026quot;\u0026quot;\rimport os\rimport os.path as osp\rimport numpy as np\rimport cv2\r​ def cv_imread_chinese(file_path): cv_img = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), cv2.IMREAD_COLOR) return cv_img\n​ def folder_rename(src_folder_path, target_folder_path): os.makedirs(target_folder_path, exist_ok=True) file_names = os.listdir(src_folder_path) for i, file_name in enumerate(file_names): print(\u0026quot;{}:{}\u0026quot;.format(i, file_name)) src_name= osp.join(src_folder_path, file_name) src_img = cv_imread_chinese(src_name) target_path = osp.join(target_folder_path, \u0026ldquo;yolo_data_{}.jpg\u0026rdquo;.format(i)) cv2.imwrite( target_path,src_img ) # os.rename(src_name, target_name)\nif __name__ == '__main__':\r# 脚本应该生成在一个新的目录下，防止出错\rfolder_rename(\u0026quot;test_data/中文路径\u0026quot;, \u0026quot;test_data/english_path\u0026quot;)\r​\n有了图像数据集之后，我们就可以开始进行标注了, 这里我放了一段黑神话悟空中的图像数据集，一会我们将会使用这个图像数据集来完成标注的任务。\n使用labelimg 在目标检测的领域中，一般情况下你将会接触到三个格式的数据，分别是txt格式的yolo数据，xml格式的voc数据集以及json格式的coco数据，他们在目标检测数据的表示上面各有不同。一般情况下，为了训练方便，大家选择常用的coco数据，但是为了灵活性考虑的话， 可能是标注为voc格式数据更好，因为yolo格式中的数据label的id是固定的，这将会导致后面增加类别或者减少类别的时候会出现问题。下面是一个典型的标注区域，在标注之前请你先进行一下自动保存的设置，这样的话就不用反复的进行保存的确认了。\n下面有一些常用的快捷键，使用这些快捷键可以帮助你快速进行标注。\n快捷键 功能 Ctrl + u Load all of the images from a directory Ctrl + r Change the default annotation target dir Ctrl + s Save Ctrl + d Copy the current label and rect box Ctrl + Shift + d Delete the current image Space Flag the current image as verified w Create a rect box d Next image a Previous image del Delete the selected rect box Ctrl++ Zoom in Ctrl– Zoom out ↑→↓← Keyboard arrows to move selected rect box 使用yolo的格式进行标注 如果使用yolo格式进行标注，你可以提前定义好标签然后进行加载。\npython3 labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\rYOLO（You Only Look Once）是一种广泛用于目标检测的算法，YOLO的训练数据格式对于构建自定义数据集非常重要。YOLO的标签文件通常是 .txt 格式，并且每个图像文件都对应一个同名的标签文件。这些标签文件中存储了图像中物体的类别和位置信息。\nYOLO数据格式说明： 图像文件 ： * 图像文件通常是 `.jpg`、`.png` 或其他常见的图像格式。\r* 图像的尺寸和分辨率可以根据数据集调整，但在训练前图像通常会被缩放到固定尺寸（例如 YOLOv3 的默认输入是 416x416）。\r标签文件 ： * 标签文件是纯文本文件，扩展名为 `.txt`。\r* 每个图像文件有一个对应的 `.txt` 文件，文件名与图像文件名相同（但扩展名不同）。\r* 标签文件中每一行代表一个目标，每一行包含以下五个值：\r\u0026lt;object-class\u0026gt; \u0026lt;x_center\u0026gt; \u0026lt;y_center\u0026gt; \u0026lt;width\u0026gt; \u0026lt;height\u0026gt;\r* **object-class** : 物体的类别ID，从 0 开始。假设数据集中有 3 类物体，类别编号会是 0、1、2 等。\r* **x_center** : 物体边界框中心点的 x 坐标，相对于图像的宽度进行归一化，范围为 0 到 1 之间（例如：0.5 表示在图像的正中间）。\r* **y_center** : 物体边界框中心点的 y 坐标，相对于图像的高度进行归一化，范围为 0 到 1 之间。\r* **width** : 边界框的宽度，相对于图像的宽度进行归一化，范围为 0 到 1 之间。\r* **height** : 边界框的高度，相对于图像的高度进行归一化，范围为 0 到 1 之间。\r具体例子： 假设有一个图像文件 dog.jpg，其对应的标签文件 dog.txt 内容如下：\n0.5 0.5 0.4 0.6\r0.3 0.3 0.2 0.3\r第一行：物体类别为 0（假设是狗），其边界框中心位于图像的正中间 (0.5, 0.5)，宽度为图像的 40%（0.4），高度为图像的 60%（0.6）。 第二行：物体类别为 1（假设是猫），其边界框中心位于 (0.3, 0.3)，宽度为图像的 20%（0.2），高度为图像的 30%（0.3）。 YOLO格式的注意事项： 所有的坐标和尺寸都需要归一化为相对于图像宽度和高度的值。即 (x_center, y_center, width, height) 都是 0 到 1 之间的小数。 物体类别ID是从 0 开始的整数。 如果一张图片中有多个物体，标签文件中每个物体都占据一行。 类别文件： 通常，YOLO会有一个额外的类别文件，通常是 classes.names 或 obj.names，该文件列出所有的类别名称，每行一个类别。例如：\ndog\rcat\rcar\r这个文件的顺序和标签文件中的 object-class 对应。\n使用voc的格式进行标注 Pascal VOC（Visual Object Classes）是目标检测领域非常流行的数据集格式，尤其在早期目标检测研究中被广泛使用。Pascal VOC 数据集格式与 YOLO 格式不同，VOC 使用 XML 文件来标注目标检测数据。这些 XML 文件基于 PASCAL VOC Challenge 的标准，且结构化信息较为丰富，便于扩展。\nVOC数据格式说明 每个图像对应一个 XML 标注文件，格式为 Pascal VOC 定义的 XML 文件。文件结构描述了图像中物体的类别、位置（使用未归一化的像素坐标）、难度等信息。\nPascal VOC数据结构 图像文件 ：\n图像文件通常为 .jpg 或 .png 格式，存储在 JPEGImages 文件夹中。 标签文件 ：\n标签文件是 XML 格式的，通常存储在 Annotations 文件夹中，每个图像文件对应一个 XML 标签文件，文件名与图像名相同。 主要的 XML 标签说明 下面是一个 Pascal VOC 的标签文件的例子 2007_000027.xml：\n\u0026lt;annotation\u0026gt;\r\u0026lt;folder\u0026gt;VOC2007\u0026lt;/folder\u0026gt;\r\u0026lt;filename\u0026gt;2007_000027.jpg\u0026lt;/filename\u0026gt;\r\u0026lt;size\u0026gt;\r\u0026lt;width\u0026gt;486\u0026lt;/width\u0026gt;\r\u0026lt;height\u0026gt;500\u0026lt;/height\u0026gt;\r\u0026lt;depth\u0026gt;3\u0026lt;/depth\u0026gt;\r\u0026lt;/size\u0026gt;\r\u0026lt;object\u0026gt;\r\u0026lt;name\u0026gt;dog\u0026lt;/name\u0026gt;\r\u0026lt;pose\u0026gt;Left\u0026lt;/pose\u0026gt;\r\u0026lt;truncated\u0026gt;1\u0026lt;/truncated\u0026gt;\r\u0026lt;difficult\u0026gt;0\u0026lt;/difficult\u0026gt;\r\u0026lt;bndbox\u0026gt;\r\u0026lt;xmin\u0026gt;48\u0026lt;/xmin\u0026gt;\r\u0026lt;ymin\u0026gt;240\u0026lt;/ymin\u0026gt;\r\u0026lt;xmax\u0026gt;195\u0026lt;/xmax\u0026gt;\r\u0026lt;ymax\u0026gt;371\u0026lt;/ymax\u0026gt;\r\u0026lt;/bndbox\u0026gt;\r\u0026lt;/object\u0026gt;\r\u0026lt;object\u0026gt;\r\u0026lt;name\u0026gt;person\u0026lt;/name\u0026gt;\r\u0026lt;pose\u0026gt;Unspecified\u0026lt;/pose\u0026gt;\r\u0026lt;truncated\u0026gt;0\u0026lt;/truncated\u0026gt;\r\u0026lt;difficult\u0026gt;0\u0026lt;/difficult\u0026gt;\r\u0026lt;bndbox\u0026gt;\r\u0026lt;xmin\u0026gt;8\u0026lt;/xmin\u0026gt;\r\u0026lt;ymin\u0026gt;12\u0026lt;/ymin\u0026gt;\r\u0026lt;xmax\u0026gt;352\u0026lt;/xmax\u0026gt;\r\u0026lt;ymax\u0026gt;498\u0026lt;/ymax\u0026gt;\r\u0026lt;/bndbox\u0026gt;\r\u0026lt;/object\u0026gt;\r\u0026lt;/annotation\u0026gt;\r标签解析 ****：根节点，包含所有图像和物体的信息。\n****：图像所在文件夹的名称。\n****：图像文件名，通常是.jpg 文件。\n****：图像的尺寸信息：\n* ****：图像的宽度，以像素为单位。\r* ****：图像的高度，以像素为单位。\r* ****：图像的通道数，通常为 3（RGB 图像）。\r****：每个图像中的物体对象的标签，包含多个子元素。图像中每个检测到的目标都有一个\u0026lt;object\u0026gt; 标签： * ****：物体的类别名称，例如 “dog”、“person” 等。\r* ****：物体的姿态（选填），描述物体的方向（如 “Left”、“Right” 等）。\r* ****：标记物体是否被截断，1 表示物体被截断，0 表示没有。被截断的物体可能只显示了一部分。\r* ****：标记物体是否难以检测，1 表示难检测，0 表示容易检测。这个标签常用于评估算法的检测性能。\r* ****：物体边界框（bounding box）的坐标，包含：\r* ****：边界框左上角的 x 坐标。\r* ****：边界框左上角的 y 坐标。\r* ****：边界框右下角的 x 坐标。\r* ****：边界框右下角的 y 坐标。\r* 坐标是以像素为单位的 **绝对值** ，而不是像 YOLO 中归一化到 0 和 1 的坐标。\rPascal VOC文件结构 一个典型的 VOC 数据集的文件结构如下：\nVOCdevkit/\r└── VOC2007/\r├── Annotations/ # 存储 XML 格式的标签文件\r├── ImageSets/ # 包含训练、测试集划分信息的文件夹\r│ └── Main/ # 包含 train.txt, val.txt, test.txt 等\r├── JPEGImages/ # 存储所有的图像文件\r├── SegmentationClass/ # 存储语义分割的标签文件（可选）\r└── SegmentationObject/ # 存储对象级分割的标签文件（可选）\rAnnotations ：存储每张图像的标注文件，格式为 XML。 ImageSets/Main ：存储数据集划分的文件（如 train.txt, val.txt, test.txt），每个文件包含用于训练、验证、测试的图像文件名（不含扩展名）。 JPEGImages ：存储所有的图像文件。 SegmentationClass 和 SegmentationObject ：用于语义分割任务的标注（可选）。 Pascal VOC标签文件的注意事项 每个 \u0026lt;object\u0026gt; 标签描述图像中的一个目标对象，因此如果图像中有多个目标，则 XML 文件中会有多个 \u0026lt;object\u0026gt; 标签。 标注的坐标为像素级别，通常不需要归一化。 truncated 和 difficult 标签用于更细致地描述目标对象，尤其是在评估模型时，difficult 标签的目标可以选择忽略。 如何将Pascal VOC格式转换为其他格式（如YOLO）： 如果需要将 Pascal VOC 格式转换为 YOLO 格式，可以将每个目标的 (xmin, ymin, xmax, ymax) 坐标转换为 YOLO 格式中的 (x_center, y_center, width, height)，并进行归一化操作。具体步骤如下：\n计算边界框的中心点： * `x_center = (xmin + xmax) / 2`\r* `y_center = (ymin + ymax) / 2`\r计算边界框的宽度和高度： * `width = xmax - xmin`\r* `height = ymax - ymin`\r将中心点坐标、宽度和高度归一化为相对于图像宽度和高度的值。 如果您需要具体的代码示例来实现这个转换，我也可以为您提供。\n使用coco的格式进行标注 COCO（Common Objects in Context）是计算机视觉领域最广泛使用的目标检测数据集之一。COCO 数据集采用 JSON 文件格式来存储目标检测、分割、关键点检测等标注信息。由于 COCO 数据集信息非常丰富，其标注文件也相对复杂，尤其相比于 YOLO 和 Pascal VOC 格式。\nCOCO 数据集格式概述 COCO 数据集的标注文件是 JSON 格式，包含了大量关于图像、类别、目标检测框、分割、关键点等的详细信息。COCO 目标检测数据的标注文件通常会包括以下几个关键部分：\ninfo ：关于数据集的描述信息（版本、日期、贡献者等）。 images ：图像的基本信息，包括图像的文件名、宽度、高度、唯一 ID 等。 annotations ：目标的标注信息，包含类别、边界框、分割区域、目标 ID 等。 categories ：类别信息，定义了所有的目标类别及其对应的 ID。 1. JSON文件结构 典型的 COCO 目标检测标注文件结构如下：\n{\r\u0026quot;info\u0026quot;: {...}, # 数据集的基本信息\r\u0026quot;licenses\u0026quot;: [...], # 图像的授权信息\r\u0026quot;images\u0026quot;: [...], # 图像的信息\r\u0026quot;annotations\u0026quot;: [...], # 每个目标的标注信息\r\u0026quot;categories\u0026quot;: [...] # 类别信息\r}\r2. 主要字段说明 2.1 images：图像信息 images 列表存储了每个图像的基本信息。每个图像有如下字段：\n\u0026quot;images\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: 397133,\r\u0026quot;width\u0026quot;: 640,\r\u0026quot;height\u0026quot;: 480,\r\u0026quot;file_name\u0026quot;: \u0026quot;000000397133.jpg\u0026quot;\r},\r...\r]\rid：图像的唯一标识符，其他部分如 annotations 将使用这个 ID 引用对应的图像。 width：图像的宽度（以像素为单位）。 height：图像的高度（以像素为单位）。 file_name：图像的文件名。 2.2 annotations：标注信息 annotations 列表存储了每个目标的标注信息，包括目标的边界框、类别、分割掩码等。一个典型的 annotation 如下：\n\u0026quot;annotations\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: 1,\r\u0026quot;image_id\u0026quot;: 397133,\r\u0026quot;category_id\u0026quot;: 18,\r\u0026quot;bbox\u0026quot;: [73.77, 150.54, 227.85, 304.35],\r\u0026quot;area\u0026quot;: 69321.27,\r\u0026quot;segmentation\u0026quot;: [[192.81, 247.09, 225.11, 249.06, ...]],\r\u0026quot;iscrowd\u0026quot;: 0\r},\r...\r]\rid：标注的唯一 ID。 image_id：对应的图像 ID，用于将标注与图像对应。 category_id：类别的 ID，表示这个目标的类别，参考 categories 部分。 bbox：目标的边界框（bounding box），格式为 [x, y, width, height]，这里的 x 和 y 是边界框左上角的坐标，width 和 height 是边界框的宽度和高度（以像素为单位）。 area：边界框的面积（bbox 的面积），用于评估模型的检测效果。 segmentation：多边形格式的分割掩码，用于语义分割任务。每个目标可以有一个或多个分割掩码。 iscrowd：表示目标是否是密集的、难以区分的对象。如果 iscrowd = 1，则意味着目标是一群密集的物体（如一群人、多个物体），否则为 0。 2.3 categories：类别信息 categories 列表存储了所有目标类别的信息。每个类别的结构如下：\n\u0026quot;categories\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: 18,\r\u0026quot;name\u0026quot;: \u0026quot;dog\u0026quot;,\r\u0026quot;supercategory\u0026quot;: \u0026quot;animal\u0026quot;\r},\r...\r]\rid：类别的唯一标识符，annotations 部分的 category_id 会引用这个 ID。 name：类别的名称，比如 “dog”、“person” 等。 supercategory：该类别所属的更高层级的类别分类（可选），比如 “animal”。 3. 边界框（bbox）格式说明 COCO 数据集中，边界框的格式为 [x, y, width, height]，其中：\nx 和 y 是边界框左上角的坐标，表示该框在图像中的位置。 width 是边界框的宽度。 height 是边界框的高度。 与 YOLO 和 Pascal VOC 不同，COCO 中的边界框是 绝对坐标 ，不需要归一化到 0 和 1 之间，而是直接使用像素值。\n4. 完整COCO文件示例 以下是一个完整的 COCO 标注文件的简化示例（JSON 格式）：\n{\r\u0026quot;info\u0026quot;: {\r\u0026quot;year\u0026quot;: 2024,\r\u0026quot;version\u0026quot;: \u0026quot;1.0\u0026quot;,\r\u0026quot;description\u0026quot;: \u0026quot;COCO-style dataset\u0026quot;,\r\u0026quot;date_created\u0026quot;: \u0026quot;2024-10-23\u0026quot;\r},\r\u0026quot;licenses\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: 1,\r\u0026quot;name\u0026quot;: \u0026quot;Creative Commons Attribution 4.0 License\u0026quot;\r}\r],\r\u0026quot;images\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: 1,\r\u0026quot;width\u0026quot;: 640,\r\u0026quot;height\u0026quot;: 480,\r\u0026quot;file_name\u0026quot;: \u0026quot;000000000001.jpg\u0026quot;\r}\r],\r\u0026quot;annotations\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: 1,\r\u0026quot;image_id\u0026quot;: 1,\r\u0026quot;category_id\u0026quot;: 18,\r\u0026quot;bbox\u0026quot;: [73.77, 150.54, 227.85, 304.35],\r\u0026quot;area\u0026quot;: 69321.27,\r\u0026quot;segmentation\u0026quot;: [[192.81, 247.09, 225.11, 249.06, ...]],\r\u0026quot;iscrowd\u0026quot;: 0\r}\r],\r\u0026quot;categories\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: 18,\r\u0026quot;name\u0026quot;: \u0026quot;dog\u0026quot;,\r\u0026quot;supercategory\u0026quot;: \u0026quot;animal\u0026quot;\r}\r]\r}\rCOCO文件结构 COCO 数据集的典型文件结构如下：\ncoco/\r├── annotations/ # 存放标注的 JSON 文件\r│ ├── instances_train2017.json # 目标检测任务的标注（训练集）\r│ ├── instances_val2017.json # 目标检测任务的标注（验证集）\r├── images/ # 图像数据存放位置\r│ ├── train2017/ # 训练集图像\r│ └── val2017/ # 验证集图像\r└── ...\r如何将COCO格式转换为其他格式（如YOLO）： 从COCO的bbox字段提取： * 从 `bbox` 中提取左上角坐标 `(x, y)`，宽度 `width` 和高度 `height`。\r* 计算 YOLO 所需的中心点坐标和相对宽度、高度。\r* 将坐标归一化到 0 到 1 之间。\r类别映射 ： * 根据 `category_id`，将 COCO 中的类别映射到 YOLO 类别 ID。\r格式之间的相互转换 当然，有的时候大家获得数据可能是别人以及标注好的，可能是voc的格式或者是json的格式，但是你可能只是想在yolo上进行训练，所以在这里，我提供两段脚本，分别用于voc格式向yolo格式数据的转化和coco数据向yolo格式数据的转化。\nvoc格式数据转化为yolo格式数据\nimport os\rimport xml.etree.ElementTree as ET\nVOC格式的目标类别列表（根据实际数据集的类别名称修改） 这是一份VOC数据集的类别示例，您可以根据实际类别修改 classes = [\u0026ldquo;aeroplane\u0026rdquo;, \u0026ldquo;bicycle\u0026rdquo;, \u0026ldquo;bird\u0026rdquo;, \u0026ldquo;boat\u0026rdquo;, \u0026ldquo;bottle\u0026rdquo;, \u0026ldquo;bus\u0026rdquo;, \u0026ldquo;car\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;chair\u0026rdquo;, \u0026ldquo;cow\u0026rdquo;, \u0026ldquo;diningtable\u0026rdquo;, \u0026ldquo;dog\u0026rdquo;, \u0026ldquo;horse\u0026rdquo;, \u0026ldquo;motorbike\u0026rdquo;, \u0026ldquo;person\u0026rdquo;, \u0026ldquo;pottedplant\u0026rdquo;, \u0026ldquo;sheep\u0026rdquo;, \u0026ldquo;sofa\u0026rdquo;, \u0026ldquo;train\u0026rdquo;, \u0026ldquo;tvmonitor\u0026rdquo;]\n读取XML文件，转换为YOLO格式 def convert_voc_to_yolo(xml_file, output_dir, img_width, img_height): tree = ET.parse(xml_file) root = tree.getroot()\n# 获取图像文件名\rimage_id = os.path.splitext(os.path.basename(xml_file))[0]\rtxt_file_path = os.path.join(output_dir, f\u0026quot;{image_id}.txt\u0026quot;)\rwith open(txt_file_path, 'w') as txt_file:\rfor obj in root.findall('object'):\r# 获取类别\rclass_name = obj.find('name').text\rif class_name not in classes:\rcontinue # 如果类别不在我们定义的类表中，跳过\rclass_id = classes.index(class_name)\r# 获取边界框信息\rxml_bbox = obj.find('bndbox')\rxmin = int(xml_bbox.find('xmin').text)\rymin = int(xml_bbox.find('ymin').text)\rxmax = int(xml_bbox.find('xmax').text)\rymax = int(xml_bbox.find('ymax').text)\r# 计算YOLO格式的 (x_center, y_center, width, height)，并归一化\rx_center = (xmin + xmax) / 2.0 / img_width\ry_center = (ymin + ymax) / 2.0 / img_height\rwidth = (xmax - xmin) / img_width\rheight = (ymax - ymin) / img_height\r# 写入到YOLO格式的txt文件中\rtxt_file.write(f\u0026quot;{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\u0026quot;)\rprint(f\u0026quot;Converted {xml_file} to {txt_file_path}\u0026quot;)\r批量转换函数 def convert_all_voc_to_yolo(voc_annotations_dir, output_dir, img_width, img_height): if not os.path.exists(output_dir): os.makedirs(output_dir)\nxml_files = [f for f in os.listdir(voc_annotations_dir) if f.endswith('.xml')]\rfor xml_file in xml_files:\rconvert_voc_to_yolo(os.path.join(voc_annotations_dir, xml_file), output_dir, img_width, img_height)\r使用示例：定义VOC格式的注释目录、输出目录、图像尺寸 voc_annotations_dir = \u0026lsquo;./VOCdevkit/VOC2007/Annotations\u0026rsquo; output_dir = \u0026lsquo;./VOCdevkit/VOC2007/YOLO_labels\u0026rsquo; img_width = 640 # 替换为图像的实际宽度 img_height = 480 # 替换为图像的实际高度\nconvert_all_voc_to_yolo(voc_annotations_dir, output_dir, img_width, img_height)\n主要步骤说明： 1. **类别映射 (`classes`)**：在脚本中，`classes` 列表存储了您目标检测任务中的类别。请确保 VOC 数据集中的类别与 YOLO 使用的类别相匹配，并且 `classes` 列表中的顺序与您希望的类别顺序一致。\r2. **XML 解析** ：脚本使用 Python 的 `xml.etree.ElementTree` 模块来解析 Pascal VOC 的 XML 标注文件，并提取目标的类别和边界框坐标。\r3. **YOLO 格式转换** ：对于每个边界框，脚本将 `(xmin, ymin, xmax, ymax)` 转换为 YOLO 所需的 `(x_center, y_center, width, height)` 格式，并将其归一化（相对于图像的宽度和高度）。\r4. **输出 YOLO 文件** ：每个图像生成一个 `.txt` 文件，文件名与图像名相同。每个 `.txt` 文件的每一行对应一个目标，包含目标类别的 ID 和归一化后的边界框信息。\r5. **批量处理** ：`convert_all_voc_to_yolo` 函数可以批量转换整个目录中的 XML 文件。\r如何将COCO格式转换为其他格式（如YOLO）： 1. 从COCO的`bbox`字段提取： * 从 `bbox` 中提取左上角坐标 `(x, y)`，宽度 `width` 和高度 `height`。\r* 计算 YOLO 所需的中心点坐标和相对宽度、高度。\r* 将坐标归一化到 0 到 1 之间。\r2. 类别映射： * 根据 `category_id`，将 COCO 中的类别映射到 YOLO 类别 ID。\rcoco格式数据转换为yolo格式数据\nimport json\rimport os\nCOCO格式的类别信息，按顺序填入类别名称 这里是一个简单示例，COCO标准数据集可能包含80个类别，您可根据需要调整 coco_classes = [ \u0026lsquo;person\u0026rsquo;, \u0026lsquo;bicycle\u0026rsquo;, \u0026lsquo;car\u0026rsquo;, \u0026lsquo;motorcycle\u0026rsquo;, \u0026lsquo;airplane\u0026rsquo;, \u0026lsquo;bus\u0026rsquo;, \u0026rsquo;train\u0026rsquo;, \u0026rsquo;truck\u0026rsquo;, \u0026lsquo;boat\u0026rsquo;, \u0026rsquo;traffic light\u0026rsquo;, \u0026lsquo;fire hydrant\u0026rsquo;, \u0026lsquo;stop sign\u0026rsquo;, \u0026lsquo;parking meter\u0026rsquo;, \u0026lsquo;bench\u0026rsquo;, \u0026lsquo;bird\u0026rsquo;, \u0026lsquo;cat\u0026rsquo;, \u0026lsquo;dog\u0026rsquo;, \u0026lsquo;horse\u0026rsquo;, \u0026lsquo;sheep\u0026rsquo;, \u0026lsquo;cow\u0026rsquo;, \u0026rsquo;elephant\u0026rsquo;, \u0026lsquo;bear\u0026rsquo;, \u0026lsquo;zebra\u0026rsquo;, \u0026lsquo;giraffe\u0026rsquo;, \u0026lsquo;backpack\u0026rsquo;, \u0026lsquo;umbrella\u0026rsquo;, \u0026lsquo;handbag\u0026rsquo;, \u0026rsquo;tie\u0026rsquo;, \u0026lsquo;suitcase\u0026rsquo;, \u0026lsquo;frisbee\u0026rsquo;, \u0026lsquo;skis\u0026rsquo;, \u0026lsquo;snowboard\u0026rsquo;, \u0026lsquo;sports ball\u0026rsquo;, \u0026lsquo;kite\u0026rsquo;, \u0026lsquo;baseball bat\u0026rsquo;, \u0026lsquo;baseball glove\u0026rsquo;, \u0026lsquo;skateboard\u0026rsquo;, \u0026lsquo;surfboard\u0026rsquo;, \u0026rsquo;tennis racket\u0026rsquo;, \u0026lsquo;bottle\u0026rsquo;, \u0026lsquo;wine glass\u0026rsquo;, \u0026lsquo;cup\u0026rsquo;, \u0026lsquo;fork\u0026rsquo;, \u0026lsquo;knife\u0026rsquo;, \u0026lsquo;spoon\u0026rsquo;, \u0026lsquo;bowl\u0026rsquo;, \u0026lsquo;banana\u0026rsquo;, \u0026lsquo;apple\u0026rsquo;, \u0026lsquo;sandwich\u0026rsquo;, \u0026lsquo;orange\u0026rsquo;, \u0026lsquo;broccoli\u0026rsquo;, \u0026lsquo;carrot\u0026rsquo;, \u0026lsquo;hot dog\u0026rsquo;, \u0026lsquo;pizza\u0026rsquo;, \u0026lsquo;donut\u0026rsquo;, \u0026lsquo;cake\u0026rsquo;, \u0026lsquo;chair\u0026rsquo;, \u0026lsquo;couch\u0026rsquo;, \u0026lsquo;potted plant\u0026rsquo;, \u0026lsquo;bed\u0026rsquo;, \u0026lsquo;dining table\u0026rsquo;, \u0026rsquo;toilet\u0026rsquo;, \u0026rsquo;tv\u0026rsquo;, \u0026rsquo;laptop\u0026rsquo;, \u0026lsquo;mouse\u0026rsquo;, \u0026lsquo;remote\u0026rsquo;, \u0026lsquo;keyboard\u0026rsquo;, \u0026lsquo;cell phone\u0026rsquo;, \u0026lsquo;microwave\u0026rsquo;, \u0026lsquo;oven\u0026rsquo;, \u0026rsquo;toaster\u0026rsquo;, \u0026lsquo;sink\u0026rsquo;, \u0026lsquo;refrigerator\u0026rsquo;, \u0026lsquo;book\u0026rsquo;, \u0026lsquo;clock\u0026rsquo;, \u0026lsquo;vase\u0026rsquo;, \u0026lsquo;scissors\u0026rsquo;, \u0026rsquo;teddy bear\u0026rsquo;, \u0026lsquo;hair drier\u0026rsquo;, \u0026rsquo;toothbrush\u0026rsquo; ]\n加载COCO的JSON文件 def load_coco_annotations(json_file): with open(json_file, \u0026lsquo;r\u0026rsquo;) as f: data = json.load(f) return data\n将COCO的bbox转换为YOLO格式 def convert_coco_to_yolo(image_id, annotations, img_width, img_height, output_dir): txt_file_path = os.path.join(output_dir, f\u0026quot;{image_id}.txt\u0026quot;)\nwith open(txt_file_path, 'w') as txt_file:\rfor ann in annotations:\rif ann['image_id'] != image_id:\rcontinue\r# 获取类别ID，COCO的category_id需要映射到YOLO的class id\rcategory_id = ann['category_id'] - 1 # 假设类别从1开始编号，需要减1\r# 获取COCO的bbox (x, y, width, height)\rbbox = ann['bbox']\rx, y, width, height = bbox\r# 计算YOLO的x_center, y_center并归一化\rx_center = (x + width / 2.0) / img_width\ry_center = (y + height / 2.0) / img_height\rnorm_width = width / img_width\rnorm_height = height / img_height\r# 写入YOLO格式的txt文件中\rtxt_file.write(f\u0026quot;{category_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\\n\u0026quot;)\rprint(f\u0026quot;Converted annotations for image {image_id} to {txt_file_path}\u0026quot;)\r批量处理COCO的图像和标注信息 def convert_all_coco_to_yolo(coco_json_file, output_dir): data = load_coco_annotations(coco_json_file)\n# 获取图像信息和标注信息\rimages = data['images']\rannotations = data['annotations']\r# 创建输出目录\rif not os.path.exists(output_dir):\ros.makedirs(output_dir)\r# 遍历所有图像\rfor image in images:\rimage_id = image['id']\rimg_width = image['width']\rimg_height = image['height']\r# 转换每个图像的标注\rconvert_coco_to_yolo(image_id, annotations, img_width, img_height, output_dir)\r使用示例：定义COCO的JSON文件、输出目录 coco_json_file = \u0026lsquo;./coco/annotations/instances_train2017.json\u0026rsquo; # 替换为COCO的JSON文件路径 output_dir = \u0026lsquo;./coco/YOLO_labels\u0026rsquo; # 输出目录\n执行转换 convert_all_coco_to_yolo(coco_json_file, output_dir)\n代码详细说明： 1. **类别映射 (`coco_classes`)**：该列表包含 COCO 数据集中所有类别的名称。假设类别编号从 1 开始，因此类别 ID 减 1 转换为 0 开始的编号，这与 YOLO 的类别 ID 对应。\r2. **加载 COCO 标注文件** ：函数 `load_coco_annotations` 用于加载 COCO 格式的 JSON 文件，该文件包含了图像、类别、边界框、分割等信息。\r3. **COCO 到 YOLO 的边界框转换** ：函数 `convert_coco_to_yolo` 负责将每个目标的 COCO 格式边界框 `[x, y, width, height]` 转换为 YOLO 所需的 `(x_center, y_center, width, height)`，并将其归一化为相对于图像宽度和高度的值。\r4. **输出 YOLO 格式** ：转换后的数据将保存为 `.txt` 文件，文件名与图像名相同。每行表示一个物体，内容为：\rphp\r​ Copy code \u0026lt;class_id\u0026gt; \u0026lt;x_center\u0026gt; \u0026lt;y_center\u0026gt; 5. **批量处理** ：函数 `convert_all_coco_to_yolo` 遍历所有图像，并为每个图像生成一个对应的 `.txt` 文件。\r使用说明： * **COCO 标注文件路径** ：修改 `coco_json_file` 为 COCO 数据集中 JSON 标注文件的路径（例如 `instances_train2017.json`）。\r* **输出目录** ：修改 `output_dir` 为保存 YOLO 格式的标签文件的目录。\r运行脚本后，YOLO 格式的标签文件将被保存在指定的输出目录中，每个图像有一个 .txt 文件。\n注意事项 上面叙述了很多内容，但是主要我想表达的还是如果你需要进行高质量的标注，熟悉这些数据格式将是非常重要的，熟悉数据的格式也可以帮助你更加了解你的任务。\n尽量只用数字、英文以及下划线来进行表示。 yolo模式下最好一次性标注完成。 学会快捷方式的使用将会让你事半功倍。 ","date":"0001-01-01T00:00:00Z","permalink":"https://nickk111.github.io/p/","title":""},{"content":"使用YOLOv8训练自己的数据集（原理解析+数据标注说明+训练教程+图形化系统开发） 使用YOLOv8训练自己的数据集 Hello，大家好，本次我们来教大家使用YOLOV8训练自己的数据集。\n视频地址：手把手教你用YOLOv8训练自己的数据集（原理解析+代码实践）_哔哩哔哩_bilibili\nGitHub资源地址：\nYOLO系列目前已经更新到了V11，并且YOLO系列模型已经目前稳定运行了一段时间。 从原理、数据标注和环境配置 ，帮助小伙伴们掌握YOLOv8的基本内容。注意本次的教程除了支持v8模型的训练，还适用v3、v5、v9、v10等一系列模型的训练。\n为了帮助大家能灵活选择自己喜欢的内容，我们选择分P的方式进行更新。比如，有的小伙伴只喜欢理论的部分，有的小伙伴只喜欢实操的部分，这样大家可以根据自己的需要各取所需。\nYOLOv8原理解析 Ultralytics开发的YOLOv8是一款尖端、最先进（SOTA）的模型，它借鉴了之前YOLO版本的成功经验，并引入了新的特性和改进，以进一步提高性能和灵活性。YOLOv8旨在实现快速、准确和易于使用，因此是各种目标检测、图像分割和图像分类任务的绝佳选择。注意，此时的YOLOv8的模型已经基本完成了最终的进化，除了支持最经典的目标检测任务之外，还添加了对语义分割、分类和追踪等任务的支持。当然我们本期还是选择大家最熟悉的检测任务来进行展开，关于后续的其他任务我们再另外录制。\n首先我们来看一下YOLOv8算法的性能。下图是官方提供了性能图，其中左图的横坐标表示的是网络的参数量，右图的横坐标表示的网络在A100显卡上的推理速度，纵坐标方面表示表示的都是模型的精度。可以看出，YOLOv8模型的在同样的参数量下，比其他系列的YOLO模型有明显的精度提升，在右图展示的同样的map精度下，YOLOv8的模型也同样有更快的速度，还真就是那个更高、更快、更强。\n下面的表格则是来自YOLOv8 - Ultralytics YOLO Docs提供的在coco数据集上的测试结果，从表中可以看出，对于他的nano模型而言，在只有3.2M的参数量下，就可以达到37.3的mAP，非常优秀的数值表现。\nYOLOv8算法的核心点可以总结为下面几点。\n给出了一个全新的视觉模型 ，保持精度的同时，实现了较高的检测速度，并且同时支持支持图像分类、物体检测和实例分割等多种视觉任务。并且提供了多个规模的模型（nano、small、medium、large和x-large），满足用户不同场景的需要。 新的骨干网络 ：YOLOv8引入了一个新的骨干网络，可能参考了YOLOv7 ELAN设计思想，将YOLOv5中的C3结构换成了梯度流更丰富的C2f结构，并对不同尺度模型调整了不同的通道数，大幅提升了模型性能。 解耦头的设计 ：YOLOv8的Head部分从原先的耦合头变成了解耦头结构，将分类和检测头分离，并且从Anchor-Based转变为Anchor-Free，简化了模型结构并提高了推理速度。 新的损失函数 ：YOLOv8在Loss计算方面采用了TaskAlignedAssigner正样本分配策略，并引入了Distribution Focal Loss，确保了检测结果的准确性和鲁棒性。 OK，说完这里的性能表现，我们就一起来看看YOLOv8结构方面的内容吧。\n结构说明 首先是YOLOv8的网络结构图\n骨干网络部分： 骨干网络部分的c2f结构可能借鉴了YOLOv7的设计。将原先的c3模块更新了c2f的模块，其中c3表示使用了3个常规的卷积模块，c2f表示使用了2个卷积模块并且更快（fast）。在不改变原始架构和梯度传输路径的前提下， 使用分组卷积踢来以及使用洗牌和合并的操作组合不同组的特征，增强模型从不同特征图中的学习能力，达到改善参数的作用。\n下图是YOLOv7中原文提到的Elan的结构，主要是使用了更多的连接和合并的操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class C3(nn.Module): \u0026#34;\u0026#34;\u0026#34; 这里是使用了3个卷积层的csp结构 CSP Bottleneck with 3 convolutions. \u0026#34;\u0026#34;\u0026#34; def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): \u0026#34;\u0026#34;\u0026#34;Initialize the CSP Bottleneck with given channels, number, shortcut, groups, and expansion values.\u0026#34;\u0026#34;\u0026#34; super().__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # optional act=FReLU(c2) self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n))) def forward(self, x): \u0026#34;\u0026#34;\u0026#34;Forward pass through the CSP bottleneck with 2 convolutions.\u0026#34;\u0026#34;\u0026#34; return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1)) class C2f(nn.Module): \u0026#34;\u0026#34;\u0026#34; 这里使用了分支处理的操作，使用的是通过关闭残差链接的方式实现 先进行分支的操作然后再进行特征融合的操作 Faster Implementation of CSP Bottleneck with 2 convolutions. \u0026#34;\u0026#34;\u0026#34; def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5): \u0026#34;\u0026#34;\u0026#34;Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups, expansion. \u0026#34;\u0026#34;\u0026#34; super().__init__() self.c = int(c2 * e) # hidden channels self.cv1 = Conv(c1, 2 * self.c, 1, 1) self.cv2 = Conv((2 + n) * self.c, c2, 1) # optional act=FReLU(c2) self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n)) def forward(self, x): \u0026#34;\u0026#34;\u0026#34;Forward pass through C2f layer.\u0026#34;\u0026#34;\u0026#34; y = list(self.cv1(x).chunk(2, 1)) y.extend(m(y[-1]) for m in self.m) return self.cv2(torch.cat(y, 1)) def forward_split(self, x): \u0026#34;\u0026#34;\u0026#34;Forward pass using split() instead of chunk().\u0026#34;\u0026#34;\u0026#34; y = list(self.cv1(x).split((self.c, self.c), 1)) y.extend(m(y[-1]) for m in self.m) return self.cv2(torch.cat(y, 1)) 除此之外，YOLOv8的主干还去掉了neck部分中的2个卷积层，以及将block的数量从原先的 3-6-9-3 改成了 3-6-6-3。另外，在之前的YOLOv5的网络结构中，只需要更改一下w和h系数就能统一适配不同规模的模型，但是对于YOLOv8而言，其中N和S的结构一致，L和X的结构一致，这两对模型可以只通过修改缩放系数就完成替换。在YOLOv10中，作者也提到了这个观点，为了追求网络的灵活性，导致网络同质化比较严重，其中有些冗余的模块是可以去除的，也说明现在的网络结构向着比较定制化的方向进行，当然，这句话是我的个人观点。\n解码头部分： 解码头的部分选择使用了分类的解码头，也就是边界框回归是一个分支以及分类的是一个分支。如下图所示，上面的子图是原先的解码头部，经过主干网络进行特征提取之后得到特征图，之后直接进入一个模块中进行解码，这里的数值计算包含3个部分，分别是用于边界框回归的CIoU、用于置信度计算的obj和用于分类类别计算的CLS。改进之后的头部如下面的子图所示，经过主干网络进行特征提取之后，上面的子分支用于回归，下面的子分支则用于分类，去除了之前的obj的部分，在回归的分支中，使用的是Distribution Focal Loss。\n其中DFL损失函数的定义如下，通俗来讲就是训练的过程中，目标的边界框不应该是一个确定的数值，目标的边界框应该是一个分布，比如对于浪花这个物体而言，他的边界就是不清晰的，通过这样的损失函数可以减少网络在训练过程中出现的过拟合的现象。\n其中，DFL实现的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def distribution_focal_loss(pred, label): r\u0026#34;\u0026#34;\u0026#34;Distribution Focal Loss (DFL) is from `Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection \u0026lt;https://arxiv.org/abs/2006.04388\u0026gt;`_. Args: pred (torch.Tensor): Predicted general distribution of bounding boxes (before softmax) with shape (N, n+1), n is the max value of the integral set `{0, ..., n}` in paper. label (torch.Tensor): Target distance label for bounding boxes with shape (N,). Returns: torch.Tensor: Loss tensor with shape (N,). \u0026#34;\u0026#34;\u0026#34; #label为y, pred为y^(y的估计值） #因为y_i \u0026lt;= y \u0026lt;= y_i+1(paper) #取dis_left = y_i, dis_right = y_i+1 dis_left = label.long() dis_right = dis_left + 1 weight_left = dis_right.float() - label #y_i+1-y weight_right = label - dis_left.float() #y-y_i #paper中的log(S)这里用CE loss = ( F.cross_entropy(pred, dis_left, reduction=\u0026#34;none\u0026#34;) * weight_left + F.cross_entropy(pred, dis_right, reduction=\u0026#34;none\u0026#34;) * weight_right ) return loss 损失函数说明 YOLOv8的loss计算包含了分类和回归两个部分，没有了之前的objectness的分支部分，其中分类的分支采用的是BCE Loss，回归的分支使用了两个部分，分别是上面提到的Distribution Focal Loss和CIoU Loss，3个损失函数按照一定的权重比例进行加权。\n关于正负样本的分配，其中YOLOv5中使用的是静态的分布策略，简单来说，静态的分布策略是将标签中的GT Box和Anchor Templates模板计算IoU，如果IoU大于设定的阈值就认为是匹配成功，匹配成功的边界框将会参与到CIoU Loss的计算中。当然这里所述的是简化的版本，实际子计算的过程中还会去计算GT Box和对应的的Anchor Templates模板高宽的比例。假设对某个GT Box而言，其实只要GT Box满足在某个Anchor Template宽和高的× 0.25 0.25倍和4.0倍之间就算匹配成功。关于这部分更详细的解释可以看YOLOv5网络详解_yolov5网络结构详解-CSDN博客。在YOLOv8中，使用的是动态分布的策略（YOLOX 的 simOTA、TOOD 的 TaskAlignedAssigner 和 RTMDet 的 DynamicSoftLabelAssigner），这里直接使用的是 TOOD 的 TaskAlignedAssigner。 TaskAlignedAssigner 的匹配策略简单总结为： 根据分类与回归的分数加权的分数选择正样本。\ns是标注类别对应的预测分数值，u是预测框和gt框之间的iou。计算出分数之后，根据分数选取topK大的作为正样本，其余作为负样本。\n数据增强说明 数据增强的部分和YOLOv5基本保持了一致，包含了颜色变换、马赛克数据增强、随机剪切等一系列常规的数据增强的方式。并且使用YOLOX的数据增强策略，在前面的部分使用数据增强，而在最后的10个epoch中关闭数据增强。如下图所示。\n对于一些常见的数据增强的方式的说明。\n训练策略说明 YOLOv8 的推理过程和 YOLOv5 几乎一样，唯一差别在于前面需要对 Distribution Focal Loss 中的积分表示 bbox 形式进行解码，变成常规的 4 维度 bbox，后续计算过程就和 YOLOv5 一样了。\n以 COCO 80 类为例，假设输入图片大小为 640x640，MMYOLO 中实现的推理过程示意图如下所示：\n其推理和后处理过程为：\n(1) bbox 积分形式转换为 4d bbox 格式\n对 Head 输出的 bbox 分支进行转换，利用 Softmax 和 Conv 计算将积分形式转换为 4 维 bbox 格式\n(2) 维度变换\nYOLOv8 输出特征图尺度为 80x80、40x40 和 20x20 的三个特征图。Head 部分输出分类和回归共 6 个尺度的特征图。 将 3 个不同尺度的类别预测分支、bbox 预测分支进行拼接，并进行维度变换。为了后续方便处理，会将原先的通道维度置换到最后，类别预测分支 和 bbox 预测分支 shape 分别为 (b, 80x80+40x40+20x20, 80)=(b,8400,80)，(b,8400,4)。\n(3) 解码还原到原图尺度\n分类预测分支进行 Sigmoid 计算，而 bbox 预测分支需要进行解码，还原为真实的原图解码后 xyxy 格式。\n(4) 阈值过滤\n遍历 batch 中的每张图，采用 score_thr 进行阈值过滤。在这过程中还需要考虑 multi_label 和 nms_pre，确保过滤后的检测框数目不会多于 nms_pre。\n(5) 还原到原图尺度和 nms\n基于前处理过程，将剩下的检测框还原到网络输出前的原图尺度，然后进行 nms 即可。最终输出的检测框不能多于 max_per_img。\n有一个特别注意的点： YOLOv5 中采用的 Batch shape 推理策略，在 YOLOv8 推理中暂时没有开启，不清楚后面是否会开启，在 MMYOLO 中快速测试了下，如果开启 Batch shape 会涨大概 0.1~0.2。\n代码解析 下载代码之后，你将会看到下面的代码目录结构，其中42_demo是我准备的简易的执行文件，其余文件都是官方的文件和目录，每个文件大概的作用如下。\n42_demo/ 这个目录下的文件是用于我们本次教程的脚本，我们将训练、测试、预测等脚本进行了单独的封装，方便初学者或者不是计算机专业的同学运行，每个脚本对应的含义如下。\n其中比较重要的是训练的脚本start_train.py，这个脚本记录了数据的加载和一些训练的超参数，内容如下。\nimport time\rfrom ultralytics import YOLO\r# yolov8n模型训练：训练模型的数据为'A_my_data.yaml'，轮数为100，图片大小为640，设备为本地的GPU显卡，关闭多线程的加载，图像加载的批次大小为4，开启图片缓存\rmodel = YOLO('yolov8n.pt') # load a pretrained model (recommended for training)\rresults = model.train(data='A_my_data.yaml', epochs=100, imgsz=640, device=[0,], workers=0, batch=4, cache=True) # 开始训练\rtime.sleep(10) # 睡眠10s，主要是用于服务器多次训练的过程中使用\r​ ​\n预测的测试脚本主要用于单张图像的检测，脚本为start_single_detect.py\nfrom ultralytics import YOLO\r# Load a model\rmodel = YOLO(\u0026quot;yolov8n.pt\u0026quot;) # pretrained YOLOv8n model\r# Run batched inference on a list of images\rresults = model([\u0026quot;images/resources/demo.jpg\u0026quot;, ]) # return a list of Results objects\r# Process results list\rfor result in results:\rboxes = result.boxes # Boxes object for bounding box outputs\rmasks = result.masks # Masks object for segmentation masks outputs\rkeypoints = result.keypoints # Keypoints object for pose outputs\rprobs = result.probs # Probs object for classification outputs\robb = result.obb # Oriented boxes object for OBB outputs\rresult.show() # display to screen\rresult.save(filename=\u0026quot;images/resources/result.jpg\u0026quot;) # save to disk\r​\ndocker/ Docker 是一个开源的应用容器引擎，它允许开发者打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。\n简单来说，Docker 提供了一个可以运行你的应用程序的虚拟环境，但它比传统的虚拟机更加轻量和快速。使用 Docker，你可以轻松地在不同的机器和平台上部署、运行和管理应用程序，而无需担心环境配置和依赖问题。\n这个目录是根据用户不同的软硬件情况写的配置文件，但是一般情况下大家使用的不是很多，对于看我内容的小伙伴来说，大部分都是学生，使用的更少，所以这里的内容我们就不详细说明了。\ndocs/ 这个目录用于放置对这个代码解释的官方文档，包含了各个不同的语言。\nexamples/ 这个目录下有官方提供的一些案例，并且包含了一些模型导出之后C++的调用脚本，这里的脚本大多数时候只有在实际部署的时候才会使用到，关于硬件部署是一个比较复杂的内容，这块的内容我们会单独抽时间来讲。\ntest/ test目录存放了一些自动化测试的脚本。\nultralytics/ 该目录是整个项目的核心目录，存放了网络结构的底层实现和网络、数据集一系列的配置文件，平常修改网络结构和新增数据都会在这个目录下执行。\nassets目录下存放了两张经典的测试图像，用于模型的初步验证。\ncfg目录下存放了数据、模型和追踪的配置文件，举个例子，其中datasets下面的A_my_data.yaml就是我们本次教程使用的数据集配置文件，在这个路径中我们指明了数据集的路径和类别等信息。而models目录下的yolov8.yaml配置文件则指定了我们要使用到的模型。详细的内容如下：\nA_my_data.yaml\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\rpath: ../../person_42_yolo_format\rtrain: # train images (relative to 'path') 16551 images\r- images/train\rval: # val images (relative to 'path') 4952 images\r- images/val\rtest: # test images (optional)\r- images/test\r# Classes\r# ['Chave', 'DISJUNTOR', 'TP', 'Pararraio', 'TC']\rnames:\r: person\r​\nyolov8.yaml\n# Ultralytics YOLO 🚀, AGPL-3.0 license\r# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect\r# Parameters\rnc: 80 # number of classes\rscales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'\r# [depth, width, max_channels]\rn: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\rs: [0.33, 0.50, 1024] # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients, 28.8 GFLOPs\rm: [0.67, 0.75, 768] # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients, 79.3 GFLOPs\rl: [1.00, 1.00, 512] # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs\rx: [1.00, 1.25, 512] # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs\r# YOLOv8.0n backbone\rbackbone:\r# [from, repeats, module, args]\r- [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\r- [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\r- [-1, 3, C2f, [128, True]]\r- [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\r- [-1, 6, C2f, [256, True]]\r- [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\r- [-1, 6, C2f, [512, True]]\r- [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\r- [-1, 3, C2f, [1024, True]]\r- [-1, 1, SPPF, [1024, 5]] # 9\r# YOLOv8.0n head\rhead:\r- [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]]\r- [[-1, 6], 1, Concat, [1]] # cat backbone P4\r- [-1, 3, C2f, [512]] # 12\r- [-1, 1, nn.Upsample, [None, 2, \u0026quot;nearest\u0026quot;]]\r- [[-1, 4], 1, Concat, [1]] # cat backbone P3\r- [-1, 3, C2f, [256]] # 15 (P3/8-small)\r- [-1, 1, Conv, [256, 3, 2]]\r- [[-1, 12], 1, Concat, [1]] # cat head P4\r- [-1, 3, C2f, [512]] # 18 (P4/16-medium)\r- [-1, 1, Conv, [512, 3, 2]]\r- [[-1, 9], 1, Concat, [1]] # cat head P5\r- [-1, 3, C2f, [1024]] # 21 (P5/32-large)\r- [[15, 18, 21], 1, Detect, [nc]] # Detect(P3, P4, P5)\r​\n后面的data、engine、hub、models、nn、solution、trackers、utils则分别定义了数据、模型训练引擎、训练可视化、模型、网络结构、解决方案、追踪和工具类的底层代码实现。比如在nn目录下的models目录下的block.py中就给出了c2f模块的定义。\nclass C2f(nn.Module):\r\u0026quot;\u0026quot;\u0026quot;\r这里使用了分支处理的操作，使用的是通过关闭残差链接的方式实现\r先进行分支的操作然后再进行特征融合的操作\rFaster Implementation of CSP Bottleneck with 2 convolutions.\r\u0026quot;\u0026quot;\u0026quot;\rdef __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\r\u0026quot;\u0026quot;\u0026quot;Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,\rexpansion.\r\u0026quot;\u0026quot;\u0026quot;\rsuper().__init__()\rself.c = int(c2 * e) # hidden channels\rself.cv1 = Conv(c1, 2 * self.c, 1, 1)\rself.cv2 = Conv((2 + n) * self.c, c2, 1) # optional act=FReLU(c2)\rself.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\rdef forward(self, x):\r\u0026quot;\u0026quot;\u0026quot;Forward pass through C2f layer.\u0026quot;\u0026quot;\u0026quot;\ry = list(self.cv1(x).chunk(2, 1))\ry.extend(m(y[-1]) for m in self.m)\rreturn self.cv2(torch.cat(y, 1))\rdef forward_split(self, x):\r\u0026quot;\u0026quot;\u0026quot;Forward pass using split() instead of chunk().\u0026quot;\u0026quot;\u0026quot;\ry = list(self.cv1(x).split((self.c, self.c), 1))\ry.extend(m(y[-1]) for m in self.m)\rreturn self.cv2(torch.cat(y, 1))\r​\nultralytics.egg-info/ egg-info是一个目录 ，它在安装或开发Python包时由setuptools生成，用于存储关于该包的元数据。这些元数据对于包的管理、分发和安装至关重要，它们帮助pip和其他工具了解包的详细信息、版本信息、依赖关系等。这个目录是我们执行pip指令以开发者模式安装的时候出现的。\nCITATION.cff 这个文件中包含了引用这个项目的格式说明。\nCONTRIBUTING.md 这个里面说明了你可以如何为这个项目提供自己的贡献，让自己的名字出现在作者名单中。\nLICENSE 提供了这该项目的许可信息。\nmkdocs.yml 用于定义项目的各种设置和配置选项。\npyproject.toml pyproject.toml 是一个在 Python 项目中广泛使用的配置文件，它主要用于定义项目的构建系统要求、依赖关系以及相关的工具配置。现在这个 pyproject.toml 也是官方从PEP 518开始推荐的项目配置方式，感兴趣的小伙伴可以去看一下poetry这个库，通过poetry new可以很方便快捷的生成一个项目的脚手架代码。\nREADME.md 说明文件，也就是你现在看到的这个内容。\nREADME.zh-CN.md 中文格式的说明文件。\n数据集准备 我们在这里准备了一系列标注好的数据集，如果大家不想自己标注可以看看这里是否有你需要得：肆十二大作业系列清单-CSDN博客\n现在来到我们数据集准备的章节，这个章节教会大家如何自己构建一个yolo的检测数据以及如何使用。\n首先这个位置提供了我们本次教程中使用到的一份标准的行人检测的数据集，一个标准的数据集的构成包含下面的几个目录。\n├─images # 图像文件夹\r│ ├─train # 训练集图像\r│ ├─val # 验证集图像\r│ └─test # 测试集图像\r└─labels # 标签文件夹，标签格式为yolo的txt格式\r├─train # 训练集标签\r├─val # 验证集标签\r└─test # 测试集标签\r以训练集为例，给小伙伴们展示一下一个正确的对应关系是怎样的，这里训练集图像数量和标签数量是一致的，并且名称上面去除后缀之后是一一对应的。\n以2007_000480.jpg为例，下面是对应的2007_000480.txt中的标注文件内容。这个标注文件有三行，对应的是图像中的3个人物，其中每行包含5个数字，分别是类别，归一化处理之后目标中心点的x坐标、y坐标、目标归一化处理之后宽度w和目标归一化处理之后的宽度h。\n如果大家要标注自己的数据集，则可以使用labelimg来进行标注，标注之前有几点注意事项。\n标注的图像和路径尽量不要包含中文，图像名称最好是只有数字或者英文。 标注的时候尽量使用jpg的格式，如果是gif一类的格式后续可能有其他的麻烦。 标注的时候请一定要确保选择了yolo格式，如果不是yolo格式后续处理起来会非常麻烦。 最好一次性标注完，负责可能导致前后两次标签的结果不一致。 ok，首先大家可以在你的虚拟环境中通过pip install labelimg的指令安装标注然后，然后在命令行中键入labelimg来启动标注软件，如下图所示。\n接下来，我们打开要标注的文件夹就可以进行标注了，标注之后一定要检查是否标注文件为txt格式。\n下面是labelimg的常用的快捷方式，大家可以熟悉下面的快捷方式帮助你提升标注的效率。\nctrl+s 保存 ctrl+d 复制当前标签和矩形框 Ctrl + r 更改默认注释目录（xml文件保存的地址） Ctrl + u 加载目录中的所有图像，鼠标点击Open dir同功能 w 创建矩阵 d 下一张 a 上一张 delete 删除选定的矩阵框 space 将当前图像标记为已标记 环境配置 来到我们最熟悉的章节，环境配置，老生常谈，环境配置基本是一致的，开始前请先学习pycharm和anaconda的使用，不熟悉的小伙伴可以移步这个位置：【大作业系列入门教程】如何使用Anaconda和Pycharm_2024毕设系列如何使用anaconda-CSDN博客\n下载安装包到本地，首先请执行下列指令确保你已经配置好了国内的源。\nconda config --remove-key channels\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/\rconda config --set show_channel_urls yes\rpip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\r之后请先安装pytorch，请根据你设备的实际情况来选择执行GPU安装的指令或者是CPU安装的指令。\nconda install pytorch==1.8.0 torchvision torchaudio cudatoolkit=10.2 # 注意这条命令指定Pytorch的版本和cuda的版本\rconda install pytorch==1.10.0 torchvision torchaudio cudatoolkit=11.3 # 30系列以上显卡gpu版本pytorch安装指令\rconda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cpuonly # CPU的小伙伴直接执行这条命令即可\r接着来到项目的目录下，以开发者的模式安装其余需要的库。\npip install -v -e .\rOK，我们来做一个简单的测试，观看是否生效，如果你看到了下面的结果则说明你的环境配置没有问题。\n如果没有出现上面的检测结果，则说明你的配置出现了问题，可以在评论区中留言，让小伙伴们一同给你解答。\n模型训练和测试 模型的训练 训练的脚本对应的是：\nimport time\rfrom ultralytics import YOLO\r​ # yolov8n模型训练：训练模型的数据为\u0026rsquo;A_my_data.yaml\u0026rsquo;，轮数为100，图片大小为640，设备为本地的GPU显卡，关闭多线程的加载，图像加载的批次大小为4，开启图片缓存 model = YOLO(\u0026lsquo;yolov8n.pt\u0026rsquo;) # load a pretrained model (recommended for training) results = model.train(data=\u0026lsquo;A_my_data.yaml\u0026rsquo;, epochs=100, imgsz=640, device=[0,], workers=0, batch=4, cache=True) # 开始训练 time.sleep(10) # 睡眠10s，主要是用于服务器多次训练的过程中使用\n​ ​\n开始之前请配置好你的数据集的路径和图片名称，比如我们今天训练的行人检测的数据集的配置如下：\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\rpath: ../../person_42_yolo_format\rtrain: # train images (relative to 'path') 16551 images\r- images/train\rval: # val images (relative to 'path') 4952 images\r- images/val\rtest: # test images (optional)\r- images/test\r# Classes\r# ['Chave', 'DISJUNTOR', 'TP', 'Pararraio', 'TC']\rnames:\r: person\r​\n配置好了直接右键运行即可，比如我们这里是训练100轮，训练的过程中进度条会实时进行更新。\n训练完毕之后，将会在runs目录下生成一系列的结果图像。\n其中如果你需要在你的报告中说明模型的训练过程和训练结果，使用最多的分别是results.png和PR_curve.png，如下图所示。\n模型的测试 测试的脚本对应的是：\nfrom ultralytics import YOLO\r# 加载自己训练好的模型，填写相对于这个脚本的相对路径或者填写绝对路径均可\rmodel = YOLO(\u0026quot;runs/detect/yolov8n/weights/best.pt\u0026quot;)\r# 开始进行验证，验证的数据集为'A_my_data.yaml'，图像大小为640，批次大小为4，置信度分数为0.25，交并比的阈值为0.6，设备为0，关闭多线程（windows下使用多线程加载数据容易出现问题）\rvalidation_results = model.val(data='A_my_data.yaml', imgsz=640, batch=4, conf=0.25, iou=0.6, device=\u0026quot;0\u0026quot;, workers=0)\r​\n执行测试的脚本，将会输出下面的验证指标。\n如果你不需要图形化界面，可以通过下面的脚本来直接对图像进行预测。\nfrom ultralytics import YOLO\r# Load a model\rmodel = YOLO(\u0026quot;yolov8n.pt\u0026quot;) # pretrained YOLOv8n model\r# Run batched inference on a list of images\rresults = model([\u0026quot;images/resources/demo.jpg\u0026quot;, ]) # return a list of Results objects\r# Process results list\rfor result in results:\rboxes = result.boxes # Boxes object for bounding box outputs\rmasks = result.masks # Masks object for segmentation masks outputs\rkeypoints = result.keypoints # Keypoints object for pose outputs\rprobs = result.probs # Probs object for classification outputs\robb = result.obb # Oriented boxes object for OBB outputs\rresult.show() # display to screen\rresult.save(filename=\u0026quot;images/resources/result.jpg\u0026quot;) # save to disk\r​\n图形化界面开发 图形化界面部分的开发，我们采用了新的PySide6/PyQt6。PyQt和PySide都是C++的程序开发框架Qt的Python实现。PyQt是第三方组织对Qt官方提供的Python实现，也是Qt for Python最主要的实现。Qt官方对Python的支持力度越来越大，由于各种原因，Qt的官方选择使用PySide提供对Python Qt的支持。所以，Python Qt实际上存在两个分支：Qt4对应PyQt4和PySide；Qt5对应PyQt5和PySide2；Qt6则对应了PyQt6和PySide6。由于官方提供的PySide6从功能上来说更强，所以我们还是切换为PySide6作为本次图形化界面开发的框架。（实测下来他们直接的切换对于我们这些开发人员来说非常丝滑，我本次的更新中也只是更换了一下导入的过程）\n下面是我们这个图形化界面程序的实现，值得注意的是，我们的图形化界面没有使用到designer的图形化工具，所以我们的代码是没有UI文件的，如果有小伙伴对designer那种可视化的图形化界面比较感兴趣，可以去学习一下那种方式，构建出来的图形化界面可能比较炫一些。\nimport copy # 用于图像复制\rimport os # 用于系统路径查找\rimport shutil # 用于复制\rfrom PySide6.QtGui import * # GUI组件\rfrom PySide6.QtCore import * # 字体、边距等系统变量\rfrom PySide6.QtWidgets import * # 窗口等小组件\rimport threading # 多线程\rimport sys # 系统库\rimport cv2 # opencv图像处理\rimport torch # 深度学习框架\rimport os.path as osp # 路径查找\rimport time # 时间计算\rfrom ultralytics import YOLO # yolo核心算法\r# 常用的字符串常量\rWINDOW_TITLE =\u0026quot;Target detection system\u0026quot;\rWELCOME_SENTENCE = \u0026quot;欢迎使用基于yolov8的行人检测系统\u0026quot;\rICON_IMAGE = \u0026quot;images/UI/lufei.png\u0026quot;\rIMAGE_LEFT_INIT = \u0026quot;images/UI/up.jpeg\u0026quot;\rIMAGE_RIGHT_INIT = \u0026quot;images/UI/right.jpeg\u0026quot;\r​ class MainWindow(QTabWidget): def init(self): # 初始化界面 super().init() self.setWindowTitle(WINDOW_TITLE) # 系统界面标题 self.resize(1200, 800) # 系统初始化大小 self.setWindowIcon(QIcon(ICON_IMAGE)) # 系统logo图像 self.output_size = 480 # 上传的图像和视频在系统界面上显示的大小 self.img2predict = \u0026quot;\u0026quot; # 要进行预测的图像路径 # self.device = \u0026lsquo;cpu\u0026rsquo; self.init_vid_id = \u0026lsquo;0\u0026rsquo; # 摄像头修改 self.vid_source = int(self.init_vid_id) self.cap = cv2.VideoCapture(self.vid_source) self.stopEvent = threading.Event() self.webcam = True self.stopEvent.clear() self.model_path = \u0026ldquo;yolov8n.pt\u0026rdquo; # todo 指明模型加载的位置的设备 self.model = self.model_load(weights=self.model_path) self.conf_thres = 0.25 # 置信度的阈值 self.iou_thres = 0.45 # NMS操作的时候 IOU过滤的阈值 self.vid_gap = 30 # 摄像头视频帧保存间隔。 self.initUI() # 初始化图形化界面 self.reset_vid() # 重新设置视频参数，重新初始化是为了防止视频加载出错\n# 模型初始化\r@torch.no_grad()\rdef model_load(self, weights=\u0026quot;\u0026quot;):\r\u0026quot;\u0026quot;\u0026quot;\r模型加载\r\u0026quot;\u0026quot;\u0026quot;\rmodel_loaded = YOLO(weights)\rreturn model_loaded\rdef initUI(self):\r\u0026quot;\u0026quot;\u0026quot;\r图形化界面初始化\r\u0026quot;\u0026quot;\u0026quot;\r# ********************* 图片识别界面 *****************************\rfont_title = QFont('楷体', 16)\rfont_main = QFont('楷体', 14)\rimg_detection_widget = QWidget()\rimg_detection_layout = QVBoxLayout()\rimg_detection_title = QLabel(\u0026quot;图片识别功能\u0026quot;)\rimg_detection_title.setFont(font_title)\rmid_img_widget = QWidget()\rmid_img_layout = QHBoxLayout()\rself.left_img = QLabel()\rself.right_img = QLabel()\rself.left_img.setPixmap(QPixmap(IMAGE_LEFT_INIT))\rself.right_img.setPixmap(QPixmap(IMAGE_RIGHT_INIT))\rself.left_img.setAlignment(Qt.AlignCenter)\rself.right_img.setAlignment(Qt.AlignCenter)\rmid_img_layout.addWidget(self.left_img)\rmid_img_layout.addWidget(self.right_img)\rself.img_num_label = QLabel(\u0026quot;当前检测结果：待检测\u0026quot;)\rself.img_num_label.setFont(font_main)\rmid_img_widget.setLayout(mid_img_layout)\rup_img_button = QPushButton(\u0026quot;上传图片\u0026quot;)\rdet_img_button = QPushButton(\u0026quot;开始检测\u0026quot;)\rup_img_button.clicked.connect(self.upload_img)\rdet_img_button.clicked.connect(self.detect_img)\rup_img_button.setFont(font_main)\rdet_img_button.setFont(font_main)\rup_img_button.setStyleSheet(\u0026quot;QPushButton{color:white}\u0026quot;\r\u0026quot;QPushButton:hover{background-color: rgb(2,110,180);}\u0026quot;\r\u0026quot;QPushButton{background-color:rgb(48,124,208)}\u0026quot;\r\u0026quot;QPushButton{border:2px}\u0026quot;\r\u0026quot;QPushButton{border-radius:5px}\u0026quot;\r\u0026quot;QPushButton{padding:5px 5px}\u0026quot;\r\u0026quot;QPushButton{margin:5px 5px}\u0026quot;)\rdet_img_button.setStyleSheet(\u0026quot;QPushButton{color:white}\u0026quot;\r\u0026quot;QPushButton:hover{background-color: rgb(2,110,180);}\u0026quot;\r\u0026quot;QPushButton{background-color:rgb(48,124,208)}\u0026quot;\r\u0026quot;QPushButton{border:2px}\u0026quot;\r\u0026quot;QPushButton{border-radius:5px}\u0026quot;\r\u0026quot;QPushButton{padding:5px 5px}\u0026quot;\r\u0026quot;QPushButton{margin:5px 5px}\u0026quot;)\rimg_detection_layout.addWidget(img_detection_title, alignment=Qt.AlignCenter)\rimg_detection_layout.addWidget(mid_img_widget, alignment=Qt.AlignCenter)\rimg_detection_layout.addWidget(self.img_num_label)\rimg_detection_layout.addWidget(up_img_button)\rimg_detection_layout.addWidget(det_img_button)\rimg_detection_widget.setLayout(img_detection_layout)\r# ********************* 视频识别界面 *****************************\rvid_detection_widget = QWidget()\rvid_detection_layout = QVBoxLayout()\rvid_title = QLabel(\u0026quot;视频检测功能\u0026quot;)\rvid_title.setFont(font_title)\rself.vid_img = QLabel()\rself.vid_img.setPixmap(QPixmap(\u0026quot;images/UI/up.jpeg\u0026quot;))\rvid_title.setAlignment(Qt.AlignCenter)\rself.vid_img.setAlignment(Qt.AlignCenter)\rself.webcam_detection_btn = QPushButton(\u0026quot;摄像头实时监测\u0026quot;)\rself.mp4_detection_btn = QPushButton(\u0026quot;视频文件检测\u0026quot;)\rself.vid_stop_btn = QPushButton(\u0026quot;停止检测\u0026quot;)\rself.webcam_detection_btn.setFont(font_main)\rself.mp4_detection_btn.setFont(font_main)\rself.vid_stop_btn.setFont(font_main)\rself.webcam_detection_btn.setStyleSheet(\u0026quot;QPushButton{color:white}\u0026quot;\r\u0026quot;QPushButton:hover{background-color: rgb(2,110,180);}\u0026quot;\r\u0026quot;QPushButton{background-color:rgb(48,124,208)}\u0026quot;\r\u0026quot;QPushButton{border:2px}\u0026quot;\r\u0026quot;QPushButton{border-radius:5px}\u0026quot;\r\u0026quot;QPushButton{padding:5px 5px}\u0026quot;\r\u0026quot;QPushButton{margin:5px 5px}\u0026quot;)\rself.mp4_detection_btn.setStyleSheet(\u0026quot;QPushButton{color:white}\u0026quot;\r\u0026quot;QPushButton:hover{background-color: rgb(2,110,180);}\u0026quot;\r\u0026quot;QPushButton{background-color:rgb(48,124,208)}\u0026quot;\r\u0026quot;QPushButton{border:2px}\u0026quot;\r\u0026quot;QPushButton{border-radius:5px}\u0026quot;\r\u0026quot;QPushButton{padding:5px 5px}\u0026quot;\r\u0026quot;QPushButton{margin:5px 5px}\u0026quot;)\rself.vid_stop_btn.setStyleSheet(\u0026quot;QPushButton{color:white}\u0026quot;\r\u0026quot;QPushButton:hover{background-color: rgb(2,110,180);}\u0026quot;\r\u0026quot;QPushButton{background-color:rgb(48,124,208)}\u0026quot;\r\u0026quot;QPushButton{border:2px}\u0026quot;\r\u0026quot;QPushButton{border-radius:5px}\u0026quot;\r\u0026quot;QPushButton{padding:5px 5px}\u0026quot;\r\u0026quot;QPushButton{margin:5px 5px}\u0026quot;)\rself.webcam_detection_btn.clicked.connect(self.open_cam)\rself.mp4_detection_btn.clicked.connect(self.open_mp4)\rself.vid_stop_btn.clicked.connect(self.close_vid)\rvid_detection_layout.addWidget(vid_title)\rvid_detection_layout.addWidget(self.vid_img)\r# todo 添加摄像头检测标签逻辑\rself.vid_num_label = QLabel(\u0026quot;当前检测结果：{}\u0026quot;.format(\u0026quot;等待检测\u0026quot;))\rself.vid_num_label.setFont(font_main)\rvid_detection_layout.addWidget(self.vid_num_label)\rvid_detection_layout.addWidget(self.webcam_detection_btn)\rvid_detection_layout.addWidget(self.mp4_detection_btn)\rvid_detection_layout.addWidget(self.vid_stop_btn)\rvid_detection_widget.setLayout(vid_detection_layout)\r# ********************* 模型切换界面 *****************************\rabout_widget = QWidget()\rabout_layout = QVBoxLayout()\rabout_title = QLabel(WELCOME_SENTENCE)\rabout_title.setFont(QFont('楷体', 18))\rabout_title.setAlignment(Qt.AlignCenter)\rabout_img = QLabel()\rabout_img.setPixmap(QPixmap('images/UI/zhu.jpg'))\rself.model_label = QLabel(\u0026quot;当前模型：{}\u0026quot;.format(self.model_path))\rself.model_label.setFont(font_main)\rchange_model_button = QPushButton(\u0026quot;切换模型\u0026quot;)\rchange_model_button.setFont(font_main)\rchange_model_button.setStyleSheet(\u0026quot;QPushButton{color:white}\u0026quot;\r\u0026quot;QPushButton:hover{background-color: rgb(2,110,180);}\u0026quot;\r\u0026quot;QPushButton{background-color:rgb(48,124,208)}\u0026quot;\r\u0026quot;QPushButton{border:2px}\u0026quot;\r\u0026quot;QPushButton{border-radius:5px}\u0026quot;\r\u0026quot;QPushButton{padding:5px 5px}\u0026quot;\r\u0026quot;QPushButton{margin:5px 5px}\u0026quot;)\rrecord_button = QPushButton(\u0026quot;查看历史记录\u0026quot;)\rrecord_button.setFont(font_main)\rrecord_button.clicked.connect(self.check_record)\rrecord_button.setStyleSheet(\u0026quot;QPushButton{color:white}\u0026quot;\r\u0026quot;QPushButton:hover{background-color: rgb(2,110,180);}\u0026quot;\r\u0026quot;QPushButton{background-color:rgb(48,124,208)}\u0026quot;\r\u0026quot;QPushButton{border:2px}\u0026quot;\r\u0026quot;QPushButton{border-radius:5px}\u0026quot;\r\u0026quot;QPushButton{padding:5px 5px}\u0026quot;\r\u0026quot;QPushButton{margin:5px 5px}\u0026quot;)\rchange_model_button.clicked.connect(self.change_model)\rabout_img.setAlignment(Qt.AlignCenter)\rlabel_super = QLabel() # todo 更换作者信息\rlabel_super.setText(\u0026quot;\u0026lt;a href='https://blog.csdn.net/ECHOSON'\u0026gt;作者：肆十二\u0026lt;/a\u0026gt;\u0026quot;)\rlabel_super.setFont(QFont('楷体', 16))\rlabel_super.setOpenExternalLinks(True)\rlabel_super.setAlignment(Qt.AlignRight)\rabout_layout.addWidget(about_title)\rabout_layout.addStretch()\rabout_layout.addWidget(about_img)\rabout_layout.addWidget(self.model_label)\rabout_layout.addStretch()\rabout_layout.addWidget(change_model_button)\rabout_layout.addWidget(record_button)\rabout_layout.addWidget(label_super)\rabout_widget.setLayout(about_layout)\rself.left_img.setAlignment(Qt.AlignCenter)\rself.addTab(about_widget, '主页')\rself.addTab(img_detection_widget, '图片检测')\rself.addTab(vid_detection_widget, '视频检测')\rself.setTabIcon(0, QIcon(ICON_IMAGE))\rself.setTabIcon(1, QIcon(ICON_IMAGE))\rself.setTabIcon(2, QIcon(ICON_IMAGE))\r我们在代码的这个位置可以更换这个系统的默认标题和默认的logo图像。\n以及在这个位置可以更换为你自己的模型。\n","date":"0001-01-01T00:00:00Z","permalink":"https://nickk111.github.io/p/","title":""}]